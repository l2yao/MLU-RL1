{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)\n",
    "\n",
    "## Lecture 2 Support Notebook\n",
    "# TOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "<p>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Law-of-Large-Numbers\">\n",
    "        <span class=\"toc-item-num\">1&nbsp;&nbsp;</span>\n",
    "        Law of Large Numbers\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Cleaning-Robot-GridWorld\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>\n",
    "        Cleaning Robot GridWorld\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Monte-Carlo-Method\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>\n",
    "        Monte Carlo Method\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#MC-Exploring-Starts\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>\n",
    "        MC Exploring Starts\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#MC-Control\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>\n",
    "        MC Control\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#TD(0)\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>\n",
    "        TD(0)\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#TD(lambda)\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>\n",
    "        TD(lambda)\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#SARSA\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>\n",
    "        SARSA\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Q-learning\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>\n",
    "        Q-learning\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "#MIT License\n",
    "#Copyright (c) 2017 Massimiliano Patacchiola\n",
    "#\n",
    "#Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "#of this software and associated documentation files (the \"Software\"), to deal\n",
    "#in the Software without restriction, including without limitation the rights\n",
    "#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "#copies of the Software, and to permit persons to whom the Software is\n",
    "#furnished to do so, subject to the following conditions:\n",
    "#\n",
    "#The above copyright notice and this permission notice shall be included in all\n",
    "#copies or substantial portions of the Software.\n",
    "#\n",
    "#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "#SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL THIS PACKAGE THE FIRST TIME YOU RUN THIS INSTANCE\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "#%pylab inline\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") # this line is for use in regular SageMaker\n",
    "if \"MLU-Repo-RL/RL/\" not in sys.path:\n",
    "    sys.path.append(\"MLU-Repo-RL/RL/\") # this line is for use in SageMaker Studio\n",
    "from lib_rl.gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Law of Large Numbers\n",
    "Rolling a six-sided dice produces the expectation\n",
    "(1+2+3+4+5+6)/6=3.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation (rolling 3 times): 4.666666666666667\n",
      "Expectation (rolling 10 times): 3.1\n",
      "Expectation (rolling 100 times): 3.41\n",
      "Expectation (rolling 1000 times): 3.444\n",
      "Expectation (rolling 100000 times): 3.49895\n"
     ]
    }
   ],
   "source": [
    "# Trowing a dice for N times and evaluating the expectation\n",
    "dice = np.random.randint(low=1, high=7, size=3)\n",
    "print(\"Expectation (rolling 3 times): \" + str(np.mean(dice)))\n",
    "dice = np.random.randint(low=1, high=7, size=10)\n",
    "print(\"Expectation (rolling 10 times): \" + str(np.mean(dice)))\n",
    "dice = np.random.randint(low=1, high=7, size=100)\n",
    "print(\"Expectation (rolling 100 times): \" + str(np.mean(dice)))\n",
    "dice = np.random.randint(low=1, high=7, size=1000)\n",
    "print(\"Expectation (rolling 1000 times): \" + str(np.mean(dice)))\n",
    "dice = np.random.randint(low=1, high=7, size=100000)\n",
    "print(\"Expectation (rolling 100000 times): \" + str(np.mean(dice)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn\n",
    "As you can see the estimation of the expectation converges to the true value of 3.5. <br/>\n",
    "What we are doing in MC reinforcement learning is exactly the same but in this case we want to estimate the **value** for each state based on the return of each episode. <br/>\n",
    "As for the dice, more episodes we take into account more accurate our estimation will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOP\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        TOP\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Robot GridWorld\n",
    "+ The class called GridWorld creates a grid world of any size, add obstacles and terminal states. \n",
    "+ The cleaning robot will move in the grid world following a specific policy. Letâ€™s bring to life our 4x3 world.\n",
    "The class GridWorld has many similarities with [OpenAI Gym package](https://gym.openai.com/):\n",
    "+ The method **step()** moves forward at t+1 and returns:\n",
    "    + the **reward**, \n",
    "    + the **observation** (position of the robot), and \n",
    "    + a variable called **done** which is True when the episode is finished (the robot reached a terminal state).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Declare our environmnet variable\n",
    "# The world has 3 rows and 4 columns\n",
    "env = GridWorld(3, 4)\n",
    "# Define the state matrix\n",
    "# Adding obstacle at position (1,1)\n",
    "# Adding the two terminal states\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(state_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8 0.1 0.  0.1]\n",
      " [0.1 0.8 0.1 0. ]\n",
      " [0.  0.1 0.8 0.1]\n",
      " [0.1 0.  0.1 0.8]]\n"
     ]
    }
   ],
   "source": [
    "# Define the reward matrix\n",
    "# The reward is -0.04 for all states but the terminal\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "# Define the transition matrix\n",
    "# For each one of the four actions there is a probability\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "# Define the policy matrix\n",
    "# 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT, NaN=Obstacle, -1=NoAction\n",
    "# This is the optimal policy for world with reward=-0.04\n",
    "policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                          [0, np.NaN,  0,  -1],\n",
    "                          [0,      3,  3,   3]])\n",
    "# Set the matrices \n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)\n",
    "print (policy_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In a few lines I defined a grid world with the properties of our example.\n",
    "+ The policy is the optimal policy for a reward of -0.04 as we saw in the first lecture. \n",
    "+ Let's reset the environment (move the robot to starting position) and using the render() method to display the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -  -  -  * \n",
      " -  #  -  * \n",
      " â—‹  -  -  - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reset the environment\n",
    "observation = env.reset()\n",
    "#Display the world printing on terminal\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run an episode using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ACTION: 0.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " -  -  -  * \n",
      " â—‹  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 0.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " â—‹  -  -  * \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 1.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " -  â—‹  -  * \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 1.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " -  -  â—‹  * \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 1.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " -  -  -  * \n",
      " -  #  â—‹  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 0.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " -  -  â—‹  * \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 1.0\n",
      "REWARD: 1.0\n",
      "DONE: True\n",
      " -  -  -  â—‹ \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    action = policy_matrix[observation[0], observation[1]]\n",
    "    observation, reward, done = env.step(action)\n",
    "    print(\"\")\n",
    "    print(\"ACTION: \" + str(action))\n",
    "    print(\"REWARD: \" + str(reward))\n",
    "    print(\"DONE: \" + str(done))\n",
    "    env.render()\n",
    "    if done: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOP\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        TOP\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Method\n",
    "Here I will use: \n",
    "+ A discount factor of $\\gamma=0.999$\n",
    "+ The best policy $\\pi^*$\n",
    "+ The same transition model used in the previous lecture*. \n",
    "\n",
    "*Remember that with the current transition model the robot will go in the desired direction only in 80% of the cases. \n",
    "### First, The Return Function\n",
    "$G(t) = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{t=0}^ {\\infty} \\gamma^t R(S_{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(state_list, gamma):\n",
    "    \"\"\"\"\n",
    "    Summary line.\n",
    "    ------------\n",
    "    Function that estimates the return\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        state_list: tuple\n",
    "            List containing a tuple (position, reward).\n",
    "        gamma: float\n",
    "            The discount factor gamma.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        return_value: float\n",
    "            A value representing the return for that action list. \n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0\n",
    "    return_value = 0\n",
    "    for visit in state_list:\n",
    "        reward = visit[1]\n",
    "        return_value += reward * np.power(gamma, counter)\n",
    "        counter += 1\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility matrix after 1 iterations:\n",
      "[[0.71385957 0.75461418 0.83624584 1.        ]\n",
      " [0.67314571 0.         0.87712296 0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Utility matrix after 10001 iterations:\n",
      "[[ 0.81258549  0.87021201  0.92147643  1.        ]\n",
      " [ 0.76184848  0.          0.70314085 -1.        ]\n",
      " [ 0.70591512  0.65226754  0.          0.        ]]\n",
      "Utility matrix after 20001 iterations:\n",
      "[[ 0.81053044  0.86855081  0.91984667  1.        ]\n",
      " [ 0.75965483  0.          0.68475416 -1.        ]\n",
      " [ 0.70532575  0.65204334  0.          0.        ]]\n",
      "Utility matrix after 30001 iterations:\n",
      "[[ 0.80999002  0.86791341  0.91915506  1.        ]\n",
      " [ 0.75895368  0.          0.68257497 -1.        ]\n",
      " [ 0.70340358  0.65265229  0.          0.        ]]\n",
      "Utility matrix after 40001 iterations:\n",
      "[[ 0.80932971  0.86716144  0.9184321   1.        ]\n",
      " [ 0.75819847  0.          0.67667488 -1.        ]\n",
      " [ 0.70267753  0.65264453  0.          0.        ]]\n",
      "Utility matrix after 50000 iterations:\n",
      "[[ 0.80895301  0.86671845  0.91797607  1.        ]\n",
      " [ 0.75784604  0.          0.67261791 -1.        ]\n",
      " [ 0.70387211  0.65419682  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Defining an empty utility matrix\n",
    "utility_matrix = np.zeros((3,4))\n",
    "# init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full((3,4), 1.0e-10) \n",
    "gamma = 0.999 #discount factor\n",
    "tot_epoch = 50000\n",
    "print_epoch = 10000\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    #Starting a new episode\n",
    "    episode_list = list()\n",
    "    #Reset and return the first observation\n",
    "    observation= env.reset(exploring_starts=False)\n",
    "    for _ in range(1000):\n",
    "        # Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        # Move one step in the environment and get obs and reward\n",
    "        observation, reward, done = env.step(action)\n",
    "        # Append the visit in the episode list\n",
    "        episode_list.append((observation, reward))\n",
    "        if done: break\n",
    "    # The episode is finished, now estimating the utilities\n",
    "    counter = 0\n",
    "    # Checkup to identify if it is the first visit to a state\n",
    "    checkup_matrix = np.zeros((3,4))\n",
    "    # This cycle is the implementation of First-Visit MC.\n",
    "    # For each state stored in the episode list it checks if it\n",
    "    # is the first visit and then estimates the return.\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        row = observation[0]\n",
    "        col = observation[1]\n",
    "        reward = visit[1]\n",
    "        if(checkup_matrix[row, col] == 0):\n",
    "            return_value = get_return(episode_list[counter:], gamma)\n",
    "            running_mean_matrix[row, col] += 1\n",
    "            utility_matrix[row, col] += return_value\n",
    "            checkup_matrix[row, col] = 1\n",
    "        counter += 1\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"Utility matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(utility_matrix / running_mean_matrix)\n",
    "\n",
    "#Time to check the state-value matrix obtained\n",
    "print(\"Utility matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(utility_matrix / running_mean_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above will print the estimation of the state-value matrix every 1000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOP\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        TOP\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Exploring Starts\n",
    "+ To enable the exploring starts in our code the only thing to do is to set the parameter exploring_strarts in the reset() function to True.\n",
    "+ Every time a new episode begins, the robot will start from a random position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility matrix after 1 iterations:\n",
      "[[0.87712296 0.918041   0.959      1.        ]\n",
      " [0.79540959 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Utility matrix after 10001 iterations:\n",
      "[[ 0.81026248  0.868537    0.91979558  1.        ]\n",
      " [ 0.75772467  0.          0.66832482 -1.        ]\n",
      " [ 0.70289022  0.65622204  0.60444678  0.34647217]]\n",
      "Utility matrix after 20001 iterations:\n",
      "[[ 0.80925457  0.86744368  0.91851261  1.        ]\n",
      " [ 0.75718637  0.          0.65995054 -1.        ]\n",
      " [ 0.70006232  0.65313244  0.60595218  0.33536195]]\n",
      "Utility matrix after 30001 iterations:\n",
      "[[ 0.80834709  0.86671298  0.91794931  1.        ]\n",
      " [ 0.75569356  0.          0.65826502 -1.        ]\n",
      " [ 0.69747972  0.64923281  0.60282439  0.34505005]]\n",
      "Utility matrix after 40001 iterations:\n",
      "[[ 0.80707175  0.8653966   0.91682541  1.        ]\n",
      " [ 0.75492972  0.          0.65628554 -1.        ]\n",
      " [ 0.69742164  0.64818843  0.59982525  0.35682387]]\n",
      "Utility matrix after 50000 iterations:\n",
      "[[ 0.80840535  0.86656165  0.91749534  1.        ]\n",
      " [ 0.75669248  0.          0.66375795 -1.        ]\n",
      " [ 0.69920226  0.64945207  0.59968589  0.34993682]]\n"
     ]
    }
   ],
   "source": [
    "# Defining an empty utility matrix\n",
    "utility_matrix = np.zeros((3,4))\n",
    "# init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full((3,4), 1.0e-10) \n",
    "gamma = 0.999 #discount factor\n",
    "tot_epoch = 50000\n",
    "print_epoch = 10000\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    #Starting a new episode\n",
    "    episode_list = list()\n",
    "    #Reset and return the first observation\n",
    "    observation= env.reset(exploring_starts=True)\n",
    "    for _ in range(1000):\n",
    "        # Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        # Move one step in the environment and get obs and reward\n",
    "        observation, reward, done = env.step(action)\n",
    "        # Append the visit in the episode list\n",
    "        episode_list.append((observation, reward))\n",
    "        if done: break\n",
    "    # The episode is finished, now estimating the utilities\n",
    "    counter = 0\n",
    "    # Checkup to identify if it is the first visit to a state\n",
    "    checkup_matrix = np.zeros((3,4))\n",
    "    # This cycle is the implementation of First-Visit MC.\n",
    "    # For each state stored in the episode list it checks if it\n",
    "    # is the first visit and then estimates the return.\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        row = observation[0]\n",
    "        col = observation[1]\n",
    "        reward = visit[1]\n",
    "        if(checkup_matrix[row, col] == 0):\n",
    "            return_value = get_return(episode_list[counter:], gamma)\n",
    "            running_mean_matrix[row, col] += 1\n",
    "            utility_matrix[row, col] += return_value\n",
    "            checkup_matrix[row, col] = 1\n",
    "        counter += 1\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"Utility matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(utility_matrix / running_mean_matrix)\n",
    "\n",
    "#Time to check the utility matrix obtained\n",
    "print(\"Utility matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(utility_matrix / running_mean_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOP\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        TOP\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Control\n",
    "We will use again the function **get_return()** but this time the input will be a list containing the tuple (observation, action, reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(state_list, gamma):\n",
    "    '''Get the return for a list of action-state values.\n",
    "    @return get the Return\n",
    "    '''\n",
    "    counter = 0\n",
    "    return_value = 0\n",
    "    for visit in state_list:\n",
    "        reward = visit[2]\n",
    "        return_value += reward * np.power(gamma, counter)\n",
    "        counter += 1\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use another new function called **update_policy()**, which will make the policy greedy with respect to the current state-action function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(episode_list, policy_matrix, state_action_matrix):\n",
    "    '''Update a policy making it greedy in respect of the state-action matrix.\n",
    "    @return the updated policy\n",
    "    '''\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        col = observation[1] + (observation[0]*4)\n",
    "        if(policy_matrix[observation[0], observation[1]] != -1):      \n",
    "            policy_matrix[observation[0], observation[1]] = \\\n",
    "                np.argmax(state_action_matrix[:,col])\n",
    "    return policy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The update_policy() function is part of the improvement step of the GPI and it is fundamental in order to get convergence to an optimal policy. \n",
    "+ I will use also the function print_policy() which I already used in the previous lecture in order to print on terminal the policy using the symbols: ^, >, v, <, *, #. \n",
    "+ In the main() function, I initialized a random policy matrix and the state_action_matrix that contains the utilities of each state-action pair. \n",
    "+ The matrix can be initialised to zeros or to random values, it does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy_matrix):\n",
    "    '''Print the policy using specific symbol.\n",
    "    * terminal state\n",
    "    ^ > v < up, right, down, left\n",
    "    # obstacle\n",
    "    '''\n",
    "    counter = 0\n",
    "    shape = policy_matrix.shape\n",
    "    policy_string = \"\"\n",
    "    for row in range(shape[0]):\n",
    "        for col in range(shape[1]):\n",
    "            if(policy_matrix[row,col] == -1): policy_string += \" *  \"            \n",
    "            elif(policy_matrix[row,col] == 0): policy_string += \" ^  \"\n",
    "            elif(policy_matrix[row,col] == 1): policy_string += \" >  \"\n",
    "            elif(policy_matrix[row,col] == 2): policy_string += \" v  \"           \n",
    "            elif(policy_matrix[row,col] == 3): policy_string += \" <  \"\n",
    "            elif(np.isnan(policy_matrix[row,col])): policy_string += \" #  \"\n",
    "            counter += 1\n",
    "        policy_string += '\\n'\n",
    "    print(policy_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the main loop of the algorithm, which is not so different from the loop used for MC prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n",
      "\n",
      "State-Action matrix after 1 iterations:\n",
      "[[ 7.71696287e+09  9.33849552e+08  3.97822401e+09  6.94396100e+09\n",
      "  -2.49301551e+01  8.55541615e+09  8.31173144e+09  2.26406827e+09\n",
      "   7.33239643e+09  6.03950161e+08  3.88134064e+09  9.70449289e+09]\n",
      " [ 6.41951536e+09 -2.48911585e+01  7.67571513e+08  5.51195807e+09\n",
      "   2.51512823e+09  2.12100012e+09  5.45856439e+08  2.14481050e+09\n",
      "   1.58636431e+09  8.80395855e+09  5.57438991e+08  7.02723037e+09]\n",
      " [ 9.64446449e+08 -2.47451203e+01  8.16166341e+08  1.16510824e+09\n",
      "   7.63951689e+09  5.30423046e+09  9.95403416e+09  5.01049261e+09\n",
      "   9.34478792e+09  3.87166691e+09  4.90449884e+09  3.34983171e+09]\n",
      " [-2.45579184e+01  2.10857095e+09  9.87472961e+09  1.14452396e+09\n",
      "   9.58739992e+09  3.45244244e+09  1.96063362e+09  5.59657106e+09\n",
      "   5.62846834e+09  5.93031962e+09  1.53663511e+09  8.43722100e+09]]\n",
      "Policy matrix after 1 iterations:\n",
      "[[ 0.  3.  1. -1.]\n",
      " [ 3. nan  0. -1.]\n",
      " [ 0.  1.  1.  2.]]\n",
      " ^   <   >   *  \n",
      " <   #   ^   *  \n",
      " ^   >   >   v  \n",
      "\n",
      "\n",
      "State-Action matrix after 3001 iterations:\n",
      "[[-1.79757654e-01  2.99519428e-01  5.36916562e-01  6.94396100e+09\n",
      "   4.50586424e-01  8.55541615e+09  6.83317952e-01  2.26406827e+09\n",
      "  -3.70650470e-01  1.05690087e-01  5.04079012e-01 -9.75324352e-01]\n",
      " [ 3.74644465e-01  8.34139653e-01  9.25738730e-01  5.51195807e+09\n",
      "  -4.80727727e-01  2.12100012e+09 -7.04008765e-01  2.14481050e+09\n",
      "  -2.54556697e-01 -6.09878621e-01 -7.83267279e-01 -6.07883047e-01]\n",
      " [-7.13644142e-01  3.54394006e-01  1.00715199e-01  1.16510824e+09\n",
      "  -3.85577084e-01  5.30423046e+09 -5.55978316e-01  5.01049261e+09\n",
      "  -8.28264427e-01 -5.99375281e-01 -1.06318140e+00  1.78991314e-02]\n",
      " [ 1.12458362e-01 -8.19219817e-01  8.37340705e-02  1.14452396e+09\n",
      "  -4.73656004e-01  3.45244244e+09 -3.51768109e-01  5.59657106e+09\n",
      "  -5.01072053e-01 -3.43206258e-01 -2.37536591e-01 -5.64832317e-01]]\n",
      "Policy matrix after 3001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 1.  0.  0.  2.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " >   ^   ^   v  \n",
      "\n",
      "\n",
      "State-Action matrix after 6001 iterations:\n",
      "[[ 2.62191913e-01  3.60312003e-01  7.08276572e-01  6.94396100e+09\n",
      "   7.02114152e-01  8.55541615e+09  6.86977543e-01  2.26406827e+09\n",
      "   5.57669006e-01  2.15952829e-01  5.26629959e-01 -9.53731710e-01]\n",
      " [ 6.50328731e-01  8.94800584e-01  9.43647136e-01  5.51195807e+09\n",
      "   2.24461569e-01  2.12100012e+09 -6.89877464e-01  2.14481050e+09\n",
      "  -9.93151523e-02 -4.77243465e-01 -5.96477477e-01 -2.11590581e-01]\n",
      " [-4.11665223e-02  6.27499553e-01  4.12071073e-01  1.16510824e+09\n",
      "   6.62011139e-02  5.30423046e+09 -6.85611412e-02  5.01049261e+09\n",
      "  -1.40221613e-01 -1.60057566e-01 -6.20451342e-01  9.91418600e-02]\n",
      " [ 4.87390489e-01  6.24982236e-02  4.55035305e-01  1.14452396e+09\n",
      "  -1.86702260e-01  3.45244244e+09  2.01253522e-02  5.59657106e+09\n",
      "   5.12959496e-02  4.24348298e-02  1.05812031e-01 -1.00702676e-01]]\n",
      "Policy matrix after 6001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  0.  0.  2.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   ^   ^   v  \n",
      "\n",
      "\n",
      "State-Action matrix after 9001 iterations:\n",
      "[[ 4.56657258e-01  4.07728174e-01  7.82689637e-01  6.94396100e+09\n",
      "   7.45751951e-01  8.55541615e+09  6.86357171e-01  2.26406827e+09\n",
      "   6.44598230e-01  2.79364334e-01  5.30064232e-01 -9.23604612e-01]\n",
      " [ 7.29089346e-01  9.00137544e-01  9.46910173e-01  5.51195807e+09\n",
      "   4.22053373e-01  2.12100012e+09 -6.90659050e-01  2.14481050e+09\n",
      "  -6.69833980e-02 -3.59732705e-01 -5.02907849e-01 -1.45073195e-01]\n",
      " [ 2.51525360e-01  7.05528313e-01  5.53715715e-01  1.16510824e+09\n",
      "   2.75715816e-01  5.30423046e+09  6.83328289e-02  5.01049261e+09\n",
      "   1.12952198e-01  9.29338662e-02 -3.95852781e-01  1.08894930e-01]\n",
      " [ 5.78590902e-01  3.18310489e-01  5.61292788e-01  1.14452396e+09\n",
      "   1.82779824e-02  3.45244244e+09  2.55163452e-01  5.59657106e+09\n",
      "   2.60610835e-01  2.34182817e-01  1.81975931e-01  6.97046611e-03]]\n",
      "Policy matrix after 9001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  0.  0.  2.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   ^   ^   v  \n",
      "\n",
      "\n",
      "State-Action matrix after 12001 iterations:\n",
      "[[ 5.44213134e-01  4.51778172e-01  8.14785292e-01  6.94396100e+09\n",
      "   7.61848135e-01  8.55541615e+09  6.89087017e-01  2.26406827e+09\n",
      "   6.73789786e-01  3.17174013e-01  5.40774557e-01 -9.13479449e-01]\n",
      " [ 7.63631455e-01  9.03889363e-01  9.49871849e-01  5.51195807e+09\n",
      "   4.96495959e-01  2.12100012e+09 -6.63029721e-01  2.14481050e+09\n",
      "  -1.75754925e-02 -2.86576452e-01 -3.90082743e-01 -6.53323126e-02]\n",
      " [ 3.98887484e-01  7.37475489e-01  5.91826928e-01  1.16510824e+09\n",
      "   3.62087489e-01  5.30423046e+09  1.74973997e-01  5.01049261e+09\n",
      "   2.31124305e-01  1.96783605e-01 -2.18983971e-01  1.30033650e-01]\n",
      " [ 6.35000064e-01  4.50663671e-01  6.30623456e-01  1.14452396e+09\n",
      "   1.34526546e-01  3.45244244e+09  3.66566479e-01  5.59657106e+09\n",
      "   3.63968410e-01  2.90594232e-01  2.41916456e-01  9.25586870e-02]]\n",
      "Policy matrix after 12001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  0.  0.  2.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   ^   ^   v  \n",
      "\n",
      "\n",
      "State-Action matrix after 15001 iterations:\n",
      "[[ 5.99878473e-01  4.86200357e-01  8.30825062e-01  6.94396100e+09\n",
      "   7.73461225e-01  8.55541615e+09  6.92703513e-01  2.26406827e+09\n",
      "   6.88413161e-01  3.43879384e-01  5.46368686e-01 -8.98397688e-01]\n",
      " [ 7.84770879e-01  9.06070322e-01  9.52371741e-01  5.51195807e+09\n",
      "   5.52524675e-01  2.12100012e+09 -6.70954169e-01  2.14481050e+09\n",
      "   2.48846301e-02 -2.23621075e-01 -3.23606145e-01 -2.71453035e-02]\n",
      " [ 4.79331647e-01  7.58799017e-01  6.33002111e-01  1.16510824e+09\n",
      "   4.35047949e-01  5.30423046e+09  2.38492804e-01  5.01049261e+09\n",
      "   3.01010620e-01  2.50507311e-01 -1.31182635e-01  1.53580466e-01]\n",
      " [ 6.62181237e-01  5.26074586e-01  6.75866742e-01  1.14452396e+09\n",
      "   2.29477038e-01  3.45244244e+09  4.20458997e-01  5.59657106e+09\n",
      "   4.34581983e-01  3.42372926e-01  2.80994315e-01  2.20962555e-01]]\n",
      "Policy matrix after 15001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  0.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   ^   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 18001 iterations:\n",
      "[[ 6.33566369e-01  5.08359848e-01  8.47557520e-01  6.94396100e+09\n",
      "   7.78186740e-01  8.55541615e+09  6.93869920e-01  2.26406827e+09\n",
      "   7.01020530e-01  3.56748075e-01  5.59548368e-01 -8.81779823e-01]\n",
      " [ 7.96481195e-01  9.04394204e-01  9.52111892e-01  5.51195807e+09\n",
      "   5.84179763e-01  2.12100012e+09 -6.76811293e-01  2.14481050e+09\n",
      "   6.40300045e-02 -1.45025942e-01 -2.39072076e-01  1.11116138e-02]\n",
      " [ 5.36585560e-01  7.74436193e-01  6.26612494e-01  1.16510824e+09\n",
      "   4.81441559e-01  5.30423046e+09  2.73723225e-01  5.01049261e+09\n",
      "   3.75079578e-01  3.13587513e-01 -5.48904493e-02  1.62636808e-01]\n",
      " [ 6.79260975e-01  5.77140071e-01  6.95263519e-01  1.14452396e+09\n",
      "   2.82274764e-01  3.45244244e+09  4.72878956e-01  5.59657106e+09\n",
      "   4.74466130e-01  5.28697648e-01  3.44778871e-01  2.94831997e-01]]\n",
      "Policy matrix after 18001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 21001 iterations:\n",
      "[[ 6.50132577e-01  5.30391094e-01  8.60290303e-01  6.94396100e+09\n",
      "   7.82949071e-01  8.55541615e+09  6.95390892e-01  2.26406827e+09\n",
      "   7.11035288e-01  3.66647894e-01  5.70019125e-01 -8.73201354e-01]\n",
      " [ 8.05144252e-01  9.04317997e-01  9.52895531e-01  5.51195807e+09\n",
      "   6.05346248e-01  2.12100012e+09 -6.71421401e-01  2.14481050e+09\n",
      "   1.06568203e-01 -8.41882052e-02 -1.59664197e-01  2.42785148e-02]\n",
      " [ 5.64402911e-01  7.88582780e-01  6.43431194e-01  1.16510824e+09\n",
      "   5.16579097e-01  5.30423046e+09  2.89128283e-01  5.01049261e+09\n",
      "   4.21470634e-01  3.64061832e-01  1.87444159e-02  1.76619029e-01]\n",
      " [ 7.00547126e-01  6.11679474e-01  7.25680146e-01  1.14452396e+09\n",
      "   3.42268493e-01  3.45244244e+09  4.90044208e-01  5.59657106e+09\n",
      "   5.03258218e-01  5.94824640e-01  3.94808763e-01  3.27100663e-01]]\n",
      "Policy matrix after 21001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 24001 iterations:\n",
      "[[ 6.64863827e-01  5.48634048e-01  8.68844130e-01  6.94396100e+09\n",
      "   7.85057649e-01  8.55541615e+09  6.96869296e-01  2.26406827e+09\n",
      "   7.16093627e-01  3.77030790e-01  5.80782133e-01 -8.57018981e-01]\n",
      " [ 8.11295077e-01  9.04394641e-01  9.53355312e-01  5.51195807e+09\n",
      "   6.26721135e-01  2.12100012e+09 -6.68553760e-01  2.14481050e+09\n",
      "   1.38789184e-01 -4.41023906e-02 -1.16536608e-01  4.25406032e-02]\n",
      " [ 5.90089869e-01  7.96257983e-01  6.52677512e-01  1.16510824e+09\n",
      "   5.37696740e-01  5.30423046e+09  3.20967206e-01  5.01049261e+09\n",
      "   4.54051006e-01  3.99273517e-01  8.82521332e-02  1.87453561e-01]\n",
      " [ 7.17229338e-01  6.34191461e-01  7.41399514e-01  1.14452396e+09\n",
      "   3.81087868e-01  3.45244244e+09  5.13802952e-01  5.59657106e+09\n",
      "   5.30752266e-01  6.17083377e-01  4.26243888e-01  3.51710381e-01]]\n",
      "Policy matrix after 24001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 27001 iterations:\n",
      "[[ 6.86398393e-01  5.73156852e-01  8.77615740e-01  6.94396100e+09\n",
      "   7.87262572e-01  8.55541615e+09  6.95310121e-01  2.26406827e+09\n",
      "   7.18793442e-01  3.84775721e-01  5.86178310e-01 -8.45356943e-01]\n",
      " [ 8.17010013e-01  9.05600649e-01  9.54084881e-01  5.51195807e+09\n",
      "   6.41758090e-01  2.12100012e+09 -6.43000142e-01  2.14481050e+09\n",
      "   1.67473685e-01 -3.56836503e-03 -7.19440024e-02  6.34310038e-02]\n",
      " [ 6.12647000e-01  8.01640308e-01  6.45049906e-01  1.16510824e+09\n",
      "   5.64179658e-01  5.30423046e+09  3.34176950e-01  5.01049261e+09\n",
      "   4.73551800e-01  4.27931500e-01  1.44580428e-01  1.95867935e-01]\n",
      " [ 7.30179133e-01  6.61175838e-01  7.42198927e-01  1.14452396e+09\n",
      "   4.17356277e-01  3.45244244e+09  5.28509358e-01  5.59657106e+09\n",
      "   5.51289966e-01  6.31124744e-01  4.43154496e-01  3.62625095e-01]]\n",
      "Policy matrix after 27001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 30001 iterations:\n",
      "[[ 6.90881815e-01  5.92622622e-01  8.81868598e-01  6.94396100e+09\n",
      "   7.87077065e-01  8.55541615e+09  6.93948748e-01  2.26406827e+09\n",
      "   7.19980578e-01  3.94102958e-01  5.90531657e-01 -8.29143411e-01]\n",
      " [ 8.18703133e-01  9.04208157e-01  9.53537749e-01  5.51195807e+09\n",
      "   6.46927062e-01  2.12100012e+09 -6.47372540e-01  2.14481050e+09\n",
      "   1.96952119e-01  4.10779341e-02 -4.23609040e-02  7.79247087e-02]\n",
      " [ 6.31167032e-01  8.00836539e-01  6.39926576e-01  1.16510824e+09\n",
      "   5.79386956e-01  5.30423046e+09  3.56447093e-01  5.01049261e+09\n",
      "   4.95537911e-01  4.53653521e-01  1.78263675e-01  2.12078437e-01]\n",
      " [ 7.35005773e-01  6.77116609e-01  7.58836011e-01  1.14452396e+09\n",
      "   4.36707494e-01  3.45244244e+09  5.24738504e-01  5.59657106e+09\n",
      "   5.68784444e-01  6.36541599e-01  4.56791705e-01  3.64488856e-01]]\n",
      "Policy matrix after 30001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 33001 iterations:\n",
      "[[ 7.04136028e-01  6.05870775e-01  8.86998604e-01  6.94396100e+09\n",
      "   7.87918676e-01  8.55541615e+09  6.96639584e-01  2.26406827e+09\n",
      "   7.23548650e-01  4.03340446e-01  5.95173800e-01 -8.22240613e-01]\n",
      " [ 8.21791894e-01  9.04950573e-01  9.54651194e-01  5.51195807e+09\n",
      "   6.59352261e-01  2.12100012e+09 -6.57995657e-01  2.14481050e+09\n",
      "   2.23275086e-01  7.12291329e-02 -3.85456708e-03  9.31087990e-02]\n",
      " [ 6.39061799e-01  8.07083765e-01  6.39595507e-01  1.16510824e+09\n",
      "   5.90710251e-01  5.30423046e+09  3.72578808e-01  5.01049261e+09\n",
      "   5.18721130e-01  4.71357974e-01  2.12037922e-01  2.16864829e-01]\n",
      " [ 7.41922413e-01  6.88587395e-01  7.71035489e-01  1.14452396e+09\n",
      "   4.54966577e-01  3.45244244e+09  5.38981752e-01  5.59657106e+09\n",
      "   5.80487211e-01  6.46206163e-01  4.76230805e-01  3.74855565e-01]]\n",
      "Policy matrix after 33001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 36001 iterations:\n",
      "[[ 7.11298316e-01  6.19066333e-01  8.91310332e-01  6.94396100e+09\n",
      "   7.89192476e-01  8.55541615e+09  6.98429997e-01  2.26406827e+09\n",
      "   7.26453708e-01  4.10637335e-01  5.99966216e-01 -8.25952005e-01]\n",
      " [ 8.23403636e-01  9.04598697e-01  9.54741102e-01  5.51195807e+09\n",
      "   6.69095737e-01  2.12100012e+09 -6.56081980e-01  2.14481050e+09\n",
      "   2.45959396e-01  9.91965813e-02  1.29532974e-02  1.04315435e-01]\n",
      " [ 6.48021921e-01  8.11538724e-01  6.41970861e-01  1.16510824e+09\n",
      "   6.03349485e-01  5.30423046e+09  3.89785483e-01  5.01049261e+09\n",
      "   5.36127635e-01  4.86833500e-01  2.47639220e-01  2.26172511e-01]\n",
      " [ 7.44450473e-01  6.90619914e-01  7.74283933e-01  1.14452396e+09\n",
      "   4.76578983e-01  3.45244244e+09  5.52588596e-01  5.59657106e+09\n",
      "   5.91497778e-01  6.52568945e-01  4.84531685e-01  3.83575432e-01]]\n",
      "Policy matrix after 36001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 39001 iterations:\n",
      "[[ 7.16550452e-01  6.31251460e-01  8.95298676e-01  6.94396100e+09\n",
      "   7.90374321e-01  8.55541615e+09  6.98143775e-01  2.26406827e+09\n",
      "   7.27632803e-01  4.17423737e-01  6.02784407e-01 -8.15190622e-01]\n",
      " [ 8.25892945e-01  9.05394332e-01  9.55036855e-01  5.51195807e+09\n",
      "   6.77975595e-01  2.12100012e+09 -6.54850617e-01  2.14481050e+09\n",
      "   2.68541063e-01  1.24610835e-01  4.72858437e-02  1.11694886e-01]\n",
      " [ 6.58118642e-01  8.17136015e-01  6.50752054e-01  1.16510824e+09\n",
      "   6.13807410e-01  5.30423046e+09  4.00453869e-01  5.01049261e+09\n",
      "   5.46963364e-01  5.04120846e-01  2.62522061e-01  2.37703110e-01]\n",
      " [ 7.49523672e-01  6.97404378e-01  7.81360531e-01  1.14452396e+09\n",
      "   4.99022758e-01  3.45244244e+09  5.59437434e-01  5.59657106e+09\n",
      "   5.94863823e-01  6.56606740e-01  5.01112452e-01  3.85551644e-01]]\n",
      "Policy matrix after 39001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 42001 iterations:\n",
      "[[ 7.22437999e-01  6.41321076e-01  8.96926258e-01  6.94396100e+09\n",
      "   7.90453009e-01  8.55541615e+09  6.97718564e-01  2.26406827e+09\n",
      "   7.28100080e-01  4.23987920e-01  6.04550186e-01 -8.09868658e-01]\n",
      " [ 8.26815331e-01  9.04718885e-01  9.54784238e-01  5.51195807e+09\n",
      "   6.81948605e-01  2.12100012e+09 -6.49296382e-01  2.14481050e+09\n",
      "   2.86931543e-01  1.53090742e-01  6.74985423e-02  1.21165136e-01]\n",
      " [ 6.65525912e-01  8.23281333e-01  6.54722383e-01  1.16510824e+09\n",
      "   6.19019122e-01  5.30423046e+09  4.10552561e-01  5.01049261e+09\n",
      "   5.59699784e-01  5.17823808e-01  2.78851214e-01  2.40981700e-01]\n",
      " [ 7.52655031e-01  7.01909459e-01  7.87121610e-01  1.14452396e+09\n",
      "   5.14991466e-01  3.45244244e+09  5.61600918e-01  5.59657106e+09\n",
      "   6.05448421e-01  6.58906061e-01  5.09191814e-01  3.92666531e-01]]\n",
      "Policy matrix after 42001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 45001 iterations:\n",
      "[[ 7.29540519e-01  6.52249905e-01  8.98700107e-01  6.94396100e+09\n",
      "   7.90331871e-01  8.55541615e+09  6.97319660e-01  2.26406827e+09\n",
      "   7.28158179e-01  4.31643624e-01  6.05005405e-01 -8.06195011e-01]\n",
      " [ 8.27809647e-01  9.04439506e-01  9.54885853e-01  5.51195807e+09\n",
      "   6.88399319e-01  2.12100012e+09 -6.58061585e-01  2.14481050e+09\n",
      "   3.04632048e-01  1.64343158e-01  8.53222614e-02  1.41802229e-01]\n",
      " [ 6.71852537e-01  8.28262748e-01  6.60535493e-01  1.16510824e+09\n",
      "   6.25201848e-01  5.30423046e+09  4.18066488e-01  5.01049261e+09\n",
      "   5.66926539e-01  5.24389937e-01  2.91336316e-01  2.45242891e-01]\n",
      " [ 7.55583325e-01  7.07681519e-01  7.90120043e-01  1.14452396e+09\n",
      "   5.29070967e-01  3.45244244e+09  5.70494249e-01  5.59657106e+09\n",
      "   6.04501151e-01  6.60883214e-01  5.18145708e-01  3.94777495e-01]]\n",
      "Policy matrix after 45001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 48001 iterations:\n",
      "[[ 7.36086020e-01  6.62187287e-01  8.99820545e-01  6.94396100e+09\n",
      "   7.90347184e-01  8.55541615e+09  6.96231626e-01  2.26406827e+09\n",
      "   7.28204262e-01  4.36478900e-01  6.03760089e-01 -8.04830250e-01]\n",
      " [ 8.28886162e-01  9.04326196e-01  9.54833987e-01  5.51195807e+09\n",
      "   6.93397634e-01  2.12100012e+09 -6.50487045e-01  2.14481050e+09\n",
      "   3.18760417e-01  1.83424442e-01  1.05307944e-01  1.55550746e-01]\n",
      " [ 6.76887211e-01  8.31006725e-01  6.61051374e-01  1.16510824e+09\n",
      "   6.29844926e-01  5.30423046e+09  4.13459371e-01  5.01049261e+09\n",
      "   5.77284331e-01  5.30071975e-01  3.01507408e-01  2.49883076e-01]\n",
      " [ 7.60893120e-01  7.12994298e-01  7.94817336e-01  1.14452396e+09\n",
      "   5.40338894e-01  3.45244244e+09  5.84564409e-01  5.59657106e+09\n",
      "   6.08587348e-01  6.61372098e-01  5.24120338e-01  3.95608717e-01]]\n",
      "Policy matrix after 48001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 51001 iterations:\n",
      "[[ 7.42138339e-01  6.70075062e-01  9.02257810e-01  6.94396100e+09\n",
      "   7.90925521e-01  8.55541615e+09  6.98827632e-01  2.26406827e+09\n",
      "   7.28657993e-01  4.44425987e-01  6.07533673e-01 -7.95688236e-01]\n",
      " [ 8.30052467e-01  9.04431393e-01  9.55138193e-01  5.51195807e+09\n",
      "   6.99360049e-01  2.12100012e+09 -6.46890429e-01  2.14481050e+09\n",
      "   3.27727222e-01  2.09030291e-01  1.21618145e-01  1.64830656e-01]\n",
      " [ 6.83535681e-01  8.31384237e-01  6.66309359e-01  1.16510824e+09\n",
      "   6.29726808e-01  5.30423046e+09  4.14529128e-01  5.01049261e+09\n",
      "   5.84953297e-01  5.34379699e-01  3.19828944e-01  2.57565749e-01]\n",
      " [ 7.62908185e-01  7.19722254e-01  7.99043602e-01  1.14452396e+09\n",
      "   5.53451158e-01  3.45244244e+09  5.87405213e-01  5.59657106e+09\n",
      "   6.14059593e-01  6.62870625e-01  5.30171438e-01  3.99015067e-01]]\n",
      "Policy matrix after 51001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 54001 iterations:\n",
      "[[ 7.47309328e-01  6.77558505e-01  9.03840340e-01  6.94396100e+09\n",
      "   7.91667037e-01  8.55541615e+09  6.99820671e-01  2.26406827e+09\n",
      "   7.29571086e-01  4.49539664e-01  6.09122204e-01 -7.87792775e-01]\n",
      " [ 8.31272684e-01  9.04781967e-01  9.55348090e-01  5.51195807e+09\n",
      "   7.00417668e-01  2.12100012e+09 -6.46444262e-01  2.14481050e+09\n",
      "   3.43748411e-01  2.30238635e-01  1.29171708e-01  1.75406347e-01]\n",
      " [ 6.89071549e-01  8.35304453e-01  6.68916921e-01  1.16510824e+09\n",
      "   6.33536582e-01  5.30423046e+09  4.08386799e-01  5.01049261e+09\n",
      "   5.93964732e-01  5.38360022e-01  3.34360469e-01  2.62997994e-01]\n",
      " [ 7.65573707e-01  7.24567237e-01  8.02484638e-01  1.14452396e+09\n",
      "   5.62806540e-01  3.45244244e+09  5.94020215e-01  5.59657106e+09\n",
      "   6.15748639e-01  6.65182546e-01  5.40174296e-01  4.02508300e-01]]\n",
      "Policy matrix after 54001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 57001 iterations:\n",
      "[[ 7.52128505e-01  6.84878902e-01  9.05568666e-01  6.94396100e+09\n",
      "   7.92450427e-01  8.55541615e+09  6.99612281e-01  2.26406827e+09\n",
      "   7.31196766e-01  4.53689457e-01  6.09762512e-01 -7.82257138e-01]\n",
      " [ 8.32430125e-01  9.05159892e-01  9.55580642e-01  5.51195807e+09\n",
      "   7.00952068e-01  2.12100012e+09 -6.44512519e-01  2.14481050e+09\n",
      "   3.55205673e-01  2.40924303e-01  1.32199149e-01  1.76778693e-01]\n",
      " [ 6.93186139e-01  8.39234672e-01  6.70834299e-01  1.16510824e+09\n",
      "   6.40158018e-01  5.30423046e+09  4.10956152e-01  5.01049261e+09\n",
      "   5.99507875e-01  5.47327953e-01  3.43484393e-01  2.67622947e-01]\n",
      " [ 7.66822699e-01  7.28496674e-01  8.04059199e-01  1.14452396e+09\n",
      "   5.70890474e-01  3.45244244e+09  5.98969971e-01  5.59657106e+09\n",
      "   6.20979832e-01  6.67621443e-01  5.42699030e-01  4.02875246e-01]]\n",
      "Policy matrix after 57001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 60001 iterations:\n",
      "[[ 7.53140825e-01  6.91602796e-01  9.06414478e-01  6.94396100e+09\n",
      "   7.92115213e-01  8.55541615e+09  6.98050978e-01  2.26406827e+09\n",
      "   7.30923704e-01  4.59622831e-01  6.07315195e-01 -7.82405483e-01]\n",
      " [ 8.32796219e-01  9.04933374e-01  9.55258254e-01  5.51195807e+09\n",
      "   7.05595444e-01  2.12100012e+09 -6.42982518e-01  2.14481050e+09\n",
      "   3.64871369e-01  2.51923938e-01  1.38235952e-01  1.79650807e-01]\n",
      " [ 6.95478583e-01  8.39769502e-01  6.72377292e-01  1.16510824e+09\n",
      "   6.43372990e-01  5.30423046e+09  4.08850302e-01  5.01049261e+09\n",
      "   6.06391000e-01  5.49551027e-01  3.53066587e-01  2.70743843e-01]\n",
      " [ 7.68648676e-01  7.30762130e-01  8.07452067e-01  1.14452396e+09\n",
      "   5.80542597e-01  3.45244244e+09  5.99839173e-01  5.59657106e+09\n",
      "   6.25251451e-01  6.67263752e-01  5.47345408e-01  3.98424714e-01]]\n",
      "Policy matrix after 60001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 63001 iterations:\n",
      "[[ 7.54537883e-01  6.97188871e-01  9.08132313e-01  6.94396100e+09\n",
      "   7.92347354e-01  8.55541615e+09  6.98606820e-01  2.26406827e+09\n",
      "   7.31827534e-01  4.64515149e-01  6.10009042e-01 -7.81662337e-01]\n",
      " [ 8.33509752e-01  9.04707367e-01  9.55126634e-01  5.51195807e+09\n",
      "   7.07501188e-01  2.12100012e+09 -6.44811336e-01  2.14481050e+09\n",
      "   3.73633996e-01  2.70362143e-01  1.45312727e-01  1.85873833e-01]\n",
      " [ 6.99058432e-01  8.38969346e-01  6.77346465e-01  1.16510824e+09\n",
      "   6.46476048e-01  5.30423046e+09  4.08256649e-01  5.01049261e+09\n",
      "   6.12678586e-01  5.53213740e-01  3.68385444e-01  2.76273926e-01]\n",
      " [ 7.70129877e-01  7.35798954e-01  8.10357342e-01  1.14452396e+09\n",
      "   5.88425565e-01  3.45244244e+09  6.00147779e-01  5.59657106e+09\n",
      "   6.30716273e-01  6.68772911e-01  5.53648478e-01  4.00434222e-01]]\n",
      "Policy matrix after 63001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 66001 iterations:\n",
      "[[ 7.53994721e-01  7.02613365e-01  9.06164899e-01  6.94396100e+09\n",
      "   7.92297175e-01  8.55541615e+09  6.99407724e-01  2.26406827e+09\n",
      "   7.31933273e-01  4.68271743e-01  6.13302583e-01 -7.83744235e-01]\n",
      " [ 8.33761185e-01  9.04376622e-01  9.54996070e-01  5.51195807e+09\n",
      "   7.09792955e-01  2.12100012e+09 -6.46583111e-01  2.14481050e+09\n",
      "   3.83427712e-01  2.86889336e-01  1.53708544e-01  1.92546275e-01]\n",
      " [ 7.00195867e-01  8.39210522e-01  6.77583246e-01  1.16510824e+09\n",
      "   6.45671339e-01  5.30423046e+09  4.14429562e-01  5.01049261e+09\n",
      "   6.15923210e-01  5.57477076e-01  3.76774665e-01  2.87481417e-01]\n",
      " [ 7.72841925e-01  7.39852022e-01  8.13965623e-01  1.14452396e+09\n",
      "   5.95184406e-01  3.45244244e+09  6.05571233e-01  5.59657106e+09\n",
      "   6.33339821e-01  6.69816134e-01  5.57714551e-01  4.07740742e-01]]\n",
      "Policy matrix after 66001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 69001 iterations:\n",
      "[[ 7.58080978e-01  7.07566368e-01  9.06775057e-01  6.94396100e+09\n",
      "   7.92395093e-01  8.55541615e+09  6.98471998e-01  2.26406827e+09\n",
      "   7.32207118e-01  4.72715492e-01  6.13942692e-01 -7.81366252e-01]\n",
      " [ 8.34137033e-01  9.04155184e-01  9.54625136e-01  5.51195807e+09\n",
      "   7.11490231e-01  2.12100012e+09 -6.47497282e-01  2.14481050e+09\n",
      "   3.91688731e-01  3.03325213e-01  1.68342579e-01  1.96240224e-01]\n",
      " [ 7.02614121e-01  8.41142901e-01  6.74240793e-01  1.16510824e+09\n",
      "   6.45448044e-01  5.30423046e+09  4.11652365e-01  5.01049261e+09\n",
      "   6.18013580e-01  5.60853756e-01  3.87785458e-01  2.83934742e-01]\n",
      " [ 7.73366148e-01  7.43660080e-01  8.14159827e-01  1.14452396e+09\n",
      "   6.01110481e-01  3.45244244e+09  6.08287529e-01  5.59657106e+09\n",
      "   6.35434896e-01  6.72051985e-01  5.62747544e-01  4.04540123e-01]]\n",
      "Policy matrix after 69001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 72001 iterations:\n",
      "[[ 7.60484478e-01  7.11250493e-01  9.06349703e-01  6.94396100e+09\n",
      "   7.91830824e-01  8.55541615e+09  6.97020556e-01  2.26406827e+09\n",
      "   7.31557907e-01  4.76841959e-01  6.12792707e-01 -7.80317886e-01]\n",
      " [ 8.34232999e-01  9.03741754e-01  9.54353065e-01  5.51195807e+09\n",
      "   7.13386219e-01  2.12100012e+09 -6.48026999e-01  2.14481050e+09\n",
      "   3.98613077e-01  3.17135063e-01  1.75823502e-01  1.92601962e-01]\n",
      " [ 7.05139846e-01  8.42990429e-01  6.80002523e-01  1.16510824e+09\n",
      "   6.46442123e-01  5.30423046e+09  4.14946560e-01  5.01049261e+09\n",
      "   6.20197624e-01  5.66846948e-01  3.93800471e-01  2.81791823e-01]\n",
      " [ 7.75852634e-01  7.49845542e-01  8.13760015e-01  1.14452396e+09\n",
      "   6.05824011e-01  3.45244244e+09  6.09347431e-01  5.59657106e+09\n",
      "   6.39368831e-01  6.72097795e-01  5.64604873e-01  4.01373955e-01]]\n",
      "Policy matrix after 72001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 75001 iterations:\n",
      "[[ 7.60652897e-01  7.14657781e-01  9.08170818e-01  6.94396100e+09\n",
      "   7.92065216e-01  8.55541615e+09  6.97824564e-01  2.26406827e+09\n",
      "   7.31997756e-01  4.80810275e-01  6.13803052e-01 -7.76582737e-01]\n",
      " [ 8.34837265e-01  9.03897265e-01  9.54660903e-01  5.51195807e+09\n",
      "   7.12524048e-01  2.12100012e+09 -6.46567938e-01  2.14481050e+09\n",
      "   4.07212811e-01  3.24491279e-01  1.83846752e-01  1.91405367e-01]\n",
      " [ 7.08660545e-01  8.45168845e-01  6.83511612e-01  1.16510824e+09\n",
      "   6.50638024e-01  5.30423046e+09  4.12509673e-01  5.01049261e+09\n",
      "   6.22913657e-01  5.70160675e-01  3.99077504e-01  2.88259733e-01]\n",
      " [ 7.76488409e-01  7.54469246e-01  8.15065102e-01  1.14452396e+09\n",
      "   6.10031472e-01  3.45244244e+09  6.13489803e-01  5.59657106e+09\n",
      "   6.43709119e-01  6.72954657e-01  5.68945846e-01  4.01134182e-01]]\n",
      "Policy matrix after 75001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 78001 iterations:\n",
      "[[ 7.62089934e-01  7.19008070e-01  9.07632175e-01  6.94396100e+09\n",
      "   7.91682919e-01  8.55541615e+09  6.97797811e-01  2.26406827e+09\n",
      "   7.32140731e-01  4.85202757e-01  6.16262479e-01 -7.74390893e-01]\n",
      " [ 8.34784383e-01  9.03321882e-01  9.54357608e-01  5.51195807e+09\n",
      "   7.11083104e-01  2.12100012e+09 -6.47491013e-01  2.14481050e+09\n",
      "   4.14821303e-01  3.34730896e-01  1.96576049e-01  1.93279391e-01]\n",
      " [ 7.11837655e-01  8.44980072e-01  6.83898487e-01  1.16510824e+09\n",
      "   6.53449025e-01  5.30423046e+09  4.12339732e-01  5.01049261e+09\n",
      "   6.25565604e-01  5.73759484e-01  4.06855355e-01  2.95104780e-01]\n",
      " [ 7.75936035e-01  7.57847183e-01  8.14935178e-01  1.14452396e+09\n",
      "   6.15467653e-01  3.45244244e+09  6.13705851e-01  5.59657106e+09\n",
      "   6.44559987e-01  6.73566206e-01  5.71952785e-01  4.05933949e-01]]\n",
      "Policy matrix after 78001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 81001 iterations:\n",
      "[[ 7.63547010e-01  7.22790448e-01  9.07039523e-01  6.94396100e+09\n",
      "   7.91712014e-01  8.55541615e+09  6.98033670e-01  2.26406827e+09\n",
      "   7.32476004e-01  4.88954038e-01  6.17590477e-01 -7.70606063e-01]\n",
      " [ 8.34976296e-01  9.03047781e-01  9.54268504e-01  5.51195807e+09\n",
      "   7.14293996e-01  2.12100012e+09 -6.45999821e-01  2.14481050e+09\n",
      "   4.21543554e-01  3.45400146e-01  2.05106817e-01  1.91241598e-01]\n",
      " [ 7.12558625e-01  8.44639970e-01  6.87961602e-01  1.16510824e+09\n",
      "   6.55429738e-01  5.30423046e+09  4.06852523e-01  5.01049261e+09\n",
      "   6.28410173e-01  5.76169843e-01  4.08103518e-01  2.98838003e-01]\n",
      " [ 7.75403161e-01  7.59085242e-01  8.14981787e-01  1.14452396e+09\n",
      "   6.21136721e-01  3.45244244e+09  6.19855462e-01  5.59657106e+09\n",
      "   6.46551820e-01  6.74248226e-01  5.76779595e-01  4.06592493e-01]]\n",
      "Policy matrix after 81001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 84001 iterations:\n",
      "[[ 7.64498020e-01  7.26808211e-01  9.07502344e-01  6.94396100e+09\n",
      "   7.91593883e-01  8.55541615e+09  6.97262642e-01  2.26406827e+09\n",
      "   7.32574994e-01  4.92404340e-01  6.16847906e-01 -7.68931824e-01]\n",
      " [ 8.35165193e-01  9.02876168e-01  9.54225087e-01  5.51195807e+09\n",
      "   7.12384675e-01  2.12100012e+09 -6.45455404e-01  2.14481050e+09\n",
      "   4.27063977e-01  3.55144993e-01  2.09435164e-01  1.88875942e-01]\n",
      " [ 7.13685769e-01  8.45043341e-01  6.87899118e-01  1.16510824e+09\n",
      "   6.57823087e-01  5.30423046e+09  4.07549181e-01  5.01049261e+09\n",
      "   6.30802530e-01  5.78361021e-01  4.10833416e-01  3.01614307e-01]\n",
      " [ 7.77172363e-01  7.60716709e-01  8.16572000e-01  1.14452396e+09\n",
      "   6.25094827e-01  3.45244244e+09  6.24484560e-01  5.59657106e+09\n",
      "   6.50396302e-01  6.74089856e-01  5.77275382e-01  4.05275555e-01]]\n",
      "Policy matrix after 84001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 87001 iterations:\n",
      "[[ 7.65998124e-01  7.29776508e-01  9.07948777e-01  6.94396100e+09\n",
      "   7.91690053e-01  8.55541615e+09  6.96476491e-01  2.26406827e+09\n",
      "   7.32703406e-01  4.94941463e-01  6.15985488e-01 -7.66902649e-01]\n",
      " [ 8.35639560e-01  9.02987276e-01  9.54255302e-01  5.51195807e+09\n",
      "   7.13284299e-01  2.12100012e+09 -6.42942917e-01  2.14481050e+09\n",
      "   4.34030630e-01  3.61624318e-01  2.14075208e-01  1.87620951e-01]\n",
      " [ 7.13924482e-01  8.46138100e-01  6.87423557e-01  1.16510824e+09\n",
      "   6.59067410e-01  5.30423046e+09  4.09810391e-01  5.01049261e+09\n",
      "   6.33736781e-01  5.80485455e-01  4.14753601e-01  3.03160640e-01]\n",
      " [ 7.78163581e-01  7.63953451e-01  8.18295906e-01  1.14452396e+09\n",
      "   6.30048151e-01  3.45244244e+09  6.28235585e-01  5.59657106e+09\n",
      "   6.53305862e-01  6.74615277e-01  5.77406090e-01  4.04270727e-01]]\n",
      "Policy matrix after 87001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 90001 iterations:\n",
      "[[ 7.67347303e-01  7.33341349e-01  9.08356313e-01  6.94396100e+09\n",
      "   7.92072758e-01  8.55541615e+09  6.97265844e-01  2.26406827e+09\n",
      "   7.32820345e-01  4.96358663e-01  6.16851310e-01 -7.61994239e-01]\n",
      " [ 8.36350109e-01  9.03237820e-01  9.54447135e-01  5.51195807e+09\n",
      "   7.15889255e-01  2.12100012e+09 -6.41552367e-01  2.14481050e+09\n",
      "   4.39906366e-01  3.66427287e-01  2.23801115e-01  1.90570121e-01]\n",
      " [ 7.16770279e-01  8.45777350e-01  6.90250294e-01  1.16510824e+09\n",
      "   6.61522199e-01  5.30423046e+09  4.13852847e-01  5.01049261e+09\n",
      "   6.35676224e-01  5.83613411e-01  4.19464607e-01  3.04009116e-01]\n",
      " [ 7.79192356e-01  7.66124599e-01  8.19123228e-01  1.14452396e+09\n",
      "   6.35251019e-01  3.45244244e+09  6.29632614e-01  5.59657106e+09\n",
      "   6.54654153e-01  6.74863729e-01  5.81405898e-01  4.04810555e-01]]\n",
      "Policy matrix after 90001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 93001 iterations:\n",
      "[[ 7.69778233e-01  7.36054818e-01  9.08427127e-01  6.94396100e+09\n",
      "   7.92176150e-01  8.55541615e+09  6.97311109e-01  2.26406827e+09\n",
      "   7.33199984e-01  4.98444370e-01  6.17040722e-01 -7.61031485e-01]\n",
      " [ 8.36632419e-01  9.03286540e-01  9.54533027e-01  5.51195807e+09\n",
      "   7.16473847e-01  2.12100012e+09 -6.38583839e-01  2.14481050e+09\n",
      "   4.46806246e-01  3.70575610e-01  2.30719855e-01  1.91077447e-01]\n",
      " [ 7.17607554e-01  8.45825162e-01  6.89613986e-01  1.16510824e+09\n",
      "   6.63942113e-01  5.30423046e+09  4.16870736e-01  5.01049261e+09\n",
      "   6.36895484e-01  5.86423530e-01  4.20385995e-01  3.05436332e-01]\n",
      " [ 7.79088056e-01  7.67598123e-01  8.22013981e-01  1.14452396e+09\n",
      "   6.38289346e-01  3.45244244e+09  6.33676772e-01  5.59657106e+09\n",
      "   6.55923667e-01  6.75650202e-01  5.83414159e-01  4.04908482e-01]]\n",
      "Policy matrix after 93001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 96001 iterations:\n",
      "[[ 7.72394808e-01  7.38914850e-01  9.08911912e-01  6.94396100e+09\n",
      "   7.92223981e-01  8.55541615e+09  6.97159166e-01  2.26406827e+09\n",
      "   7.33111027e-01  5.00755123e-01  6.17099965e-01 -7.58300286e-01]\n",
      " [ 8.37003679e-01  9.03316834e-01  9.54629332e-01  5.51195807e+09\n",
      "   7.17322089e-01  2.12100012e+09 -6.38593802e-01  2.14481050e+09\n",
      "   4.51356407e-01  3.75950780e-01  2.39546276e-01  2.00798597e-01]\n",
      " [ 7.19818343e-01  8.47665815e-01  6.89436557e-01  1.16510824e+09\n",
      "   6.61708194e-01  5.30423046e+09  4.17433194e-01  5.01049261e+09\n",
      "   6.39794278e-01  5.87583790e-01  4.21595964e-01  3.05414828e-01]\n",
      " [ 7.80594173e-01  7.69106984e-01  8.21836822e-01  1.14452396e+09\n",
      "   6.41452138e-01  3.45244244e+09  6.32694457e-01  5.59657106e+09\n",
      "   6.58235312e-01  6.76090793e-01  5.86148279e-01  4.06689403e-01]]\n",
      "Policy matrix after 96001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 99001 iterations:\n",
      "[[ 7.73958661e-01  7.43069785e-01  9.09177055e-01  6.94396100e+09\n",
      "   7.92763673e-01  8.55541615e+09  6.98299873e-01  2.26406827e+09\n",
      "   7.33570161e-01  5.03109781e-01  6.18218508e-01 -7.56802478e-01]\n",
      " [ 8.37764273e-01  9.03725977e-01  9.54938485e-01  5.51195807e+09\n",
      "   7.19717988e-01  2.12100012e+09 -6.35179723e-01  2.14481050e+09\n",
      "   4.56231915e-01  3.79745131e-01  2.41716542e-01  2.01236879e-01]\n",
      " [ 7.21402223e-01  8.48421673e-01  6.87747966e-01  1.16510824e+09\n",
      "   6.63726306e-01  5.30423046e+09  4.21117762e-01  5.01049261e+09\n",
      "   6.41831820e-01  5.89841606e-01  4.26624790e-01  3.08888238e-01]\n",
      " [ 7.81961697e-01  7.69482303e-01  8.23460713e-01  1.14452396e+09\n",
      "   6.44377395e-01  3.45244244e+09  6.36139669e-01  5.59657106e+09\n",
      "   6.60504398e-01  6.76700280e-01  5.86779072e-01  4.06639973e-01]]\n",
      "Policy matrix after 99001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 102001 iterations:\n",
      "[[ 7.75840221e-01  7.45609775e-01  9.09019644e-01  6.94396100e+09\n",
      "   7.92946020e-01  8.55541615e+09  6.99912972e-01  2.26406827e+09\n",
      "   7.34067746e-01  5.04934105e-01  6.20064506e-01 -7.55120397e-01]\n",
      " [ 8.38151430e-01  9.03887781e-01  9.55007141e-01  5.51195807e+09\n",
      "   7.21269406e-01  2.12100012e+09 -6.32794833e-01  2.14481050e+09\n",
      "   4.60823864e-01  3.90693060e-01  2.49126394e-01  2.02939934e-01]\n",
      " [ 7.22361526e-01  8.49144924e-01  6.89625235e-01  1.16510824e+09\n",
      "   6.65041015e-01  5.30423046e+09  4.23958014e-01  5.01049261e+09\n",
      "   6.44293622e-01  5.91781501e-01  4.31174187e-01  3.11306111e-01]\n",
      " [ 7.81349101e-01  7.70493258e-01  8.25385386e-01  1.14452396e+09\n",
      "   6.47994718e-01  3.45244244e+09  6.38777126e-01  5.59657106e+09\n",
      "   6.61347093e-01  6.77315450e-01  5.89676440e-01  4.07762335e-01]]\n",
      "Policy matrix after 102001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 105001 iterations:\n",
      "[[ 7.77269501e-01  7.48218067e-01  9.09298885e-01  6.94396100e+09\n",
      "   7.92976245e-01  8.55541615e+09  7.01171698e-01  2.26406827e+09\n",
      "   7.34064351e-01  5.07536893e-01  6.21975391e-01 -7.54003710e-01]\n",
      " [ 8.38586457e-01  9.04095297e-01  9.55322236e-01  5.51195807e+09\n",
      "   7.22834824e-01  2.12100012e+09 -6.33594152e-01  2.14481050e+09\n",
      "   4.65172532e-01  3.97408856e-01  2.58532473e-01  2.05596018e-01]\n",
      " [ 7.24959275e-01  8.49612085e-01  6.90696609e-01  1.16510824e+09\n",
      "   6.65452594e-01  5.30423046e+09  4.26166095e-01  5.01049261e+09\n",
      "   6.45135957e-01  5.94733737e-01  4.32008633e-01  3.14266300e-01]\n",
      " [ 7.83636261e-01  7.72389042e-01  8.26098027e-01  1.14452396e+09\n",
      "   6.50605156e-01  3.45244244e+09  6.39065965e-01  5.59657106e+09\n",
      "   6.63057848e-01  6.77359109e-01  5.91031271e-01  4.11623771e-01]]\n",
      "Policy matrix after 105001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 108001 iterations:\n",
      "[[ 7.78026671e-01  7.51046701e-01  9.09695451e-01  6.94396100e+09\n",
      "   7.92952877e-01  8.55541615e+09  7.00504509e-01  2.26406827e+09\n",
      "   7.34354725e-01  5.10469927e-01  6.20972847e-01 -7.49998293e-01]\n",
      " [ 8.38840535e-01  9.04118356e-01  9.55255790e-01  5.51195807e+09\n",
      "   7.23738564e-01  2.12100012e+09 -6.31522566e-01  2.14481050e+09\n",
      "   4.69608248e-01  4.00797823e-01  2.62563603e-01  2.09446127e-01]\n",
      " [ 7.26491373e-01  8.47200425e-01  6.93206779e-01  1.16510824e+09\n",
      "   6.67517793e-01  5.30423046e+09  4.29494514e-01  5.01049261e+09\n",
      "   6.45202122e-01  5.97665985e-01  4.31742047e-01  3.12655114e-01]\n",
      " [ 7.84806140e-01  7.74144147e-01  8.27788124e-01  1.14452396e+09\n",
      "   6.52785858e-01  3.45244244e+09  6.36588500e-01  5.59657106e+09\n",
      "   6.63585787e-01  6.78078973e-01  5.94826744e-01  4.09146258e-01]]\n",
      "Policy matrix after 108001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 111001 iterations:\n",
      "[[ 7.79002471e-01  7.54140079e-01  9.09648780e-01  6.94396100e+09\n",
      "   7.92754881e-01  8.55541615e+09  7.00008615e-01  2.26406827e+09\n",
      "   7.34353552e-01  5.12910758e-01  6.21420078e-01 -7.47674771e-01]\n",
      " [ 8.38870364e-01  9.03992980e-01  9.55182718e-01  5.51195807e+09\n",
      "   7.24297912e-01  2.12100012e+09 -6.30798025e-01  2.14481050e+09\n",
      "   4.72756691e-01  4.05247008e-01  2.64517798e-01  2.06975710e-01]\n",
      " [ 7.26564833e-01  8.47620575e-01  6.93412107e-01  1.16510824e+09\n",
      "   6.69538881e-01  5.30423046e+09  4.35110770e-01  5.01049261e+09\n",
      "   6.44948265e-01  5.98350002e-01  4.35809214e-01  3.14578511e-01]\n",
      " [ 7.86227176e-01  7.74744518e-01  8.27881480e-01  1.14452396e+09\n",
      "   6.54715324e-01  3.45244244e+09  6.35058208e-01  5.59657106e+09\n",
      "   6.63387570e-01  6.78403677e-01  5.94102179e-01  4.07677939e-01]]\n",
      "Policy matrix after 111001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 114001 iterations:\n",
      "[[ 7.79378727e-01  7.57165315e-01  9.09264309e-01  6.94396100e+09\n",
      "   7.93288594e-01  8.55541615e+09  7.00657750e-01  2.26406827e+09\n",
      "   7.34913697e-01  5.15839692e-01  6.21987699e-01 -7.48084615e-01]\n",
      " [ 8.39435136e-01  9.04344592e-01  9.55364734e-01  5.51195807e+09\n",
      "   7.24587178e-01  2.12100012e+09 -6.31144712e-01  2.14481050e+09\n",
      "   4.76506884e-01  4.08471080e-01  2.71997915e-01  2.08001933e-01]\n",
      " [ 7.28315128e-01  8.47551730e-01  6.94330276e-01  1.16510824e+09\n",
      "   6.71184004e-01  5.30423046e+09  4.37254976e-01  5.01049261e+09\n",
      "   6.46986206e-01  6.00385278e-01  4.38991235e-01  3.16855309e-01]\n",
      " [ 7.86608777e-01  7.76876994e-01  8.29772074e-01  1.14452396e+09\n",
      "   6.56974795e-01  3.45244244e+09  6.34143067e-01  5.59657106e+09\n",
      "   6.64463475e-01  6.79179486e-01  5.96042944e-01  4.08231687e-01]]\n",
      "Policy matrix after 114001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 117001 iterations:\n",
      "[[ 7.80190151e-01  7.59306246e-01  9.09559194e-01  6.94396100e+09\n",
      "   7.93361472e-01  8.55541615e+09  6.99189532e-01  2.26406827e+09\n",
      "   7.35124155e-01  5.18263581e-01  6.21009134e-01 -7.46396466e-01]\n",
      " [ 8.39626048e-01  9.04139374e-01  9.55200660e-01  5.51195807e+09\n",
      "   7.26210311e-01  2.12100012e+09 -6.31561857e-01  2.14481050e+09\n",
      "   4.80920393e-01  4.13097693e-01  2.74218217e-01  2.09855130e-01]\n",
      " [ 7.29018798e-01  8.48357896e-01  6.93229179e-01  1.16510824e+09\n",
      "   6.71449336e-01  5.30423046e+09  4.40399959e-01  5.01049261e+09\n",
      "   6.47312788e-01  6.01400356e-01  4.43643003e-01  3.17273530e-01]\n",
      " [ 7.86418911e-01  7.78182744e-01  8.28228008e-01  1.14452396e+09\n",
      "   6.58395929e-01  3.45244244e+09  6.30509933e-01  5.59657106e+09\n",
      "   6.65566496e-01  6.79863132e-01  5.95645459e-01  4.06419683e-01]]\n",
      "Policy matrix after 117001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 120001 iterations:\n",
      "[[ 7.81457400e-01  7.62030786e-01  9.09335232e-01  6.94396100e+09\n",
      "   7.93182030e-01  8.55541615e+09  7.00350304e-01  2.26406827e+09\n",
      "   7.35048976e-01  5.20899583e-01  6.22678408e-01 -7.44001986e-01]\n",
      " [ 8.39602963e-01  9.03928349e-01  9.55200828e-01  5.51195807e+09\n",
      "   7.26776567e-01  2.12100012e+09 -6.30590534e-01  2.14481050e+09\n",
      "   4.83466297e-01  4.18319670e-01  2.78307877e-01  2.15857194e-01]\n",
      " [ 7.29437226e-01  8.48131708e-01  6.96185245e-01  1.16510824e+09\n",
      "   6.72913538e-01  5.30423046e+09  4.39216043e-01  5.01049261e+09\n",
      "   6.48183996e-01  6.00947520e-01  4.48633497e-01  3.19089606e-01]\n",
      " [ 7.86133439e-01  7.78746488e-01  8.29632440e-01  1.14452396e+09\n",
      "   6.61138166e-01  3.45244244e+09  6.29596591e-01  5.59657106e+09\n",
      "   6.66053978e-01  6.79800988e-01  5.95914922e-01  4.08313552e-01]]\n",
      "Policy matrix after 120001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 123001 iterations:\n",
      "[[ 7.82514658e-01  7.64414960e-01  9.08095922e-01  6.94396100e+09\n",
      "   7.93483937e-01  8.55541615e+09  7.00189676e-01  2.26406827e+09\n",
      "   7.35358207e-01  5.22162817e-01  6.22325720e-01 -7.42009467e-01]\n",
      " [ 8.39978137e-01  9.04172384e-01  9.55251613e-01  5.51195807e+09\n",
      "   7.27410222e-01  2.12100012e+09 -6.32162082e-01  2.14481050e+09\n",
      "   4.86736671e-01  4.21568172e-01  2.84338308e-01  2.18758358e-01]\n",
      " [ 7.30801892e-01  8.48324869e-01  6.96988104e-01  1.16510824e+09\n",
      "   6.74924903e-01  5.30423046e+09  4.41080661e-01  5.01049261e+09\n",
      "   6.49512233e-01  6.03334793e-01  4.51026662e-01  3.22219529e-01]\n",
      " [ 7.87372240e-01  7.79652787e-01  8.30797694e-01  1.14452396e+09\n",
      "   6.63625242e-01  3.45244244e+09  6.30144201e-01  5.59657106e+09\n",
      "   6.67244920e-01  6.80289266e-01  5.96636215e-01  4.09556188e-01]]\n",
      "Policy matrix after 123001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 126001 iterations:\n",
      "[[ 7.83558454e-01  7.66293486e-01  9.08582103e-01  6.94396100e+09\n",
      "   7.93502166e-01  8.55541615e+09  7.00461378e-01  2.26406827e+09\n",
      "   7.35550701e-01  5.24156712e-01  6.22666155e-01 -7.39912132e-01]\n",
      " [ 8.40119779e-01  9.04205766e-01  9.55415480e-01  5.51195807e+09\n",
      "   7.28031686e-01  2.12100012e+09 -6.32252009e-01  2.14481050e+09\n",
      "   4.89961523e-01  4.24642823e-01  2.87888004e-01  2.19235238e-01]\n",
      " [ 7.32030091e-01  8.48527568e-01  6.96221697e-01  1.16510824e+09\n",
      "   6.74387040e-01  5.30423046e+09  4.40419007e-01  5.01049261e+09\n",
      "   6.51443009e-01  6.03504935e-01  4.55845263e-01  3.21764497e-01]\n",
      " [ 7.87784013e-01  7.80668147e-01  8.30080909e-01  1.14452396e+09\n",
      "   6.64409505e-01  3.45244244e+09  6.31312456e-01  5.59657106e+09\n",
      "   6.68848370e-01  6.80724070e-01  5.98413161e-01  4.09322520e-01]]\n",
      "Policy matrix after 126001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 129001 iterations:\n",
      "[[ 7.83969096e-01  7.67672371e-01  9.08621046e-01  6.94396100e+09\n",
      "   7.93531494e-01  8.55541615e+09  7.00520479e-01  2.26406827e+09\n",
      "   7.35484530e-01  5.26061210e-01  6.22866874e-01 -7.40042745e-01]\n",
      " [ 8.40310910e-01  9.04189010e-01  9.55450101e-01  5.51195807e+09\n",
      "   7.28958031e-01  2.12100012e+09 -6.32370150e-01  2.14481050e+09\n",
      "   4.92922754e-01  4.30354631e-01  2.91313961e-01  2.18476949e-01]\n",
      " [ 7.33172516e-01  8.49077584e-01  6.94240797e-01  1.16510824e+09\n",
      "   6.75979523e-01  5.30423046e+09  4.38271926e-01  5.01049261e+09\n",
      "   6.51729726e-01  6.04100836e-01  4.57260875e-01  3.23567663e-01]\n",
      " [ 7.88235078e-01  7.81600369e-01  8.30173689e-01  1.14452396e+09\n",
      "   6.66483315e-01  3.45244244e+09  6.32058169e-01  5.59657106e+09\n",
      "   6.69322313e-01  6.80608009e-01  5.99369779e-01  4.10069439e-01]]\n",
      "Policy matrix after 129001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 132001 iterations:\n",
      "[[ 7.84849480e-01  7.69723006e-01  9.08426188e-01  6.94396100e+09\n",
      "   7.94047136e-01  8.55541615e+09  7.00717783e-01  2.26406827e+09\n",
      "   7.35995386e-01  5.28401122e-01  6.23029076e-01 -7.37243273e-01]\n",
      " [ 8.40947251e-01  9.04628661e-01  9.55697485e-01  5.51195807e+09\n",
      "   7.29899053e-01  2.12100012e+09 -6.34776736e-01  2.14481050e+09\n",
      "   4.96709142e-01  4.33302314e-01  2.94789356e-01  2.20591900e-01]\n",
      " [ 7.34326557e-01  8.49603150e-01  6.95611596e-01  1.16510824e+09\n",
      "   6.77890592e-01  5.30423046e+09  4.39083157e-01  5.01049261e+09\n",
      "   6.52319600e-01  6.04673409e-01  4.58484752e-01  3.22764588e-01]\n",
      " [ 7.89359471e-01  7.83178732e-01  8.30490960e-01  1.14452396e+09\n",
      "   6.69434699e-01  3.45244244e+09  6.30501096e-01  5.59657106e+09\n",
      "   6.71039906e-01  6.81145643e-01  6.00817214e-01  4.10118495e-01]]\n",
      "Policy matrix after 132001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 135001 iterations:\n",
      "[[ 7.86229596e-01  7.71299939e-01  9.08080663e-01  6.94396100e+09\n",
      "   7.94017251e-01  8.55541615e+09  7.00069926e-01  2.26406827e+09\n",
      "   7.35984843e-01  5.30898209e-01  6.22492187e-01 -7.36098093e-01]\n",
      " [ 8.40943229e-01  9.04638779e-01  9.55705490e-01  5.51195807e+09\n",
      "   7.30969552e-01  2.12100012e+09 -6.33480475e-01  2.14481050e+09\n",
      "   4.99972467e-01  4.36884566e-01  2.97394072e-01  2.21428284e-01]\n",
      " [ 7.35131186e-01  8.50513248e-01  6.94864436e-01  1.16510824e+09\n",
      "   6.79330586e-01  5.30423046e+09  4.37694998e-01  5.01049261e+09\n",
      "   6.53287292e-01  6.05279365e-01  4.59089871e-01  3.22924002e-01]\n",
      " [ 7.88201208e-01  7.83332075e-01  8.31694810e-01  1.14452396e+09\n",
      "   6.70940855e-01  3.45244244e+09  6.32807196e-01  5.59657106e+09\n",
      "   6.71121764e-01  6.81356466e-01  6.01910213e-01  4.09153515e-01]]\n",
      "Policy matrix after 135001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 138001 iterations:\n",
      "[[ 7.87255371e-01  7.71731912e-01  9.09062541e-01  6.94396100e+09\n",
      "   7.94208543e-01  8.55541615e+09  6.99732092e-01  2.26406827e+09\n",
      "   7.36123788e-01  5.31723753e-01  6.21823287e-01 -7.32942162e-01]\n",
      " [ 8.41250576e-01  9.04680924e-01  9.55730417e-01  5.51195807e+09\n",
      "   7.32432262e-01  2.12100012e+09 -6.34365839e-01  2.14481050e+09\n",
      "   5.03050255e-01  4.38296532e-01  2.99814417e-01  2.23919176e-01]\n",
      " [ 7.35730991e-01  8.51460752e-01  6.97286090e-01  1.16510824e+09\n",
      "   6.80323344e-01  5.30423046e+09  4.38814661e-01  5.01049261e+09\n",
      "   6.53983658e-01  6.06690577e-01  4.60677267e-01  3.21568980e-01]\n",
      " [ 7.89097056e-01  7.83944592e-01  8.31573110e-01  1.14452396e+09\n",
      "   6.72805922e-01  3.45244244e+09  6.32410066e-01  5.59657106e+09\n",
      "   6.71021132e-01  6.81744075e-01  6.03466747e-01  4.07544529e-01]]\n",
      "Policy matrix after 138001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 141001 iterations:\n",
      "[[ 7.88548053e-01  7.73534072e-01  9.08716660e-01  6.94396100e+09\n",
      "   7.94267848e-01  8.55541615e+09  6.99450463e-01  2.26406827e+09\n",
      "   7.36399650e-01  5.33810719e-01  6.21950707e-01 -7.33033926e-01]\n",
      " [ 8.41421591e-01  9.04727677e-01  9.55832455e-01  5.51195807e+09\n",
      "   7.32912595e-01  2.12100012e+09 -6.37326607e-01  2.14481050e+09\n",
      "   5.06175983e-01  4.42126971e-01  3.02701945e-01  2.24726083e-01]\n",
      " [ 7.36682418e-01  8.51628792e-01  6.93327456e-01  1.16510824e+09\n",
      "   6.80649618e-01  5.30423046e+09  4.37198993e-01  5.01049261e+09\n",
      "   6.55386017e-01  6.07257471e-01  4.62999536e-01  3.23553348e-01]\n",
      " [ 7.89229608e-01  7.85214738e-01  8.32671634e-01  1.14452396e+09\n",
      "   6.73845502e-01  3.45244244e+09  6.34600899e-01  5.59657106e+09\n",
      "   6.71965292e-01  6.81786383e-01  6.02253166e-01  4.08186107e-01]]\n",
      "Policy matrix after 141001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 144001 iterations:\n",
      "[[ 7.89782968e-01  7.74971036e-01  9.08906415e-01  6.94396100e+09\n",
      "   7.94303703e-01  8.55541615e+09  6.98213329e-01  2.26406827e+09\n",
      "   7.36441990e-01  5.35682351e-01  6.20759549e-01 -7.34069246e-01]\n",
      " [ 8.41619262e-01  9.04719135e-01  9.55738575e-01  5.51195807e+09\n",
      "   7.33950963e-01  2.12100012e+09 -6.39906093e-01  2.14481050e+09\n",
      "   5.10079569e-01  4.44273430e-01  3.05869803e-01  2.25932378e-01]\n",
      " [ 7.36916785e-01  8.51557878e-01  6.93031573e-01  1.16510824e+09\n",
      "   6.81185775e-01  5.30423046e+09  4.36590602e-01  5.01049261e+09\n",
      "   6.56610824e-01  6.06396993e-01  4.61076617e-01  3.25251811e-01]\n",
      " [ 7.90071860e-01  7.86354279e-01  8.32490475e-01  1.14452396e+09\n",
      "   6.75972180e-01  3.45244244e+09  6.32526513e-01  5.59657106e+09\n",
      "   6.72178672e-01  6.82130033e-01  6.02561871e-01  4.08473830e-01]]\n",
      "Policy matrix after 144001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 147001 iterations:\n",
      "[[ 7.90324602e-01  7.75524762e-01  9.09127419e-01  6.94396100e+09\n",
      "   7.94085645e-01  8.55541615e+09  6.97855072e-01  2.26406827e+09\n",
      "   7.36325472e-01  5.36156958e-01  6.21310352e-01 -7.34930519e-01]\n",
      " [ 8.41549744e-01  9.04477825e-01  9.55664678e-01  5.51195807e+09\n",
      "   7.34040339e-01  2.12100012e+09 -6.40306198e-01  2.14481050e+09\n",
      "   5.12930948e-01  4.45631411e-01  3.06876290e-01  2.27396155e-01]\n",
      " [ 7.37626199e-01  8.51672260e-01  6.92916789e-01  1.16510824e+09\n",
      "   6.81452550e-01  5.30423046e+09  4.37449382e-01  5.01049261e+09\n",
      "   6.57734226e-01  6.07224674e-01  4.62274735e-01  3.27416625e-01]\n",
      " [ 7.90359812e-01  7.86899898e-01  8.31938260e-01  1.14452396e+09\n",
      "   6.76786069e-01  3.45244244e+09  6.32921869e-01  5.59657106e+09\n",
      "   6.73410887e-01  6.81912161e-01  6.01793187e-01  4.09567650e-01]]\n",
      "Policy matrix after 147001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 150001 iterations:\n",
      "[[ 7.90744462e-01  7.77628648e-01  9.08770986e-01  6.94396100e+09\n",
      "   7.94438899e-01  8.55541615e+09  6.97768182e-01  2.26406827e+09\n",
      "   7.36633267e-01  5.37541206e-01  6.21634035e-01 -7.35083065e-01]\n",
      " [ 8.41891315e-01  9.04663036e-01  9.55809639e-01  5.51195807e+09\n",
      "   7.34768401e-01  2.12100012e+09 -6.41325352e-01  2.14481050e+09\n",
      "   5.14916995e-01  4.48443631e-01  3.10633258e-01  2.27867884e-01]\n",
      " [ 7.38764456e-01  8.52662606e-01  6.91268803e-01  1.16510824e+09\n",
      "   6.81847131e-01  5.30423046e+09  4.37490424e-01  5.01049261e+09\n",
      "   6.58673878e-01  6.08350290e-01  4.65360777e-01  3.27843963e-01]\n",
      " [ 7.90823499e-01  7.87033175e-01  8.31866083e-01  1.14452396e+09\n",
      "   6.78806481e-01  3.45244244e+09  6.33114176e-01  5.59657106e+09\n",
      "   6.74836636e-01  6.82315165e-01  6.02577775e-01  4.09365778e-01]]\n",
      "Policy matrix after 150001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 153001 iterations:\n",
      "[[ 7.90833989e-01  7.77963049e-01  9.08889880e-01  6.94396100e+09\n",
      "   7.94562241e-01  8.55541615e+09  6.97236056e-01  2.26406827e+09\n",
      "   7.36809344e-01  5.38609437e-01  6.21675105e-01 -7.34325092e-01]\n",
      " [ 8.41988065e-01  9.04569591e-01  9.55692606e-01  5.51195807e+09\n",
      "   7.35265191e-01  2.12100012e+09 -6.39556726e-01  2.14481050e+09\n",
      "   5.16948076e-01  4.48423518e-01  3.12640276e-01  2.29676963e-01]\n",
      " [ 7.39218348e-01  8.53426349e-01  6.92393748e-01  1.16510824e+09\n",
      "   6.83135833e-01  5.30423046e+09  4.37874811e-01  5.01049261e+09\n",
      "   6.60105154e-01  6.09282759e-01  4.67649714e-01  3.29529702e-01]\n",
      " [ 7.90030813e-01  7.87871959e-01  8.31730789e-01  1.14452396e+09\n",
      "   6.80535373e-01  3.45244244e+09  6.34148262e-01  5.59657106e+09\n",
      "   6.76295597e-01  6.82714489e-01  6.03446738e-01  4.09652044e-01]]\n",
      "Policy matrix after 153001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 156001 iterations:\n",
      "[[ 7.90660925e-01  7.79289587e-01  9.08508910e-01  6.94396100e+09\n",
      "   7.94624918e-01  8.55541615e+09  6.97304479e-01  2.26406827e+09\n",
      "   7.36881794e-01  5.39591965e-01  6.21821265e-01 -7.34375907e-01]\n",
      " [ 8.42103913e-01  9.04627200e-01  9.55762862e-01  5.51195807e+09\n",
      "   7.34974555e-01  2.12100012e+09 -6.37064280e-01  2.14481050e+09\n",
      "   5.19966342e-01  4.52180399e-01  3.13094213e-01  2.31137703e-01]\n",
      " [ 7.40393758e-01  8.53775979e-01  6.94063199e-01  1.16510824e+09\n",
      "   6.82602846e-01  5.30423046e+09  4.35217471e-01  5.01049261e+09\n",
      "   6.60330360e-01  6.10435221e-01  4.71524273e-01  3.28806432e-01]\n",
      " [ 7.89303324e-01  7.88824264e-01  8.31812619e-01  1.14452396e+09\n",
      "   6.82495093e-01  3.45244244e+09  6.34371725e-01  5.59657106e+09\n",
      "   6.76227159e-01  6.82920907e-01  6.04753452e-01  4.08949773e-01]]\n",
      "Policy matrix after 156001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 159001 iterations:\n",
      "[[ 7.91688916e-01  7.81167611e-01  9.08658539e-01  6.94396100e+09\n",
      "   7.94716453e-01  8.55541615e+09  6.97630228e-01  2.26406827e+09\n",
      "   7.36963513e-01  5.41506193e-01  6.21640705e-01 -7.31754172e-01]\n",
      " [ 8.42255468e-01  9.04683346e-01  9.55756971e-01  5.51195807e+09\n",
      "   7.35362714e-01  2.12100012e+09 -6.37139423e-01  2.14481050e+09\n",
      "   5.22272633e-01  4.51215898e-01  3.13997667e-01  2.33072029e-01]\n",
      " [ 7.40759463e-01  8.53593774e-01  6.95442562e-01  1.16510824e+09\n",
      "   6.82690005e-01  5.30423046e+09  4.35362114e-01  5.01049261e+09\n",
      "   6.61093812e-01  6.10709301e-01  4.72358834e-01  3.28486234e-01]\n",
      " [ 7.89240162e-01  7.89414897e-01  8.31540039e-01  1.14452396e+09\n",
      "   6.84342311e-01  3.45244244e+09  6.38373783e-01  5.59657106e+09\n",
      "   6.77086241e-01  6.82979286e-01  6.06220100e-01  4.08863780e-01]]\n",
      "Policy matrix after 159001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 162001 iterations:\n",
      "[[ 7.91236568e-01  7.82751074e-01  9.09272066e-01  6.94396100e+09\n",
      "   7.94702868e-01  8.55541615e+09  6.98056363e-01  2.26406827e+09\n",
      "   7.36884077e-01  5.41970296e-01  6.21590067e-01 -7.32251395e-01]\n",
      " [ 8.42339062e-01  9.04757214e-01  9.55829376e-01  5.51195807e+09\n",
      "   7.36268400e-01  2.12100012e+09 -6.37035277e-01  2.14481050e+09\n",
      "   5.25223566e-01  4.54475404e-01  3.15790612e-01  2.36136232e-01]\n",
      " [ 7.41732245e-01  8.54004678e-01  6.95674170e-01  1.16510824e+09\n",
      "   6.83636980e-01  5.30423046e+09  4.34110812e-01  5.01049261e+09\n",
      "   6.61937203e-01  6.11008316e-01  4.73157445e-01  3.27888833e-01]\n",
      " [ 7.90097579e-01  7.90645259e-01  8.32465594e-01  1.14452396e+09\n",
      "   6.86390821e-01  3.45244244e+09  6.40571032e-01  5.59657106e+09\n",
      "   6.76920658e-01  6.82996390e-01  6.08092382e-01  4.08263785e-01]]\n",
      "Policy matrix after 162001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 165001 iterations:\n",
      "[[ 7.91720853e-01  7.83943152e-01  9.09273169e-01  6.94396100e+09\n",
      "   7.94807955e-01  8.55541615e+09  6.98658863e-01  2.26406827e+09\n",
      "   7.37008837e-01  5.42904230e-01  6.21874305e-01 -7.30743241e-01]\n",
      " [ 8.42522529e-01  9.04904904e-01  9.55876801e-01  5.51195807e+09\n",
      "   7.36651771e-01  2.12100012e+09 -6.37488026e-01  2.14481050e+09\n",
      "   5.27996117e-01  4.56048254e-01  3.15289672e-01  2.34586983e-01]\n",
      " [ 7.42622623e-01  8.54753157e-01  6.95428102e-01  1.16510824e+09\n",
      "   6.84743599e-01  5.30423046e+09  4.35476410e-01  5.01049261e+09\n",
      "   6.62693902e-01  6.11229292e-01  4.76759352e-01  3.30093701e-01]\n",
      " [ 7.90591467e-01  7.91651961e-01  8.33489585e-01  1.14452396e+09\n",
      "   6.87723618e-01  3.45244244e+09  6.43217079e-01  5.59657106e+09\n",
      "   6.77713250e-01  6.83136061e-01  6.08967210e-01  4.08639614e-01]]\n",
      "Policy matrix after 165001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 168001 iterations:\n",
      "[[ 7.92638697e-01  7.85057141e-01  9.08867253e-01  6.94396100e+09\n",
      "   7.94933272e-01  8.55541615e+09  6.98319664e-01  2.26406827e+09\n",
      "   7.37394000e-01  5.44505115e-01  6.22141745e-01 -7.30254529e-01]\n",
      " [ 8.42745847e-01  9.04982987e-01  9.55894031e-01  5.51195807e+09\n",
      "   7.37003668e-01  2.12100012e+09 -6.36115596e-01  2.14481050e+09\n",
      "   5.30515071e-01  4.59794172e-01  3.17293213e-01  2.36229462e-01]\n",
      " [ 7.43313698e-01  8.54208723e-01  6.96014903e-01  1.16510824e+09\n",
      "   6.85175220e-01  5.30423046e+09  4.35084237e-01  5.01049261e+09\n",
      "   6.62917725e-01  6.11954234e-01  4.77275978e-01  3.31877034e-01]\n",
      " [ 7.90815998e-01  7.92073446e-01  8.33374836e-01  1.14452396e+09\n",
      "   6.87771491e-01  3.45244244e+09  6.40515222e-01  5.59657106e+09\n",
      "   6.78811997e-01  6.83654719e-01  6.09605289e-01  4.09042789e-01]]\n",
      "Policy matrix after 168001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 171001 iterations:\n",
      "[[ 7.92970267e-01  7.86684216e-01  9.08276300e-01  6.94396100e+09\n",
      "   7.95018828e-01  8.55541615e+09  6.98320258e-01  2.26406827e+09\n",
      "   7.37494474e-01  5.44733742e-01  6.22124983e-01 -7.30551232e-01]\n",
      " [ 8.42883010e-01  9.05003475e-01  9.55896631e-01  5.51195807e+09\n",
      "   7.37528139e-01  2.12100012e+09 -6.38128899e-01  2.14481050e+09\n",
      "   5.33122247e-01  4.61152092e-01  3.17970655e-01  2.36390889e-01]\n",
      " [ 7.43060812e-01  8.53052567e-01  6.98788362e-01  1.16510824e+09\n",
      "   6.86023176e-01  5.30423046e+09  4.36182330e-01  5.01049261e+09\n",
      "   6.63324728e-01  6.13193900e-01  4.79037047e-01  3.33758212e-01]\n",
      " [ 7.90548041e-01  7.93371262e-01  8.33293636e-01  1.14452396e+09\n",
      "   6.89322599e-01  3.45244244e+09  6.40858485e-01  5.59657106e+09\n",
      "   6.79194936e-01  6.83713735e-01  6.09225855e-01  4.09079538e-01]]\n",
      "Policy matrix after 171001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 174001 iterations:\n",
      "[[ 7.93536342e-01  7.87121748e-01  9.08122434e-01  6.94396100e+09\n",
      "   7.94801902e-01  8.55541615e+09  6.97991943e-01  2.26406827e+09\n",
      "   7.37234743e-01  5.45285253e-01  6.21956138e-01 -7.29425844e-01]\n",
      " [ 8.42840779e-01  9.04881603e-01  9.55798686e-01  5.51195807e+09\n",
      "   7.37225843e-01  2.12100012e+09 -6.36380018e-01  2.14481050e+09\n",
      "   5.34588638e-01  4.62135583e-01  3.18712667e-01  2.35606916e-01]\n",
      " [ 7.43773218e-01  8.53008576e-01  6.98381069e-01  1.16510824e+09\n",
      "   6.87096972e-01  5.30423046e+09  4.36668943e-01  5.01049261e+09\n",
      "   6.63759643e-01  6.12733550e-01  4.80797345e-01  3.34004936e-01]\n",
      " [ 7.91583458e-01  7.93676288e-01  8.33053707e-01  1.14452396e+09\n",
      "   6.89721563e-01  3.45244244e+09  6.40805796e-01  5.59657106e+09\n",
      "   6.79224410e-01  6.83344483e-01  6.09937532e-01  4.08769101e-01]]\n",
      "Policy matrix after 174001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 177001 iterations:\n",
      "[[ 7.93678159e-01  7.88669927e-01  9.08821775e-01  6.94396100e+09\n",
      "   7.94878935e-01  8.55541615e+09  6.98420766e-01  2.26406827e+09\n",
      "   7.37253431e-01  5.46312708e-01  6.22559992e-01 -7.28186914e-01]\n",
      " [ 8.42934355e-01  9.04865886e-01  9.55848677e-01  5.51195807e+09\n",
      "   7.37593311e-01  2.12100012e+09 -6.34880404e-01  2.14481050e+09\n",
      "   5.36911647e-01  4.64330736e-01  3.23302881e-01  2.36577783e-01]\n",
      " [ 7.44906425e-01  8.53531404e-01  6.97028839e-01  1.16510824e+09\n",
      "   6.87380368e-01  5.30423046e+09  4.36920008e-01  5.01049261e+09\n",
      "   6.63952997e-01  6.13641347e-01  4.81349033e-01  3.33379538e-01]\n",
      " [ 7.91472284e-01  7.93853932e-01  8.32612712e-01  1.14452396e+09\n",
      "   6.90467041e-01  3.45244244e+09  6.43752258e-01  5.59657106e+09\n",
      "   6.78661347e-01  6.83569747e-01  6.10588194e-01  4.08716076e-01]]\n",
      "Policy matrix after 177001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 180001 iterations:\n",
      "[[ 7.94278392e-01  7.89862567e-01  9.09404383e-01  6.94396100e+09\n",
      "   7.94762713e-01  8.55541615e+09  6.98555909e-01  2.26406827e+09\n",
      "   7.37021715e-01  5.47527883e-01  6.22855310e-01 -7.27752608e-01]\n",
      " [ 8.42908732e-01  9.04755936e-01  9.55839603e-01  5.51195807e+09\n",
      "   7.37561032e-01  2.12100012e+09 -6.34292240e-01  2.14481050e+09\n",
      "   5.38236435e-01  4.66556713e-01  3.23508439e-01  2.37144301e-01]\n",
      " [ 7.45488257e-01  8.53662598e-01  6.97797670e-01  1.16510824e+09\n",
      "   6.87461796e-01  5.30423046e+09  4.37202734e-01  5.01049261e+09\n",
      "   6.64856961e-01  6.14041349e-01  4.83710914e-01  3.33781586e-01]\n",
      " [ 7.92125954e-01  7.94122895e-01  8.33348417e-01  1.14452396e+09\n",
      "   6.91497880e-01  3.45244244e+09  6.45046964e-01  5.59657106e+09\n",
      "   6.78229554e-01  6.83606814e-01  6.12737571e-01  4.08055211e-01]]\n",
      "Policy matrix after 180001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 183001 iterations:\n",
      "[[ 7.95145904e-01  7.90998917e-01  9.09941341e-01  6.94396100e+09\n",
      "   7.94919794e-01  8.55541615e+09  6.98570020e-01  2.26406827e+09\n",
      "   7.37283937e-01  5.48727263e-01  6.22614870e-01 -7.27800424e-01]\n",
      " [ 8.43125983e-01  9.04818521e-01  9.55896941e-01  5.51195807e+09\n",
      "   7.37911751e-01  2.12100012e+09 -6.35825069e-01  2.14481050e+09\n",
      "   5.40225168e-01  4.69461854e-01  3.24339643e-01  2.37065862e-01]\n",
      " [ 7.46209381e-01  8.53128263e-01  6.97638873e-01  1.16510824e+09\n",
      "   6.88087904e-01  5.30423046e+09  4.36969913e-01  5.01049261e+09\n",
      "   6.65429403e-01  6.15638189e-01  4.85915042e-01  3.34408341e-01]\n",
      " [ 7.92660913e-01  7.94899587e-01  8.33476017e-01  1.14452396e+09\n",
      "   6.91685825e-01  3.45244244e+09  6.46656334e-01  5.59657106e+09\n",
      "   6.79161110e-01  6.83886056e-01  6.13446349e-01  4.08414933e-01]]\n",
      "Policy matrix after 183001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 186001 iterations:\n",
      "[[ 7.95781814e-01  7.91950696e-01  9.10488998e-01  6.94396100e+09\n",
      "   7.94972768e-01  8.55541615e+09  6.98814431e-01  2.26406827e+09\n",
      "   7.37243874e-01  5.49818648e-01  6.22627981e-01 -7.27568136e-01]\n",
      " [ 8.43158580e-01  9.04794929e-01  9.55925192e-01  5.51195807e+09\n",
      "   7.37426549e-01  2.12100012e+09 -6.36569897e-01  2.14481050e+09\n",
      "   5.41845698e-01  4.72381982e-01  3.26450200e-01  2.37974667e-01]\n",
      " [ 7.47318961e-01  8.52974147e-01  6.99560932e-01  1.16510824e+09\n",
      "   6.88699733e-01  5.30423046e+09  4.36404865e-01  5.01049261e+09\n",
      "   6.66065034e-01  6.16226460e-01  4.85681529e-01  3.34504086e-01]\n",
      " [ 7.92840047e-01  7.94834492e-01  8.34590022e-01  1.14452396e+09\n",
      "   6.92760051e-01  3.45244244e+09  6.47346291e-01  5.59657106e+09\n",
      "   6.79363402e-01  6.83578441e-01  6.14954420e-01  4.07861161e-01]]\n",
      "Policy matrix after 186001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 189001 iterations:\n",
      "[[ 7.96096380e-01  7.93261731e-01  9.10724568e-01  6.94396100e+09\n",
      "   7.95029893e-01  8.55541615e+09  6.98378828e-01  2.26406827e+09\n",
      "   7.37369731e-01  5.51167378e-01  6.22309070e-01 -7.26539301e-01]\n",
      " [ 8.43201365e-01  9.04779970e-01  9.55911644e-01  5.51195807e+09\n",
      "   7.37809428e-01  2.12100012e+09 -6.37521819e-01  2.14481050e+09\n",
      "   5.42761379e-01  4.74122507e-01  3.27490671e-01  2.36184647e-01]\n",
      " [ 7.48242443e-01  8.51959457e-01  6.99808841e-01  1.16510824e+09\n",
      "   6.88732105e-01  5.30423046e+09  4.35562657e-01  5.01049261e+09\n",
      "   6.66385726e-01  6.16763088e-01  4.86233162e-01  3.35717677e-01]\n",
      " [ 7.93384405e-01  7.95180645e-01  8.34607174e-01  1.14452396e+09\n",
      "   6.93529472e-01  3.45244244e+09  6.47093633e-01  5.59657106e+09\n",
      "   6.79669656e-01  6.83805060e-01  6.15808943e-01  4.08296953e-01]]\n",
      "Policy matrix after 189001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 192001 iterations:\n",
      "[[ 7.96944465e-01  7.94205578e-01  9.11293419e-01  6.94396100e+09\n",
      "   7.95020815e-01  8.55541615e+09  6.98660835e-01  2.26406827e+09\n",
      "   7.37288446e-01  5.52232472e-01  6.22601074e-01 -7.25417585e-01]\n",
      " [ 8.43269997e-01  9.04764470e-01  9.55978084e-01  5.51195807e+09\n",
      "   7.37954240e-01  2.12100012e+09 -6.36707774e-01  2.14481050e+09\n",
      "   5.44328991e-01  4.77075216e-01  3.30360758e-01  2.31934052e-01]\n",
      " [ 7.49084770e-01  8.52251883e-01  7.01051191e-01  1.16510824e+09\n",
      "   6.88719708e-01  5.30423046e+09  4.36668502e-01  5.01049261e+09\n",
      "   6.66510416e-01  6.17936704e-01  4.88195162e-01  3.38437173e-01]\n",
      " [ 7.93668177e-01  7.95561406e-01  8.34676289e-01  1.14452396e+09\n",
      "   6.94622644e-01  3.45244244e+09  6.47625279e-01  5.59657106e+09\n",
      "   6.79919136e-01  6.83821751e-01  6.15345546e-01  4.07958786e-01]]\n",
      "Policy matrix after 192001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 195001 iterations:\n",
      "[[ 7.96242320e-01  7.95112858e-01  9.11459963e-01  6.94396100e+09\n",
      "   7.95207194e-01  8.55541615e+09  6.98541238e-01  2.26406827e+09\n",
      "   7.37454598e-01  5.53008455e-01  6.22227688e-01 -7.25751512e-01]\n",
      " [ 8.43435255e-01  9.04832492e-01  9.56005419e-01  5.51195807e+09\n",
      "   7.38837326e-01  2.12100012e+09 -6.37956177e-01  2.14481050e+09\n",
      "   5.46036380e-01  4.79332311e-01  3.30488676e-01  2.33493460e-01]\n",
      " [ 7.49605434e-01  8.52929380e-01  7.01265368e-01  1.16510824e+09\n",
      "   6.89188784e-01  5.30423046e+09  4.36125101e-01  5.01049261e+09\n",
      "   6.67356060e-01  6.18746522e-01  4.88264179e-01  3.37970038e-01]\n",
      " [ 7.93456811e-01  7.96244903e-01  8.34376913e-01  1.14452396e+09\n",
      "   6.95993687e-01  3.45244244e+09  6.48389251e-01  5.59657106e+09\n",
      "   6.79979925e-01  6.84013417e-01  6.15838973e-01  4.07700148e-01]]\n",
      "Policy matrix after 195001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 198001 iterations:\n",
      "[[ 7.96572862e-01  7.96280897e-01  9.11089675e-01  6.94396100e+09\n",
      "   7.95166582e-01  8.55541615e+09  6.98587025e-01  2.26406827e+09\n",
      "   7.37380254e-01  5.54301241e-01  6.22638936e-01 -7.26472809e-01]\n",
      " [ 8.43416978e-01  9.04785529e-01  9.55997137e-01  5.51195807e+09\n",
      "   7.38797592e-01  2.12100012e+09 -6.38629966e-01  2.14481050e+09\n",
      "   5.47263467e-01  4.79623071e-01  3.31165626e-01  2.34318317e-01]\n",
      " [ 7.49910234e-01  8.53082774e-01  7.02710125e-01  1.16510824e+09\n",
      "   6.89322081e-01  5.30423046e+09  4.35929798e-01  5.01049261e+09\n",
      "   6.67621480e-01  6.19510841e-01  4.90157236e-01  3.38939337e-01]\n",
      " [ 7.93892073e-01  7.96308271e-01  8.33906700e-01  1.14452396e+09\n",
      "   6.97228028e-01  3.45244244e+09  6.48766913e-01  5.59657106e+09\n",
      "   6.80084017e-01  6.83978815e-01  6.17190455e-01  4.08502519e-01]]\n",
      "Policy matrix after 198001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 201001 iterations:\n",
      "[[ 7.97300594e-01  7.96993875e-01  9.11131886e-01  6.94396100e+09\n",
      "   7.94916525e-01  8.55541615e+09  6.98227672e-01  2.26406827e+09\n",
      "   7.37076493e-01  5.55257939e-01  6.22590899e-01 -7.25077324e-01]\n",
      " [ 8.43311270e-01  9.04645553e-01  9.55917109e-01  5.51195807e+09\n",
      "   7.38819318e-01  2.12100012e+09 -6.39738494e-01  2.14481050e+09\n",
      "   5.48790302e-01  4.80103538e-01  3.31787641e-01  2.34500718e-01]\n",
      " [ 7.50708433e-01  8.53286244e-01  7.02703774e-01  1.16510824e+09\n",
      "   6.89928129e-01  5.30423046e+09  4.37051290e-01  5.01049261e+09\n",
      "   6.68294107e-01  6.18582503e-01  4.91153138e-01  3.38352501e-01]\n",
      " [ 7.93757891e-01  7.96902154e-01  8.33046388e-01  1.14452396e+09\n",
      "   6.98332278e-01  3.45244244e+09  6.48923226e-01  5.59657106e+09\n",
      "   6.79772937e-01  6.83565088e-01  6.17353035e-01  4.08151683e-01]]\n",
      "Policy matrix after 201001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 204001 iterations:\n",
      "[[ 7.97525481e-01  7.98416742e-01  9.10953971e-01  6.94396100e+09\n",
      "   7.94929926e-01  8.55541615e+09  6.98258047e-01  2.26406827e+09\n",
      "   7.36967511e-01  5.56041663e-01  6.23081983e-01 -7.24038981e-01]\n",
      " [ 8.43370338e-01  9.04699559e-01  9.56002886e-01  5.51195807e+09\n",
      "   7.39156590e-01  2.12100012e+09 -6.39023146e-01  2.14481050e+09\n",
      "   5.50564871e-01  4.80035435e-01  3.33559086e-01  2.34880732e-01]\n",
      " [ 7.51422398e-01  8.53974558e-01  7.01902551e-01  1.16510824e+09\n",
      "   6.90180330e-01  5.30423046e+09  4.36135745e-01  5.01049261e+09\n",
      "   6.68266506e-01  6.18372928e-01  4.92838195e-01  3.39647936e-01]\n",
      " [ 7.94023037e-01  7.97419254e-01  8.33410219e-01  1.14452396e+09\n",
      "   6.98802278e-01  3.45244244e+09  6.48468869e-01  5.59657106e+09\n",
      "   6.80688335e-01  6.83506365e-01  6.17485203e-01  4.08860534e-01]]\n",
      "Policy matrix after 204001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 207001 iterations:\n",
      "[[ 7.97167618e-01  7.99171130e-01  9.11458878e-01  6.94396100e+09\n",
      "   7.95015877e-01  8.55541615e+09  6.98298444e-01  2.26406827e+09\n",
      "   7.37046380e-01  5.57270183e-01  6.22839819e-01 -7.23906851e-01]\n",
      " [ 8.43510726e-01  9.04742978e-01  9.56049220e-01  5.51195807e+09\n",
      "   7.39715094e-01  2.12100012e+09 -6.39480137e-01  2.14481050e+09\n",
      "   5.51996634e-01  4.82192385e-01  3.34670030e-01  2.33734283e-01]\n",
      " [ 7.51464320e-01  8.54282170e-01  7.03112431e-01  1.16510824e+09\n",
      "   6.90236035e-01  5.30423046e+09  4.36708489e-01  5.01049261e+09\n",
      "   6.68927340e-01  6.19137093e-01  4.94331908e-01  3.38667806e-01]\n",
      " [ 7.94603832e-01  7.98234798e-01  8.32856535e-01  1.14452396e+09\n",
      "   6.99394592e-01  3.45244244e+09  6.50158842e-01  5.59657106e+09\n",
      "   6.80992699e-01  6.83683156e-01  6.18146854e-01  4.08167815e-01]]\n",
      "Policy matrix after 207001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 210001 iterations:\n",
      "[[ 7.97111071e-01  8.00061212e-01  9.11572249e-01  6.94396100e+09\n",
      "   7.95197760e-01  8.55541615e+09  6.98240893e-01  2.26406827e+09\n",
      "   7.37135630e-01  5.58455041e-01  6.23307327e-01 -7.24180558e-01]\n",
      " [ 8.43690599e-01  9.04765002e-01  9.56049943e-01  5.51195807e+09\n",
      "   7.40341022e-01  2.12100012e+09 -6.40653593e-01  2.14481050e+09\n",
      "   5.52902016e-01  4.82771194e-01  3.36607725e-01  2.34005968e-01]\n",
      " [ 7.52093925e-01  8.53235007e-01  7.02455776e-01  1.16510824e+09\n",
      "   6.90989820e-01  5.30423046e+09  4.36181402e-01  5.01049261e+09\n",
      "   6.69463660e-01  6.19264383e-01  4.97190908e-01  3.39177835e-01]\n",
      " [ 7.94382664e-01  7.99076903e-01  8.33430552e-01  1.14452396e+09\n",
      "   7.00865144e-01  3.45244244e+09  6.48537872e-01  5.59657106e+09\n",
      "   6.81904452e-01  6.83808027e-01  6.17848698e-01  4.08864060e-01]]\n",
      "Policy matrix after 210001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 213001 iterations:\n",
      "[[ 7.96705668e-01  8.00730595e-01  9.11805992e-01  6.94396100e+09\n",
      "   7.95361216e-01  8.55541615e+09  6.98594786e-01  2.26406827e+09\n",
      "   7.37254570e-01  5.59104371e-01  6.23289601e-01 -7.22634142e-01]\n",
      " [ 8.43816529e-01  9.04831345e-01  9.56152642e-01  5.51195807e+09\n",
      "   7.40807346e-01  2.12100012e+09 -6.40741622e-01  2.14481050e+09\n",
      "   5.54364807e-01  4.83029379e-01  3.37971975e-01  2.35078715e-01]\n",
      " [ 7.52352906e-01  8.53759580e-01  7.04423651e-01  1.16510824e+09\n",
      "   6.91706030e-01  5.30423046e+09  4.35926248e-01  5.01049261e+09\n",
      "   6.70317232e-01  6.19396317e-01  4.98387142e-01  3.39798336e-01]\n",
      " [ 7.94441722e-01  7.99431493e-01  8.34397169e-01  1.14452396e+09\n",
      "   7.02282524e-01  3.45244244e+09  6.49201217e-01  5.59657106e+09\n",
      "   6.82682573e-01  6.83726662e-01  6.18148813e-01  4.08284363e-01]]\n",
      "Policy matrix after 213001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 216001 iterations:\n",
      "[[ 7.96860449e-01  8.00813723e-01  9.11196867e-01  6.94396100e+09\n",
      "   7.95506200e-01  8.55541615e+09  6.98662909e-01  2.26406827e+09\n",
      "   7.37422823e-01  5.59790515e-01  6.23384921e-01 -7.23225042e-01]\n",
      " [ 8.43949704e-01  9.04961045e-01  9.56171453e-01  5.51195807e+09\n",
      "   7.40714670e-01  2.12100012e+09 -6.41225515e-01  2.14481050e+09\n",
      "   5.56235569e-01  4.83692700e-01  3.39884296e-01  2.36177201e-01]\n",
      " [ 7.52933344e-01  8.54398236e-01  7.05489554e-01  1.16510824e+09\n",
      "   6.92599558e-01  5.30423046e+09  4.36562953e-01  5.01049261e+09\n",
      "   6.71008230e-01  6.19434247e-01  4.99446317e-01  3.39878364e-01]\n",
      " [ 7.94800654e-01  7.99656199e-01  8.34854796e-01  1.14452396e+09\n",
      "   7.03361725e-01  3.45244244e+09  6.50188585e-01  5.59657106e+09\n",
      "   6.83355776e-01  6.83859170e-01  6.18644600e-01  4.08227126e-01]]\n",
      "Policy matrix after 216001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 219001 iterations:\n",
      "[[ 7.97545242e-01  8.01465999e-01  9.11680256e-01  6.94396100e+09\n",
      "   7.95642125e-01  8.55541615e+09  6.98284006e-01  2.26406827e+09\n",
      "   7.37541428e-01  5.60813012e-01  6.23105454e-01 -7.22446588e-01]\n",
      " [ 8.44082283e-01  9.05072656e-01  9.56262832e-01  5.51195807e+09\n",
      "   7.40971947e-01  2.12100012e+09 -6.41470911e-01  2.14481050e+09\n",
      "   5.57436774e-01  4.84212195e-01  3.39676248e-01  2.35658094e-01]\n",
      " [ 7.53469059e-01  8.54795246e-01  7.02478027e-01  1.16510824e+09\n",
      "   6.93122965e-01  5.30423046e+09  4.34402627e-01  5.01049261e+09\n",
      "   6.71668447e-01  6.19274607e-01  5.01225130e-01  3.36371464e-01]\n",
      " [ 7.94573869e-01  7.99429275e-01  8.35595145e-01  1.14452396e+09\n",
      "   7.04381903e-01  3.45244244e+09  6.50751617e-01  5.59657106e+09\n",
      "   6.83781794e-01  6.83916868e-01  6.19110048e-01  4.07319472e-01]]\n",
      "Policy matrix after 219001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 222001 iterations:\n",
      "[[ 7.97745404e-01  8.02025343e-01  9.12284030e-01  6.94396100e+09\n",
      "   7.95765946e-01  8.55541615e+09  6.98879236e-01  2.26406827e+09\n",
      "   7.37668378e-01  5.62005748e-01  6.23708485e-01 -7.22393020e-01]\n",
      " [ 8.44242321e-01  9.05184181e-01  9.56378402e-01  5.51195807e+09\n",
      "   7.41639474e-01  2.12100012e+09 -6.40198778e-01  2.14481050e+09\n",
      "   5.58427265e-01  4.85409266e-01  3.42272993e-01  2.36412274e-01]\n",
      " [ 7.53371631e-01  8.55329243e-01  7.01547179e-01  1.16510824e+09\n",
      "   6.93915977e-01  5.30423046e+09  4.37475110e-01  5.01049261e+09\n",
      "   6.72268019e-01  6.19545429e-01  5.03048471e-01  3.38234179e-01]\n",
      " [ 7.95061766e-01  7.99715207e-01  8.35453524e-01  1.14452396e+09\n",
      "   7.05351149e-01  3.45244244e+09  6.52937675e-01  5.59657106e+09\n",
      "   6.84600135e-01  6.84004129e-01  6.18877699e-01  4.07974306e-01]]\n",
      "Policy matrix after 222001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 225001 iterations:\n",
      "[[ 7.98087086e-01  8.02740274e-01  9.12772480e-01  6.94396100e+09\n",
      "   7.95711760e-01  8.55541615e+09  6.99012682e-01  2.26406827e+09\n",
      "   7.37623756e-01  5.62990332e-01  6.23591890e-01 -7.21897917e-01]\n",
      " [ 8.44252242e-01  9.05137271e-01  9.56384428e-01  5.51195807e+09\n",
      "   7.42087906e-01  2.12100012e+09 -6.41145066e-01  2.14481050e+09\n",
      "   5.59680262e-01  4.86447116e-01  3.43419909e-01  2.36877468e-01]\n",
      " [ 7.52929769e-01  8.55449604e-01  7.01340817e-01  1.16510824e+09\n",
      "   6.94045826e-01  5.30423046e+09  4.37414870e-01  5.01049261e+09\n",
      "   6.72567953e-01  6.19917934e-01  5.02465828e-01  3.39197088e-01]\n",
      " [ 7.94839179e-01  7.99705765e-01  8.36168631e-01  1.14452396e+09\n",
      "   7.05686948e-01  3.45244244e+09  6.54955182e-01  5.59657106e+09\n",
      "   6.84519558e-01  6.84050325e-01  6.19991177e-01  4.08116831e-01]]\n",
      "Policy matrix after 225001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 228001 iterations:\n",
      "[[ 7.98560248e-01  8.03688535e-01  9.12848079e-01  6.94396100e+09\n",
      "   7.95906039e-01  8.55541615e+09  6.98896632e-01  2.26406827e+09\n",
      "   7.37805744e-01  5.64194261e-01  6.23639125e-01 -7.23000572e-01]\n",
      " [ 8.44457032e-01  9.05334535e-01  9.56489545e-01  5.51195807e+09\n",
      "   7.42677408e-01  2.12100012e+09 -6.40695110e-01  2.14481050e+09\n",
      "   5.61376020e-01  4.88380982e-01  3.45086318e-01  2.36866765e-01]\n",
      " [ 7.53284605e-01  8.55544630e-01  7.02096272e-01  1.16510824e+09\n",
      "   6.94530619e-01  5.30423046e+09  4.37054703e-01  5.01049261e+09\n",
      "   6.72815799e-01  6.20739186e-01  5.01686183e-01  3.39655354e-01]\n",
      " [ 7.95414149e-01  8.00286671e-01  8.36362112e-01  1.14452396e+09\n",
      "   7.06147469e-01  3.45244244e+09  6.53220974e-01  5.59657106e+09\n",
      "   6.84773713e-01  6.84400188e-01  6.19825357e-01  4.08600787e-01]]\n",
      "Policy matrix after 228001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 231001 iterations:\n",
      "[[ 7.98616395e-01  8.04547095e-01  9.12788735e-01  6.94396100e+09\n",
      "   7.95886126e-01  8.55541615e+09  6.98583265e-01  2.26406827e+09\n",
      "   7.37769866e-01  5.65286058e-01  6.23146316e-01 -7.22896262e-01]\n",
      " [ 8.44403088e-01  9.05247790e-01  9.56445763e-01  5.51195807e+09\n",
      "   7.43104479e-01  2.12100012e+09 -6.40854694e-01  2.14481050e+09\n",
      "   5.62819625e-01  4.89781376e-01  3.44464018e-01  2.36083054e-01]\n",
      " [ 7.53370413e-01  8.55109236e-01  7.03821695e-01  1.16510824e+09\n",
      "   6.95120898e-01  5.30423046e+09  4.37293106e-01  5.01049261e+09\n",
      "   6.72597561e-01  6.20953793e-01  5.02384635e-01  3.41341473e-01]\n",
      " [ 7.95756401e-01  7.99582749e-01  8.36507416e-01  1.14452396e+09\n",
      "   7.06773440e-01  3.45244244e+09  6.52700632e-01  5.59657106e+09\n",
      "   6.85079083e-01  6.84384284e-01  6.19628459e-01  4.07769744e-01]]\n",
      "Policy matrix after 231001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 234001 iterations:\n",
      "[[ 7.98025737e-01  8.05705843e-01  9.12606703e-01  6.94396100e+09\n",
      "   7.95698418e-01  8.55541615e+09  6.98777852e-01  2.26406827e+09\n",
      "   7.37517456e-01  5.66539215e-01  6.23948346e-01 -7.23440405e-01]\n",
      " [ 8.44224127e-01  9.05061448e-01  9.56327439e-01  5.51195807e+09\n",
      "   7.43262805e-01  2.12100012e+09 -6.43604047e-01  2.14481050e+09\n",
      "   5.63336440e-01  4.91104928e-01  3.45912787e-01  2.38390768e-01]\n",
      " [ 7.53834542e-01  8.54679509e-01  7.04850657e-01  1.16510824e+09\n",
      "   6.95762975e-01  5.30423046e+09  4.37404215e-01  5.01049261e+09\n",
      "   6.71673137e-01  6.21259873e-01  5.04666658e-01  3.42941032e-01]\n",
      " [ 7.95686734e-01  7.99007996e-01  8.36630567e-01  1.14452396e+09\n",
      "   7.07629359e-01  3.45244244e+09  6.51754707e-01  5.59657106e+09\n",
      "   6.84731912e-01  6.84259915e-01  6.20418303e-01  4.09083642e-01]]\n",
      "Policy matrix after 234001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 237001 iterations:\n",
      "[[ 7.98071904e-01  8.06232429e-01  9.12885763e-01  6.94396100e+09\n",
      "   7.95767942e-01  8.55541615e+09  6.99037014e-01  2.26406827e+09\n",
      "   7.37634258e-01  5.67384028e-01  6.24531470e-01 -7.24736119e-01]\n",
      " [ 8.44256880e-01  9.05053657e-01  9.56381478e-01  5.51195807e+09\n",
      "   7.42796231e-01  2.12100012e+09 -6.44763786e-01  2.14481050e+09\n",
      "   5.64914445e-01  4.91901533e-01  3.47898824e-01  2.37728533e-01]\n",
      " [ 7.54095707e-01  8.54584446e-01  7.06972436e-01  1.16510824e+09\n",
      "   6.95966363e-01  5.30423046e+09  4.38083694e-01  5.01049261e+09\n",
      "   6.72123084e-01  6.21485822e-01  5.05883796e-01  3.41808768e-01]\n",
      " [ 7.95467693e-01  7.99528862e-01  8.36427949e-01  1.14452396e+09\n",
      "   7.08221681e-01  3.45244244e+09  6.52217612e-01  5.59657106e+09\n",
      "   6.85232231e-01  6.84451006e-01  6.21039715e-01  4.09152982e-01]]\n",
      "Policy matrix after 237001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 240001 iterations:\n",
      "[[ 7.98252180e-01  8.06758879e-01  9.13009499e-01  6.94396100e+09\n",
      "   7.95821672e-01  8.55541615e+09  6.98687581e-01  2.26406827e+09\n",
      "   7.37635851e-01  5.68077238e-01  6.24258306e-01 -7.23844894e-01]\n",
      " [ 8.44307097e-01  9.05058582e-01  9.56385927e-01  5.51195807e+09\n",
      "   7.43331436e-01  2.12100012e+09 -6.43780382e-01  2.14481050e+09\n",
      "   5.66132508e-01  4.92644805e-01  3.48539721e-01  2.37375171e-01]\n",
      " [ 7.54458926e-01  8.54748128e-01  7.06193849e-01  1.16510824e+09\n",
      "   6.96459021e-01  5.30423046e+09  4.37697306e-01  5.01049261e+09\n",
      "   6.71721439e-01  6.21322384e-01  5.06781122e-01  3.42815812e-01]\n",
      " [ 7.94897233e-01  7.99820598e-01  8.37111536e-01  1.14452396e+09\n",
      "   7.09186410e-01  3.45244244e+09  6.52862004e-01  5.59657106e+09\n",
      "   6.85557371e-01  6.84588510e-01  6.20857038e-01  4.08810675e-01]]\n",
      "Policy matrix after 240001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 243001 iterations:\n",
      "[[ 7.97872072e-01  8.07712156e-01  9.12623813e-01  6.94396100e+09\n",
      "   7.95770910e-01  8.55541615e+09  6.98423593e-01  2.26406827e+09\n",
      "   7.37603235e-01  5.68454968e-01  6.24217252e-01 -7.23723748e-01]\n",
      " [ 8.44230299e-01  9.04973062e-01  9.56335611e-01  5.51195807e+09\n",
      "   7.43200814e-01  2.12100012e+09 -6.42394346e-01  2.14481050e+09\n",
      "   5.67043114e-01  4.93572620e-01  3.48207211e-01  2.38019192e-01]\n",
      " [ 7.54985242e-01  8.54557307e-01  7.06622105e-01  1.16510824e+09\n",
      "   6.95748896e-01  5.30423046e+09  4.37861458e-01  5.01049261e+09\n",
      "   6.72018491e-01  6.21587796e-01  5.07564032e-01  3.43636732e-01]\n",
      " [ 7.94804875e-01  8.00081728e-01  8.37052173e-01  1.14452396e+09\n",
      "   7.09089030e-01  3.45244244e+09  6.52786590e-01  5.59657106e+09\n",
      "   6.86016746e-01  6.84555813e-01  6.21533325e-01  4.08930649e-01]]\n",
      "Policy matrix after 243001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 246001 iterations:\n",
      "[[ 7.98133374e-01  8.08396975e-01  9.12435707e-01  6.94396100e+09\n",
      "   7.96010967e-01  8.55541615e+09  6.98344027e-01  2.26406827e+09\n",
      "   7.37879229e-01  5.69568138e-01  6.24113900e-01 -7.23557122e-01]\n",
      " [ 8.44464247e-01  9.05087926e-01  9.56371167e-01  5.51195807e+09\n",
      "   7.43492286e-01  2.12100012e+09 -6.42178026e-01  2.14481050e+09\n",
      "   5.68329970e-01  4.94667523e-01  3.46551303e-01  2.38506301e-01]\n",
      " [ 7.55577031e-01  8.54702871e-01  7.05953920e-01  1.16510824e+09\n",
      "   6.96085571e-01  5.30423046e+09  4.40083506e-01  5.01049261e+09\n",
      "   6.72610464e-01  6.21843015e-01  5.08127567e-01  3.44774081e-01]\n",
      " [ 7.94733821e-01  8.00446770e-01  8.36870324e-01  1.14452396e+09\n",
      "   7.09993028e-01  3.45244244e+09  6.52811471e-01  5.59657106e+09\n",
      "   6.86538988e-01  6.84770110e-01  6.22382547e-01  4.08185991e-01]]\n",
      "Policy matrix after 246001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 249001 iterations:\n",
      "[[ 7.98225020e-01  8.09162272e-01  9.12560439e-01  6.94396100e+09\n",
      "   7.95903227e-01  8.55541615e+09  6.97978757e-01  2.26406827e+09\n",
      "   7.37753220e-01  5.70214281e-01  6.23802453e-01 -7.22750714e-01]\n",
      " [ 8.44403236e-01  9.04911160e-01  9.56304170e-01  5.51195807e+09\n",
      "   7.43672523e-01  2.12100012e+09 -6.43158526e-01  2.14481050e+09\n",
      "   5.69444642e-01  4.94935842e-01  3.48436843e-01  2.37791482e-01]\n",
      " [ 7.55634088e-01  8.54003415e-01  7.05376457e-01  1.16510824e+09\n",
      "   6.95553673e-01  5.30423046e+09  4.42060065e-01  5.01049261e+09\n",
      "   6.72696894e-01  6.21637198e-01  5.07981072e-01  3.45099166e-01]\n",
      " [ 7.94545108e-01  8.00680047e-01  8.37044935e-01  1.14452396e+09\n",
      "   7.10275439e-01  3.45244244e+09  6.53417563e-01  5.59657106e+09\n",
      "   6.86898449e-01  6.84799126e-01  6.22416723e-01  4.08005643e-01]]\n",
      "Policy matrix after 249001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 252001 iterations:\n",
      "[[ 7.98780333e-01  8.09120230e-01  9.12649772e-01  6.94396100e+09\n",
      "   7.96031356e-01  8.55541615e+09  6.97863709e-01  2.26406827e+09\n",
      "   7.37842392e-01  5.70735658e-01  6.23673881e-01 -7.22621277e-01]\n",
      " [ 8.44527538e-01  9.04950243e-01  9.56354854e-01  5.51195807e+09\n",
      "   7.43877244e-01  2.12100012e+09 -6.42618150e-01  2.14481050e+09\n",
      "   5.70426467e-01  4.95990819e-01  3.50188683e-01  2.37853238e-01]\n",
      " [ 7.55907237e-01  8.54670199e-01  7.05008178e-01  1.16510824e+09\n",
      "   6.95585852e-01  5.30423046e+09  4.43598947e-01  5.01049261e+09\n",
      "   6.73082866e-01  6.22386074e-01  5.08677675e-01  3.44973882e-01]\n",
      " [ 7.94692005e-01  8.01301737e-01  8.37107458e-01  1.14452396e+09\n",
      "   7.10858565e-01  3.45244244e+09  6.52854622e-01  5.59657106e+09\n",
      "   6.87387026e-01  6.84928578e-01  6.22351761e-01  4.08268076e-01]]\n",
      "Policy matrix after 252001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 255001 iterations:\n",
      "[[ 7.99022178e-01  8.09533730e-01  9.12706482e-01  6.94396100e+09\n",
      "   7.95863532e-01  8.55541615e+09  6.97985043e-01  2.26406827e+09\n",
      "   7.37548312e-01  5.70706215e-01  6.24001635e-01 -7.23723049e-01]\n",
      " [ 8.44416288e-01  9.04806088e-01  9.56320056e-01  5.51195807e+09\n",
      "   7.43865607e-01  2.12100012e+09 -6.43972079e-01  2.14481050e+09\n",
      "   5.71285609e-01  4.97245040e-01  3.52164862e-01  2.39308959e-01]\n",
      " [ 7.56065105e-01  8.54534430e-01  7.05551924e-01  1.16510824e+09\n",
      "   6.95947300e-01  5.30423046e+09  4.43300216e-01  5.01049261e+09\n",
      "   6.73361752e-01  6.22850862e-01  5.08853477e-01  3.46347115e-01]\n",
      " [ 7.94824549e-01  8.01670952e-01  8.37647284e-01  1.14452396e+09\n",
      "   7.11428721e-01  3.45244244e+09  6.52953471e-01  5.59657106e+09\n",
      "   6.87335175e-01  6.84648736e-01  6.22353205e-01  4.09486893e-01]]\n",
      "Policy matrix after 255001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 258001 iterations:\n",
      "[[ 7.99484866e-01  8.09951943e-01  9.13058062e-01  6.94396100e+09\n",
      "   7.95896531e-01  8.55541615e+09  6.97769421e-01  2.26406827e+09\n",
      "   7.37694732e-01  5.71454705e-01  6.24177145e-01 -7.22359834e-01]\n",
      " [ 8.44452976e-01  9.04804242e-01  9.56326473e-01  5.51195807e+09\n",
      "   7.44243710e-01  2.12100012e+09 -6.42684329e-01  2.14481050e+09\n",
      "   5.72361140e-01  4.97867641e-01  3.52783076e-01  2.39014903e-01]\n",
      " [ 7.55446197e-01  8.54051993e-01  7.05415977e-01  1.16510824e+09\n",
      "   6.95753898e-01  5.30423046e+09  4.42810506e-01  5.01049261e+09\n",
      "   6.73759204e-01  6.23660201e-01  5.09723483e-01  3.46365637e-01]\n",
      " [ 7.94760395e-01  8.02000584e-01  8.37648116e-01  1.14452396e+09\n",
      "   7.11902085e-01  3.45244244e+09  6.52680839e-01  5.59657106e+09\n",
      "   6.87191696e-01  6.84839578e-01  6.21924161e-01  4.09104738e-01]]\n",
      "Policy matrix after 258001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 261001 iterations:\n",
      "[[ 7.99053130e-01  8.10405560e-01  9.12908768e-01  6.94396100e+09\n",
      "   7.95948295e-01  8.55541615e+09  6.97838005e-01  2.26406827e+09\n",
      "   7.37793108e-01  5.72654545e-01  6.24171648e-01 -7.21002365e-01]\n",
      " [ 8.44490082e-01  9.04830655e-01  9.56334025e-01  5.51195807e+09\n",
      "   7.44550471e-01  2.12100012e+09 -6.42704990e-01  2.14481050e+09\n",
      "   5.72633672e-01  4.99386469e-01  3.53020429e-01  2.38250504e-01]\n",
      " [ 7.55213597e-01  8.54533815e-01  7.05377096e-01  1.16510824e+09\n",
      "   6.96139890e-01  5.30423046e+09  4.41252701e-01  5.01049261e+09\n",
      "   6.74231521e-01  6.24101724e-01  5.10697775e-01  3.46944064e-01]\n",
      " [ 7.95231154e-01  8.01807055e-01  8.37922475e-01  1.14452396e+09\n",
      "   7.12109420e-01  3.45244244e+09  6.54467169e-01  5.59657106e+09\n",
      "   6.87493420e-01  6.84822554e-01  6.22682777e-01  4.08820635e-01]]\n",
      "Policy matrix after 261001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 264001 iterations:\n",
      "[[ 7.99119344e-01  8.10823301e-01  9.12624207e-01  6.94396100e+09\n",
      "   7.96131260e-01  8.55541615e+09  6.98030459e-01  2.26406827e+09\n",
      "   7.37968768e-01  5.72934700e-01  6.24317055e-01 -7.22330279e-01]\n",
      " [ 8.44614290e-01  9.04912964e-01  9.56375663e-01  5.51195807e+09\n",
      "   7.44285535e-01  2.12100012e+09 -6.39341866e-01  2.14481050e+09\n",
      "   5.73330658e-01  5.01139864e-01  3.53497223e-01  2.39026947e-01]\n",
      " [ 7.55782182e-01  8.55039553e-01  7.05502141e-01  1.16510824e+09\n",
      "   6.96702831e-01  5.30423046e+09  4.41009962e-01  5.01049261e+09\n",
      "   6.74762172e-01  6.24435668e-01  5.11167773e-01  3.47273630e-01]\n",
      " [ 7.95386260e-01  8.01836344e-01  8.37998950e-01  1.14452396e+09\n",
      "   7.12815335e-01  3.45244244e+09  6.54437742e-01  5.59657106e+09\n",
      "   6.87851567e-01  6.84991470e-01  6.23225823e-01  4.08496607e-01]]\n",
      "Policy matrix after 264001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 267001 iterations:\n",
      "[[ 7.99359295e-01  8.11206773e-01  9.13010508e-01  6.94396100e+09\n",
      "   7.96149201e-01  8.55541615e+09  6.98392641e-01  2.26406827e+09\n",
      "   7.38013447e-01  5.73433921e-01  6.24491508e-01 -7.23273324e-01]\n",
      " [ 8.44699324e-01  9.04922346e-01  9.56453985e-01  5.51195807e+09\n",
      "   7.43815476e-01  2.12100012e+09 -6.40473486e-01  2.14481050e+09\n",
      "   5.74473177e-01  5.02635905e-01  3.54381603e-01  2.37578377e-01]\n",
      " [ 7.56310186e-01  8.54956271e-01  7.06536872e-01  1.16510824e+09\n",
      "   6.96657448e-01  5.30423046e+09  4.40978855e-01  5.01049261e+09\n",
      "   6.74475236e-01  6.25024013e-01  5.12215321e-01  3.47495057e-01]\n",
      " [ 7.95795054e-01  8.02219412e-01  8.37860137e-01  1.14452396e+09\n",
      "   7.13334693e-01  3.45244244e+09  6.55199246e-01  5.59657106e+09\n",
      "   6.88339530e-01  6.85093148e-01  6.23303789e-01  4.08022333e-01]]\n",
      "Policy matrix after 267001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 270001 iterations:\n",
      "[[ 7.99927215e-01  8.11934497e-01  9.12791716e-01  6.94396100e+09\n",
      "   7.96132132e-01  8.55541615e+09  6.98496957e-01  2.26406827e+09\n",
      "   7.38007202e-01  5.74152630e-01  6.24423149e-01 -7.23302846e-01]\n",
      " [ 8.44703120e-01  9.04894843e-01  9.56410848e-01  5.51195807e+09\n",
      "   7.43443205e-01  2.12100012e+09 -6.39738715e-01  2.14481050e+09\n",
      "   5.75522423e-01  5.03704639e-01  3.53732633e-01  2.38867418e-01]\n",
      " [ 7.56842782e-01  8.54996519e-01  7.06933886e-01  1.16510824e+09\n",
      "   6.97152581e-01  5.30423046e+09  4.39102424e-01  5.01049261e+09\n",
      "   6.75036720e-01  6.25351564e-01  5.12275068e-01  3.49161779e-01]\n",
      " [ 7.95548756e-01  8.01916004e-01  8.38284491e-01  1.14452396e+09\n",
      "   7.14056608e-01  3.45244244e+09  6.57160914e-01  5.59657106e+09\n",
      "   6.88453826e-01  6.85048816e-01  6.23038543e-01  4.08518582e-01]]\n",
      "Policy matrix after 270001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 273001 iterations:\n",
      "[[ 8.00410505e-01  8.12664892e-01  9.12675425e-01  6.94396100e+09\n",
      "   7.96227473e-01  8.55541615e+09  6.98815841e-01  2.26406827e+09\n",
      "   7.38018130e-01  5.74662655e-01  6.24469469e-01 -7.23687988e-01]\n",
      " [ 8.44848931e-01  9.04941867e-01  9.56447523e-01  5.51195807e+09\n",
      "   7.43862995e-01  2.12100012e+09 -6.38497934e-01  2.14481050e+09\n",
      "   5.76524310e-01  5.04864980e-01  3.55300357e-01  2.38679007e-01]\n",
      " [ 7.57211404e-01  8.54716349e-01  7.06453423e-01  1.16510824e+09\n",
      "   6.97220219e-01  5.30423046e+09  4.39953605e-01  5.01049261e+09\n",
      "   6.75315816e-01  6.24726294e-01  5.12852457e-01  3.50259101e-01]\n",
      " [ 7.95593400e-01  8.02532423e-01  8.37904509e-01  1.14452396e+09\n",
      "   7.14506031e-01  3.45244244e+09  6.58824105e-01  5.59657106e+09\n",
      "   6.88880810e-01  6.85040719e-01  6.23723098e-01  4.08138297e-01]]\n",
      "Policy matrix after 273001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 276001 iterations:\n",
      "[[ 8.00875336e-01  8.13029645e-01  9.12372069e-01  6.94396100e+09\n",
      "   7.96043753e-01  8.55541615e+09  6.98473209e-01  2.26406827e+09\n",
      "   7.37863707e-01  5.75464390e-01  6.24211738e-01 -7.23622360e-01]\n",
      " [ 8.44760052e-01  9.04817725e-01  9.56362836e-01  5.51195807e+09\n",
      "   7.44311847e-01  2.12100012e+09 -6.38515696e-01  2.14481050e+09\n",
      "   5.77129166e-01  5.06179041e-01  3.56980901e-01  2.37176400e-01]\n",
      " [ 7.56612413e-01  8.54344075e-01  7.06565888e-01  1.16510824e+09\n",
      "   6.97722316e-01  5.30423046e+09  4.39050309e-01  5.01049261e+09\n",
      "   6.75131939e-01  6.25226191e-01  5.11961380e-01  3.50416335e-01]\n",
      " [ 7.95864583e-01  8.02695664e-01  8.38444395e-01  1.14452396e+09\n",
      "   7.14270627e-01  3.45244244e+09  6.59366322e-01  5.59657106e+09\n",
      "   6.88793125e-01  6.84803638e-01  6.22919082e-01  4.08017015e-01]]\n",
      "Policy matrix after 276001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 279001 iterations:\n",
      "[[ 8.00985282e-01  8.13722863e-01  9.12732755e-01  6.94396100e+09\n",
      "   7.95959518e-01  8.55541615e+09  6.98649650e-01  2.26406827e+09\n",
      "   7.37753637e-01  5.76401253e-01  6.24638088e-01 -7.24063523e-01]\n",
      " [ 8.44727013e-01  9.04793092e-01  9.56404978e-01  5.51195807e+09\n",
      "   7.43900590e-01  2.12100012e+09 -6.38694620e-01  2.14481050e+09\n",
      "   5.78082516e-01  5.07290221e-01  3.57388345e-01  2.37790504e-01]\n",
      " [ 7.57039793e-01  8.53887411e-01  7.05975134e-01  1.16510824e+09\n",
      "   6.98377398e-01  5.30423046e+09  4.39517333e-01  5.01049261e+09\n",
      "   6.74964301e-01  6.24212826e-01  5.13756234e-01  3.52084448e-01]\n",
      " [ 7.96035250e-01  8.03088544e-01  8.38836098e-01  1.14452396e+09\n",
      "   7.14290642e-01  3.45244244e+09  6.60155961e-01  5.59657106e+09\n",
      "   6.88255583e-01  6.84683753e-01  6.23139516e-01  4.09017170e-01]]\n",
      "Policy matrix after 279001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 282001 iterations:\n",
      "[[ 8.00924337e-01  8.14198292e-01  9.13070474e-01  6.94396100e+09\n",
      "   7.96109814e-01  8.55541615e+09  6.98957522e-01  2.26406827e+09\n",
      "   7.37932588e-01  5.77042048e-01  6.24797698e-01 -7.24855360e-01]\n",
      " [ 8.44829108e-01  9.04877139e-01  9.56519536e-01  5.51195807e+09\n",
      "   7.44022178e-01  2.12100012e+09 -6.38789683e-01  2.14481050e+09\n",
      "   5.79095285e-01  5.08774688e-01  3.59356958e-01  2.38393257e-01]\n",
      " [ 7.57350745e-01  8.54464888e-01  7.06633427e-01  1.16510824e+09\n",
      "   6.98658996e-01  5.30423046e+09  4.41547968e-01  5.01049261e+09\n",
      "   6.75421700e-01  6.24719599e-01  5.13139906e-01  3.53995683e-01]\n",
      " [ 7.95615797e-01  8.03450049e-01  8.39413063e-01  1.14452396e+09\n",
      "   7.14653565e-01  3.45244244e+09  6.60476331e-01  5.59657106e+09\n",
      "   6.88418140e-01  6.84908827e-01  6.23689735e-01  4.09620067e-01]]\n",
      "Policy matrix after 282001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 285001 iterations:\n",
      "[[ 8.01213125e-01  8.13994401e-01  9.13053943e-01  6.94396100e+09\n",
      "   7.96142412e-01  8.55541615e+09  6.98466338e-01  2.26406827e+09\n",
      "   7.37977166e-01  5.76909424e-01  6.24534249e-01 -7.24401190e-01]\n",
      " [ 8.44856443e-01  9.04818175e-01  9.56436805e-01  5.51195807e+09\n",
      "   7.44276556e-01  2.12100012e+09 -6.39395783e-01  2.14481050e+09\n",
      "   5.79501916e-01  5.08187456e-01  3.59503156e-01  2.36927216e-01]\n",
      " [ 7.57798458e-01  8.54915651e-01  7.06133644e-01  1.16510824e+09\n",
      "   6.98995082e-01  5.30423046e+09  4.43381041e-01  5.01049261e+09\n",
      "   6.75711860e-01  6.25423240e-01  5.13468479e-01  3.55478378e-01]\n",
      " [ 7.94870651e-01  8.03419142e-01  8.38308619e-01  1.14452396e+09\n",
      "   7.15172829e-01  3.45244244e+09  6.58862742e-01  5.59657106e+09\n",
      "   6.88904436e-01  6.84997383e-01  6.24509513e-01  4.09745006e-01]]\n",
      "Policy matrix after 285001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 288001 iterations:\n",
      "[[ 8.01683566e-01  8.14300661e-01  9.13072717e-01  6.94396100e+09\n",
      "   7.96205911e-01  8.55541615e+09  6.98635886e-01  2.26406827e+09\n",
      "   7.38155038e-01  5.77318312e-01  6.24982254e-01 -7.22945767e-01]\n",
      " [ 8.44979671e-01  9.04904394e-01  9.56509371e-01  5.51195807e+09\n",
      "   7.44564347e-01  2.12100012e+09 -6.39956787e-01  2.14481050e+09\n",
      "   5.80561008e-01  5.09047601e-01  3.61373278e-01  2.38404961e-01]\n",
      " [ 7.57261811e-01  8.55272413e-01  7.06436330e-01  1.16510824e+09\n",
      "   6.98937920e-01  5.30423046e+09  4.44578154e-01  5.01049261e+09\n",
      "   6.75977109e-01  6.25694333e-01  5.14287891e-01  3.55835010e-01]\n",
      " [ 7.94912846e-01  8.03323444e-01  8.38686899e-01  1.14452396e+09\n",
      "   7.15365548e-01  3.45244244e+09  6.59192789e-01  5.59657106e+09\n",
      "   6.89487702e-01  6.85174371e-01  6.24648697e-01  4.10632955e-01]]\n",
      "Policy matrix after 288001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 291001 iterations:\n",
      "[[ 8.01298263e-01  8.14087322e-01  9.12971310e-01  6.94396100e+09\n",
      "   7.96184739e-01  8.55541615e+09  6.98448955e-01  2.26406827e+09\n",
      "   7.38114213e-01  5.77347612e-01  6.25099706e-01 -7.23097666e-01]\n",
      " [ 8.44916628e-01  9.04789397e-01  9.56409310e-01  5.51195807e+09\n",
      "   7.44256603e-01  2.12100012e+09 -6.40084558e-01  2.14481050e+09\n",
      "   5.81803101e-01  5.10099008e-01  3.62487931e-01  2.38239642e-01]\n",
      " [ 7.57514509e-01  8.55453068e-01  7.06587166e-01  1.16510824e+09\n",
      "   6.99313429e-01  5.30423046e+09  4.43288974e-01  5.01049261e+09\n",
      "   6.76217407e-01  6.26345008e-01  5.15192612e-01  3.56516820e-01]\n",
      " [ 7.94815033e-01  8.03311244e-01  8.39138650e-01  1.14452396e+09\n",
      "   7.15827614e-01  3.45244244e+09  6.59763401e-01  5.59657106e+09\n",
      "   6.89174125e-01  6.85263027e-01  6.24676498e-01  4.11067739e-01]]\n",
      "Policy matrix after 291001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  0.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 294001 iterations:\n",
      "[[ 8.01566623e-01  8.14244907e-01  9.13000672e-01  6.94396100e+09\n",
      "   7.96276650e-01  8.55541615e+09  6.98404135e-01  2.26406827e+09\n",
      "   7.38299307e-01  5.77797384e-01  6.25087602e-01 -7.23493262e-01]\n",
      " [ 8.44921198e-01  9.04787219e-01  9.56379999e-01  5.51195807e+09\n",
      "   7.43656769e-01  2.12100012e+09 -6.40753302e-01  2.14481050e+09\n",
      "   5.82675405e-01  5.11255531e-01  3.63281131e-01  2.38627458e-01]\n",
      " [ 7.57449098e-01  8.55626951e-01  7.05384214e-01  1.16510824e+09\n",
      "   6.99364391e-01  5.30423046e+09  4.43674697e-01  5.01049261e+09\n",
      "   6.76395478e-01  6.26565205e-01  5.15869792e-01  3.57626132e-01]\n",
      " [ 7.94303546e-01  8.03217859e-01  8.39837710e-01  1.14452396e+09\n",
      "   7.16250369e-01  3.45244244e+09  6.58459997e-01  5.59657106e+09\n",
      "   6.89082555e-01  6.85610425e-01  6.28213459e-01  4.11545616e-01]]\n",
      "Policy matrix after 294001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 297001 iterations:\n",
      "[[ 8.01678480e-01  8.14448649e-01  9.13140411e-01  6.94396100e+09\n",
      "   7.96332884e-01  8.55541615e+09  6.98410532e-01  2.26406827e+09\n",
      "   7.38463005e-01  5.78154103e-01  6.25185732e-01 -7.23948538e-01]\n",
      " [ 8.44993226e-01  9.04824928e-01  9.56396612e-01  5.51195807e+09\n",
      "   7.43694429e-01  2.12100012e+09 -6.40523652e-01  2.14481050e+09\n",
      "   5.83419358e-01  5.11852953e-01  3.64616480e-01  2.39156307e-01]\n",
      " [ 7.57460946e-01  8.55735583e-01  7.05772373e-01  1.16510824e+09\n",
      "   6.99857727e-01  5.30423046e+09  4.42945151e-01  5.01049261e+09\n",
      "   6.77008568e-01  6.26664902e-01  5.15962609e-01  3.57954647e-01]\n",
      " [ 7.94576675e-01  8.03299306e-01  8.40178296e-01  1.14452396e+09\n",
      "   7.16588302e-01  3.45244244e+09  6.58061032e-01  5.59657106e+09\n",
      "   6.89512884e-01  6.85760187e-01  6.29163083e-01  4.11859958e-01]]\n",
      "Policy matrix after 297001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 300001 iterations:\n",
      "[[ 8.01612207e-01  8.15253694e-01  9.12350513e-01  6.94396100e+09\n",
      "   7.96365862e-01  8.55541615e+09  6.98676268e-01  2.26406827e+09\n",
      "   7.38538100e-01  5.79000960e-01  6.25158112e-01 -7.23963486e-01]\n",
      " [ 8.45051667e-01  9.04883059e-01  9.56420675e-01  5.51195807e+09\n",
      "   7.43190907e-01  2.12100012e+09 -6.40509276e-01  2.14481050e+09\n",
      "   5.84789800e-01  5.13023982e-01  3.66051936e-01  2.38374372e-01]\n",
      " [ 7.57160495e-01  8.55678758e-01  7.07063328e-01  1.16510824e+09\n",
      "   7.00273689e-01  5.30423046e+09  4.43706173e-01  5.01049261e+09\n",
      "   6.77386866e-01  6.26169199e-01  5.16744607e-01  3.57988772e-01]\n",
      " [ 7.94614725e-01  8.03729735e-01  8.40714359e-01  1.14452396e+09\n",
      "   7.17326234e-01  3.45244244e+09  6.57585898e-01  5.59657106e+09\n",
      "   6.89470022e-01  6.85776705e-01  6.30232214e-01  4.11681825e-01]]\n",
      "Policy matrix after 300001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 303001 iterations:\n",
      "[[ 8.01720905e-01  8.15738325e-01  9.12426613e-01  6.94396100e+09\n",
      "   7.96346763e-01  8.55541615e+09  6.98556048e-01  2.26406827e+09\n",
      "   7.38473455e-01  5.79349041e-01  6.24949743e-01 -7.22766985e-01]\n",
      " [ 8.45067887e-01  9.04880484e-01  9.56375443e-01  5.51195807e+09\n",
      "   7.42889744e-01  2.12100012e+09 -6.40587424e-01  2.14481050e+09\n",
      "   5.84976935e-01  5.13603145e-01  3.66804616e-01  2.39918430e-01]\n",
      " [ 7.57565722e-01  8.55541912e-01  7.08023579e-01  1.16510824e+09\n",
      "   7.00688681e-01  5.30423046e+09  4.43545456e-01  5.01049261e+09\n",
      "   6.77377042e-01  6.26355427e-01  5.17365765e-01  3.58023128e-01]\n",
      " [ 7.94598759e-01  8.03972896e-01  8.40519274e-01  1.14452396e+09\n",
      "   7.17542614e-01  3.45244244e+09  6.56953929e-01  5.59657106e+09\n",
      "   6.89822103e-01  6.85725416e-01  6.31504081e-01  4.12089316e-01]]\n",
      "Policy matrix after 303001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 306001 iterations:\n",
      "[[ 8.02248921e-01  8.16036438e-01  9.12496465e-01  6.94396100e+09\n",
      "   7.96321222e-01  8.55541615e+09  6.98637679e-01  2.26406827e+09\n",
      "   7.38388616e-01  5.80086193e-01  6.25066963e-01 -7.22248123e-01]\n",
      " [ 8.45093789e-01  9.04855909e-01  9.56386480e-01  5.51195807e+09\n",
      "   7.43246895e-01  2.12100012e+09 -6.40250397e-01  2.14481050e+09\n",
      "   5.85571509e-01  5.15014816e-01  3.67277422e-01  2.40057044e-01]\n",
      " [ 7.57234501e-01  8.55562613e-01  7.08814074e-01  1.16510824e+09\n",
      "   7.00378746e-01  5.30423046e+09  4.42864608e-01  5.01049261e+09\n",
      "   6.77206375e-01  6.26593849e-01  5.17958345e-01  3.58158091e-01]\n",
      " [ 7.94715388e-01  8.04088614e-01  8.40605504e-01  1.14452396e+09\n",
      "   7.17720192e-01  3.45244244e+09  6.56622262e-01  5.59657106e+09\n",
      "   6.90110131e-01  6.85756337e-01  6.32361048e-01  4.12072115e-01]]\n",
      "Policy matrix after 306001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 309001 iterations:\n",
      "[[ 8.02116455e-01  8.16545266e-01  9.12445713e-01  6.94396100e+09\n",
      "   7.96329193e-01  8.55541615e+09  6.98665356e-01  2.26406827e+09\n",
      "   7.38373603e-01  5.80976947e-01  6.25139401e-01 -7.20771124e-01]\n",
      " [ 8.45167226e-01  9.04882671e-01  9.56402935e-01  5.51195807e+09\n",
      "   7.43465873e-01  2.12100012e+09 -6.41256247e-01  2.14481050e+09\n",
      "   5.85876912e-01  5.15633722e-01  3.68071930e-01  2.39223331e-01]\n",
      " [ 7.57337177e-01  8.55965583e-01  7.07989426e-01  1.16510824e+09\n",
      "   7.00711903e-01  5.30423046e+09  4.43023533e-01  5.01049261e+09\n",
      "   6.76941231e-01  6.26885207e-01  5.19535688e-01  3.59639834e-01]\n",
      " [ 7.94995415e-01  8.04326812e-01  8.40371090e-01  1.14452396e+09\n",
      "   7.18124160e-01  3.45244244e+09  6.57642401e-01  5.59657106e+09\n",
      "   6.90393977e-01  6.85726355e-01  6.32438946e-01  4.12550088e-01]]\n",
      "Policy matrix after 309001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 312001 iterations:\n",
      "[[ 8.02000592e-01  8.17179551e-01  9.12340274e-01  6.94396100e+09\n",
      "   7.96458751e-01  8.55541615e+09  6.98644431e-01  2.26406827e+09\n",
      "   7.38586500e-01  5.81573580e-01  6.25123683e-01 -7.20444624e-01]\n",
      " [ 8.45280143e-01  9.04966504e-01  9.56444499e-01  5.51195807e+09\n",
      "   7.43222376e-01  2.12100012e+09 -6.42128350e-01  2.14481050e+09\n",
      "   5.86534107e-01  5.16394677e-01  3.68535883e-01  2.39399064e-01]\n",
      " [ 7.57526754e-01  8.56305292e-01  7.07499353e-01  1.16510824e+09\n",
      "   7.01137189e-01  5.30423046e+09  4.42955003e-01  5.01049261e+09\n",
      "   6.77219543e-01  6.27302804e-01  5.20544805e-01  3.60716517e-01]\n",
      " [ 7.95218134e-01  8.04490244e-01  8.39927184e-01  1.14452396e+09\n",
      "   7.18409902e-01  3.45244244e+09  6.58257457e-01  5.59657106e+09\n",
      "   6.90692121e-01  6.86082740e-01  6.33661809e-01  4.12955154e-01]]\n",
      "Policy matrix after 312001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 315001 iterations:\n",
      "[[ 8.01910097e-01  8.17566853e-01  9.12377944e-01  6.94396100e+09\n",
      "   7.96460629e-01  8.55541615e+09  6.98496018e-01  2.26406827e+09\n",
      "   7.38634844e-01  5.82113100e-01  6.25100881e-01 -7.20042425e-01]\n",
      " [ 8.45268723e-01  9.04962470e-01  9.56425895e-01  5.51195807e+09\n",
      "   7.43530229e-01  2.12100012e+09 -6.42593387e-01  2.14481050e+09\n",
      "   5.87510971e-01  5.16887810e-01  3.70086594e-01  2.38615586e-01]\n",
      " [ 7.57670001e-01  8.56685547e-01  7.08377233e-01  1.16510824e+09\n",
      "   7.01613612e-01  5.30423046e+09  4.41518825e-01  5.01049261e+09\n",
      "   6.77229903e-01  6.27483321e-01  5.20917955e-01  3.60903776e-01]\n",
      " [ 7.94960316e-01  8.04346019e-01  8.40100344e-01  1.14452396e+09\n",
      "   7.18438961e-01  3.45244244e+09  6.57306522e-01  5.59657106e+09\n",
      "   6.90951590e-01  6.86163820e-01  6.33452725e-01  4.13053713e-01]]\n",
      "Policy matrix after 315001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 318001 iterations:\n",
      "[[ 8.01846536e-01  8.18083974e-01  9.12654089e-01  6.94396100e+09\n",
      "   7.96396672e-01  8.55541615e+09  6.98297727e-01  2.26406827e+09\n",
      "   7.38595066e-01  5.82296179e-01  6.24842117e-01 -7.19362999e-01]\n",
      " [ 8.45250009e-01  9.04899881e-01  9.56384092e-01  5.51195807e+09\n",
      "   7.43564916e-01  2.12100012e+09 -6.42861415e-01  2.14481050e+09\n",
      "   5.88513523e-01  5.17317276e-01  3.70460034e-01  2.38755620e-01]\n",
      " [ 7.57919852e-01  8.56724687e-01  7.07904849e-01  1.16510824e+09\n",
      "   7.01558973e-01  5.30423046e+09  4.41265043e-01  5.01049261e+09\n",
      "   6.77692648e-01  6.27802159e-01  5.21668032e-01  3.61458818e-01]\n",
      " [ 7.94867622e-01  8.04279121e-01  8.39886644e-01  1.14452396e+09\n",
      "   7.18647920e-01  3.45244244e+09  6.57826807e-01  5.59657106e+09\n",
      "   6.91391895e-01  6.86069325e-01  6.33360416e-01  4.13311486e-01]]\n",
      "Policy matrix after 318001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 321001 iterations:\n",
      "[[ 8.01916575e-01  8.18614965e-01  9.13048856e-01  6.94396100e+09\n",
      "   7.96293044e-01  8.55541615e+09  6.98324072e-01  2.26406827e+09\n",
      "   7.38390741e-01  5.82849173e-01  6.24948935e-01 -7.18472021e-01]\n",
      " [ 8.45205230e-01  9.04825268e-01  9.56338982e-01  5.51195807e+09\n",
      "   7.43475055e-01  2.12100012e+09 -6.43314595e-01  2.14481050e+09\n",
      "   5.88428214e-01  5.18163041e-01  3.70510177e-01  2.38391298e-01]\n",
      " [ 7.58141444e-01  8.56891463e-01  7.08219194e-01  1.16510824e+09\n",
      "   7.01582767e-01  5.30423046e+09  4.40280400e-01  5.01049261e+09\n",
      "   6.77659325e-01  6.27064684e-01  5.22817343e-01  3.61754682e-01]\n",
      " [ 7.95238192e-01  8.04287542e-01  8.39402557e-01  1.14452396e+09\n",
      "   7.19044216e-01  3.45244244e+09  6.57741597e-01  5.59657106e+09\n",
      "   6.91378086e-01  6.85887470e-01  6.34076955e-01  4.13139407e-01]]\n",
      "Policy matrix after 321001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 324001 iterations:\n",
      "[[ 8.02181340e-01  8.19273399e-01  9.13126278e-01  6.94396100e+09\n",
      "   7.96275959e-01  8.55541615e+09  6.97901135e-01  2.26406827e+09\n",
      "   7.38443248e-01  5.83431533e-01  6.24818129e-01 -7.19622148e-01]\n",
      " [ 8.45228596e-01  9.04741572e-01  9.56280324e-01  5.51195807e+09\n",
      "   7.43520816e-01  2.12100012e+09 -6.43411338e-01  2.14481050e+09\n",
      "   5.89145766e-01  5.18609074e-01  3.70667092e-01  2.38451308e-01]\n",
      " [ 7.58097929e-01  8.56932700e-01  7.07412679e-01  1.16510824e+09\n",
      "   7.01564299e-01  5.30423046e+09  4.39267802e-01  5.01049261e+09\n",
      "   6.77645150e-01  6.27514354e-01  5.23311711e-01  3.62173440e-01]\n",
      " [ 7.95128187e-01  8.04341211e-01  8.38937481e-01  1.14452396e+09\n",
      "   7.19027202e-01  3.45244244e+09  6.57079681e-01  5.59657106e+09\n",
      "   6.91718705e-01  6.86024148e-01  6.34254407e-01  4.13127457e-01]]\n",
      "Policy matrix after 324001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 327001 iterations:\n",
      "[[ 8.01816657e-01  8.19872312e-01  9.13383438e-01  6.94396100e+09\n",
      "   7.96288034e-01  8.55541615e+09  6.97902792e-01  2.26406827e+09\n",
      "   7.38429393e-01  5.84084011e-01  6.24811659e-01 -7.19352063e-01]\n",
      " [ 8.45251404e-01  9.04750327e-01  9.56252550e-01  5.51195807e+09\n",
      "   7.43694031e-01  2.12100012e+09 -6.43360169e-01  2.14481050e+09\n",
      "   5.89388802e-01  5.19122108e-01  3.71825389e-01  2.37040032e-01]\n",
      " [ 7.58274023e-01  8.56782181e-01  7.06692653e-01  1.16510824e+09\n",
      "   7.01840914e-01  5.30423046e+09  4.39509748e-01  5.01049261e+09\n",
      "   6.78182121e-01  6.27992120e-01  5.24083674e-01  3.62938224e-01]\n",
      " [ 7.95413047e-01  8.04729732e-01  8.38763025e-01  1.14452396e+09\n",
      "   7.19576681e-01  3.45244244e+09  6.57240936e-01  5.59657106e+09\n",
      "   6.91523694e-01  6.85998668e-01  6.34401642e-01  4.13088742e-01]]\n",
      "Policy matrix after 327001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 330001 iterations:\n",
      "[[ 8.02168895e-01  8.20480994e-01  9.13433692e-01  6.94396100e+09\n",
      "   7.96286147e-01  8.55541615e+09  6.97979719e-01  2.26406827e+09\n",
      "   7.38379514e-01  5.84469814e-01  6.24998577e-01 -7.18960267e-01]\n",
      " [ 8.45267449e-01  9.04747348e-01  9.56247229e-01  5.51195807e+09\n",
      "   7.44032066e-01  2.12100012e+09 -6.43699583e-01  2.14481050e+09\n",
      "   5.90314267e-01  5.20438897e-01  3.73118303e-01  2.36462731e-01]\n",
      " [ 7.58376858e-01  8.56154666e-01  7.05628735e-01  1.16510824e+09\n",
      "   7.01979011e-01  5.30423046e+09  4.39627622e-01  5.01049261e+09\n",
      "   6.77942447e-01  6.27929419e-01  5.24519677e-01  3.63609961e-01]\n",
      " [ 7.95517333e-01  8.05001912e-01  8.38883423e-01  1.14452396e+09\n",
      "   7.19751681e-01  3.45244244e+09  6.57639818e-01  5.59657106e+09\n",
      "   6.91607014e-01  6.86055470e-01  6.34920573e-01  4.13236593e-01]]\n",
      "Policy matrix after 330001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 333001 iterations:\n",
      "[[ 8.02406550e-01  8.20583423e-01  9.13221753e-01  6.94396100e+09\n",
      "   7.96394961e-01  8.55541615e+09  6.97860567e-01  2.26406827e+09\n",
      "   7.38529979e-01  5.85192379e-01  6.25029149e-01 -7.19361630e-01]\n",
      " [ 8.45352712e-01  9.04794182e-01  9.56291822e-01  5.51195807e+09\n",
      "   7.43991414e-01  2.12100012e+09 -6.44139898e-01  2.14481050e+09\n",
      "   5.91269789e-01  5.21272246e-01  3.74253612e-01  2.35738082e-01]\n",
      " [ 7.58599005e-01  8.56545078e-01  7.05366609e-01  1.16510824e+09\n",
      "   7.02059127e-01  5.30423046e+09  4.40387435e-01  5.01049261e+09\n",
      "   6.77875996e-01  6.28110195e-01  5.25097427e-01  3.63946862e-01]\n",
      " [ 7.95413171e-01  8.05098784e-01  8.39141480e-01  1.14452396e+09\n",
      "   7.20396313e-01  3.45244244e+09  6.56923655e-01  5.59657106e+09\n",
      "   6.91968105e-01  6.86318996e-01  6.35853164e-01  4.13432963e-01]]\n",
      "Policy matrix after 333001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 336001 iterations:\n",
      "[[ 8.02515029e-01  8.20941397e-01  9.13277701e-01  6.94396100e+09\n",
      "   7.96480302e-01  8.55541615e+09  6.97683283e-01  2.26406827e+09\n",
      "   7.38647146e-01  5.85887241e-01  6.24885365e-01 -7.19878763e-01]\n",
      " [ 8.45412976e-01  9.04811705e-01  9.56284053e-01  5.51195807e+09\n",
      "   7.44328980e-01  2.12100012e+09 -6.43983302e-01  2.14481050e+09\n",
      "   5.92062493e-01  5.22025700e-01  3.73823305e-01  2.36450793e-01]\n",
      " [ 7.58415043e-01  8.56264213e-01  7.03755767e-01  1.16510824e+09\n",
      "   7.02271496e-01  5.30423046e+09  4.41357474e-01  5.01049261e+09\n",
      "   6.78236004e-01  6.28018732e-01  5.25625714e-01  3.64535158e-01]\n",
      " [ 7.95857249e-01  8.04908788e-01  8.39168305e-01  1.14452396e+09\n",
      "   7.20686363e-01  3.45244244e+09  6.57002493e-01  5.59657106e+09\n",
      "   6.92113781e-01  6.86485900e-01  6.35746926e-01  4.13560410e-01]]\n",
      "Policy matrix after 336001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 339001 iterations:\n",
      "[[ 8.02393077e-01  8.21376384e-01  9.13350466e-01  6.94396100e+09\n",
      "   7.96557696e-01  8.55541615e+09  6.97681487e-01  2.26406827e+09\n",
      "   7.38695446e-01  5.86600809e-01  6.24850537e-01 -7.19353172e-01]\n",
      " [ 8.45500027e-01  9.04826027e-01  9.56292759e-01  5.51195807e+09\n",
      "   7.44710743e-01  2.12100012e+09 -6.44200979e-01  2.14481050e+09\n",
      "   5.92540666e-01  5.22399678e-01  3.74624299e-01  2.36562016e-01]\n",
      " [ 7.58353737e-01  8.56268838e-01  7.02976747e-01  1.16510824e+09\n",
      "   7.02339274e-01  5.30423046e+09  4.40791319e-01  5.01049261e+09\n",
      "   6.77930620e-01  6.28350580e-01  5.26069787e-01  3.65554407e-01]\n",
      " [ 7.96032238e-01  8.05139697e-01  8.38554728e-01  1.14452396e+09\n",
      "   7.21210117e-01  3.45244244e+09  6.57067468e-01  5.59657106e+09\n",
      "   6.92524791e-01  6.86568682e-01  6.36227872e-01  4.14169697e-01]]\n",
      "Policy matrix after 339001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 342001 iterations:\n",
      "[[ 8.02431302e-01  8.21273198e-01  9.13661307e-01  6.94396100e+09\n",
      "   7.96523314e-01  8.55541615e+09  6.97675982e-01  2.26406827e+09\n",
      "   7.38580491e-01  5.86956531e-01  6.24800334e-01 -7.19109894e-01]\n",
      " [ 8.45467218e-01  9.04786745e-01  9.56292993e-01  5.51195807e+09\n",
      "   7.44863394e-01  2.12100012e+09 -6.43341050e-01  2.14481050e+09\n",
      "   5.92992968e-01  5.23188581e-01  3.74450231e-01  2.36518223e-01]\n",
      " [ 7.58772825e-01  8.56416207e-01  7.03499301e-01  1.16510824e+09\n",
      "   7.02491844e-01  5.30423046e+09  4.39793058e-01  5.01049261e+09\n",
      "   6.78113996e-01  6.27923615e-01  5.27120937e-01  3.65698208e-01]\n",
      " [ 7.96339088e-01  8.04689904e-01  8.38816259e-01  1.14452396e+09\n",
      "   7.21234401e-01  3.45244244e+09  6.58231918e-01  5.59657106e+09\n",
      "   6.92680051e-01  6.86496851e-01  6.36702055e-01  4.14106058e-01]]\n",
      "Policy matrix after 342001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 345001 iterations:\n",
      "[[ 8.02734967e-01  8.21925375e-01  9.13907714e-01  6.94396100e+09\n",
      "   7.96544977e-01  8.55541615e+09  6.97762031e-01  2.26406827e+09\n",
      "   7.38571502e-01  5.87411028e-01  6.24788140e-01 -7.17712838e-01]\n",
      " [ 8.45541827e-01  9.04853134e-01  9.56342636e-01  5.51195807e+09\n",
      "   7.44842649e-01  2.12100012e+09 -6.42988479e-01  2.14481050e+09\n",
      "   5.93338571e-01  5.24157397e-01  3.74444695e-01  2.36851297e-01]\n",
      " [ 7.59192699e-01  8.56337034e-01  7.04268191e-01  1.16510824e+09\n",
      "   7.02297245e-01  5.30423046e+09  4.39831106e-01  5.01049261e+09\n",
      "   6.78038404e-01  6.28425956e-01  5.27950867e-01  3.66034747e-01]\n",
      " [ 7.96640256e-01  8.04908672e-01  8.38734646e-01  1.14452396e+09\n",
      "   7.21752954e-01  3.45244244e+09  6.58047194e-01  5.59657106e+09\n",
      "   6.93065837e-01  6.86542340e-01  6.37108414e-01  4.14215995e-01]]\n",
      "Policy matrix after 345001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 348001 iterations:\n",
      "[[ 8.03094668e-01  8.22342907e-01  9.13969122e-01  6.94396100e+09\n",
      "   7.96606503e-01  8.55541615e+09  6.98007918e-01  2.26406827e+09\n",
      "   7.38672183e-01  5.87823672e-01  6.24828311e-01 -7.18170378e-01]\n",
      " [ 8.45632434e-01  9.04933011e-01  9.56407557e-01  5.51195807e+09\n",
      "   7.44532276e-01  2.12100012e+09 -6.43647893e-01  2.14481050e+09\n",
      "   5.94077140e-01  5.25105339e-01  3.75406032e-01  2.36806478e-01]\n",
      " [ 7.59358953e-01  8.56713675e-01  7.04679138e-01  1.16510824e+09\n",
      "   7.02625825e-01  5.30423046e+09  4.39955216e-01  5.01049261e+09\n",
      "   6.78584410e-01  6.28593151e-01  5.28238113e-01  3.65979373e-01]\n",
      " [ 7.96597541e-01  8.05210768e-01  8.38625634e-01  1.14452396e+09\n",
      "   7.22171035e-01  3.45244244e+09  6.58926102e-01  5.59657106e+09\n",
      "   6.93307512e-01  6.86654598e-01  6.37667644e-01  4.14410397e-01]]\n",
      "Policy matrix after 348001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 351001 iterations:\n",
      "[[ 8.03194976e-01  8.22552440e-01  9.13792941e-01  6.94396100e+09\n",
      "   7.96679697e-01  8.55541615e+09  6.97995591e-01  2.26406827e+09\n",
      "   7.38824507e-01  5.88385899e-01  6.24747626e-01 -7.18615133e-01]\n",
      " [ 8.45719174e-01  9.04954553e-01  9.56433586e-01  5.51195807e+09\n",
      "   7.44657122e-01  2.12100012e+09 -6.43406317e-01  2.14481050e+09\n",
      "   5.94831475e-01  5.25958072e-01  3.75236396e-01  2.37661821e-01]\n",
      " [ 7.59793577e-01  8.56198246e-01  7.05423545e-01  1.16510824e+09\n",
      "   7.02773299e-01  5.30423046e+09  4.39300022e-01  5.01049261e+09\n",
      "   6.79047258e-01  6.28704863e-01  5.28870550e-01  3.66095648e-01]\n",
      " [ 7.96747308e-01  8.05232017e-01  8.38732004e-01  1.14452396e+09\n",
      "   7.21862985e-01  3.45244244e+09  6.59189594e-01  5.59657106e+09\n",
      "   6.93463076e-01  6.86768770e-01  6.38152681e-01  4.14410004e-01]]\n",
      "Policy matrix after 351001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 354001 iterations:\n",
      "[[ 8.03400139e-01  8.22747843e-01  9.13561764e-01  6.94396100e+09\n",
      "   7.96556287e-01  8.55541615e+09  6.97728174e-01  2.26406827e+09\n",
      "   7.38740084e-01  5.88683449e-01  6.24677413e-01 -7.18246838e-01]\n",
      " [ 8.45643162e-01  9.04873622e-01  9.56352923e-01  5.51195807e+09\n",
      "   7.44601390e-01  2.12100012e+09 -6.43804548e-01  2.14481050e+09\n",
      "   5.95352543e-01  5.25868358e-01  3.75501601e-01  2.37427183e-01]\n",
      " [ 7.59573777e-01  8.56064955e-01  7.05643323e-01  1.16510824e+09\n",
      "   7.03109095e-01  5.30423046e+09  4.39868418e-01  5.01049261e+09\n",
      "   6.78825003e-01  6.28594338e-01  5.29671991e-01  3.66170533e-01]\n",
      " [ 7.96799769e-01  8.05556083e-01  8.38264554e-01  1.14452396e+09\n",
      "   7.22015808e-01  3.45244244e+09  6.58394657e-01  5.59657106e+09\n",
      "   6.93599100e-01  6.86729136e-01  6.38116475e-01  4.14212891e-01]]\n",
      "Policy matrix after 354001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 357001 iterations:\n",
      "[[ 8.03703526e-01  8.23412283e-01  9.13595176e-01  6.94396100e+09\n",
      "   7.96469760e-01  8.55541615e+09  6.97886804e-01  2.26406827e+09\n",
      "   7.38582438e-01  5.88847929e-01  6.24829560e-01 -7.18420962e-01]\n",
      " [ 8.45598004e-01  9.04836147e-01  9.56340733e-01  5.51195807e+09\n",
      "   7.44788799e-01  2.12100012e+09 -6.43650729e-01  2.14481050e+09\n",
      "   5.95631476e-01  5.26553047e-01  3.77103829e-01  2.36897259e-01]\n",
      " [ 7.59845710e-01  8.56221179e-01  7.05565866e-01  1.16510824e+09\n",
      "   7.02838952e-01  5.30423046e+09  4.40876685e-01  5.01049261e+09\n",
      "   6.79055965e-01  6.29023269e-01  5.30218335e-01  3.67243951e-01]\n",
      " [ 7.96847649e-01  8.05145482e-01  8.38653958e-01  1.14452396e+09\n",
      "   7.22325109e-01  3.45244244e+09  6.58902061e-01  5.59657106e+09\n",
      "   6.93647799e-01  6.86567457e-01  6.37976723e-01  4.14414825e-01]]\n",
      "Policy matrix after 357001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 360001 iterations:\n",
      "[[ 8.04083026e-01  8.23865512e-01  9.13648717e-01  6.94396100e+09\n",
      "   7.96485045e-01  8.55541615e+09  6.97909942e-01  2.26406827e+09\n",
      "   7.38603872e-01  5.89221424e-01  6.24696808e-01 -7.18384733e-01]\n",
      " [ 8.45631406e-01  9.04877713e-01  9.56372066e-01  5.51195807e+09\n",
      "   7.45057877e-01  2.12100012e+09 -6.44302157e-01  2.14481050e+09\n",
      "   5.96105257e-01  5.27009812e-01  3.78337300e-01  2.36759517e-01]\n",
      " [ 7.59923204e-01  8.56240583e-01  7.05872198e-01  1.16510824e+09\n",
      "   7.03061856e-01  5.30423046e+09  4.40117491e-01  5.01049261e+09\n",
      "   6.79155656e-01  6.29240253e-01  5.31433843e-01  3.67654325e-01]\n",
      " [ 7.96423420e-01  8.05476248e-01  8.38769998e-01  1.14452396e+09\n",
      "   7.22758971e-01  3.45244244e+09  6.58607913e-01  5.59657106e+09\n",
      "   6.93988019e-01  6.86540642e-01  6.38127505e-01  4.14928571e-01]]\n",
      "Policy matrix after 360001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 363001 iterations:\n",
      "[[ 8.04403788e-01  8.24068627e-01  9.13915552e-01  6.94396100e+09\n",
      "   7.96417439e-01  8.55541615e+09  6.97502236e-01  2.26406827e+09\n",
      "   7.38590809e-01  5.89819935e-01  6.24565649e-01 -7.18636980e-01]\n",
      " [ 8.45573518e-01  9.04803483e-01  9.56322193e-01  5.51195807e+09\n",
      "   7.45218356e-01  2.12100012e+09 -6.45480749e-01  2.14481050e+09\n",
      "   5.96771614e-01  5.27401469e-01  3.78445962e-01  2.36964688e-01]\n",
      " [ 7.60144026e-01  8.56442537e-01  7.05550718e-01  1.16510824e+09\n",
      "   7.02342722e-01  5.30423046e+09  4.39818594e-01  5.01049261e+09\n",
      "   6.79613684e-01  6.29370902e-01  5.31577206e-01  3.67240720e-01]\n",
      " [ 7.96707496e-01  8.05330238e-01  8.38819090e-01  1.14452396e+09\n",
      "   7.22964807e-01  3.45244244e+09  6.59006339e-01  5.59657106e+09\n",
      "   6.94090982e-01  6.86490917e-01  6.37640095e-01  4.14567952e-01]]\n",
      "Policy matrix after 363001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 366001 iterations:\n",
      "[[ 8.04209002e-01  8.24353189e-01  9.13530781e-01  6.94396100e+09\n",
      "   7.96470714e-01  8.55541615e+09  6.97367958e-01  2.26406827e+09\n",
      "   7.38655783e-01  5.90393394e-01  6.24573298e-01 -7.18288893e-01]\n",
      " [ 8.45609688e-01  9.04819354e-01  9.56316506e-01  5.51195807e+09\n",
      "   7.45330623e-01  2.12100012e+09 -6.44753825e-01  2.14481050e+09\n",
      "   5.97243659e-01  5.28265510e-01  3.79153725e-01  2.37459438e-01]\n",
      " [ 7.60535201e-01  8.56450152e-01  7.04520459e-01  1.16510824e+09\n",
      "   7.02620614e-01  5.30423046e+09  4.40268778e-01  5.01049261e+09\n",
      "   6.79937002e-01  6.29720200e-01  5.31733323e-01  3.67089127e-01]\n",
      " [ 7.96759378e-01  8.05493181e-01  8.39284481e-01  1.14452396e+09\n",
      "   7.22886494e-01  3.45244244e+09  6.58911510e-01  5.59657106e+09\n",
      "   6.94442210e-01  6.86546210e-01  6.37997069e-01  4.14889655e-01]]\n",
      "Policy matrix after 366001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 369001 iterations:\n",
      "[[ 8.04248413e-01  8.24748038e-01  9.13561566e-01  6.94396100e+09\n",
      "   7.96419212e-01  8.55541615e+09  6.97158007e-01  2.26406827e+09\n",
      "   7.38595176e-01  5.91019727e-01  6.24632158e-01 -7.18126333e-01]\n",
      " [ 8.45590148e-01  9.04811357e-01  9.56311224e-01  5.51195807e+09\n",
      "   7.45496854e-01  2.12100012e+09 -6.43799428e-01  2.14481050e+09\n",
      "   5.97330368e-01  5.28930392e-01  3.79549998e-01  2.36561036e-01]\n",
      " [ 7.60453360e-01  8.56751961e-01  7.03942209e-01  1.16510824e+09\n",
      "   7.02540264e-01  5.30423046e+09  4.40869825e-01  5.01049261e+09\n",
      "   6.80368920e-01  6.29866832e-01  5.32031629e-01  3.66851295e-01]\n",
      " [ 7.96776095e-01  8.05803123e-01  8.39396404e-01  1.14452396e+09\n",
      "   7.23146571e-01  3.45244244e+09  6.58505228e-01  5.59657106e+09\n",
      "   6.94510097e-01  6.86350505e-01  6.37098659e-01  4.14552424e-01]]\n",
      "Policy matrix after 369001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 372001 iterations:\n",
      "[[ 8.04275954e-01  8.25226807e-01  9.13594973e-01  6.94396100e+09\n",
      "   7.96532940e-01  8.55541615e+09  6.97155519e-01  2.26406827e+09\n",
      "   7.38712024e-01  5.91399233e-01  6.24724077e-01 -7.17627451e-01]\n",
      " [ 8.45679225e-01  9.04887612e-01  9.56345898e-01  5.51195807e+09\n",
      "   7.45631290e-01  2.12100012e+09 -6.44488865e-01  2.14481050e+09\n",
      "   5.97841134e-01  5.29354886e-01  3.79574320e-01  2.36930934e-01]\n",
      " [ 7.60674259e-01  8.56846239e-01  7.02874797e-01  1.16510824e+09\n",
      "   7.02801867e-01  5.30423046e+09  4.41103351e-01  5.01049261e+09\n",
      "   6.80336328e-01  6.29795802e-01  5.32673598e-01  3.67283020e-01]\n",
      " [ 7.96786540e-01  8.05973602e-01  8.39485941e-01  1.14452396e+09\n",
      "   7.23489995e-01  3.45244244e+09  6.58538294e-01  5.59657106e+09\n",
      "   6.94757266e-01  6.86435014e-01  6.37666206e-01  4.14795485e-01]]\n",
      "Policy matrix after 372001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 375001 iterations:\n",
      "[[ 8.04566184e-01  8.24962514e-01  9.13617378e-01  6.94396100e+09\n",
      "   7.96515043e-01  8.55541615e+09  6.97115223e-01  2.26406827e+09\n",
      "   7.38691305e-01  5.91468329e-01  6.24646642e-01 -7.17121368e-01]\n",
      " [ 8.45714799e-01  9.04864066e-01  9.56313617e-01  5.51195807e+09\n",
      "   7.45706558e-01  2.12100012e+09 -6.45059659e-01  2.14481050e+09\n",
      "   5.98195517e-01  5.30524989e-01  3.80193663e-01  2.36998267e-01]\n",
      " [ 7.60482629e-01  8.56592676e-01  7.02878287e-01  1.16510824e+09\n",
      "   7.02963410e-01  5.30423046e+09  4.41058864e-01  5.01049261e+09\n",
      "   6.79928848e-01  6.29766135e-01  5.33053091e-01  3.67813101e-01]\n",
      " [ 7.96704353e-01  8.06108553e-01  8.39367265e-01  1.14452396e+09\n",
      "   7.23762967e-01  3.45244244e+09  6.58350718e-01  5.59657106e+09\n",
      "   6.95113133e-01  6.86469753e-01  6.37913467e-01  4.14931625e-01]]\n",
      "Policy matrix after 375001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 378001 iterations:\n",
      "[[ 8.04644103e-01  8.25195673e-01  9.13644455e-01  6.94396100e+09\n",
      "   7.96592179e-01  8.55541615e+09  6.97398325e-01  2.26406827e+09\n",
      "   7.38820918e-01  5.91937527e-01  6.24722218e-01 -7.16818565e-01]\n",
      " [ 8.45776299e-01  9.04910717e-01  9.56369955e-01  5.51195807e+09\n",
      "   7.45808541e-01  2.12100012e+09 -6.46223770e-01  2.14481050e+09\n",
      "   5.98753875e-01  5.31221647e-01  3.80761130e-01  2.37903205e-01]\n",
      " [ 7.60822042e-01  8.56762495e-01  7.03377153e-01  1.16510824e+09\n",
      "   7.03082834e-01  5.30423046e+09  4.40956157e-01  5.01049261e+09\n",
      "   6.80093177e-01  6.30204158e-01  5.33708244e-01  3.67981958e-01]\n",
      " [ 7.96600789e-01  8.06149927e-01  8.39511791e-01  1.14452396e+09\n",
      "   7.24010088e-01  3.45244244e+09  6.58348406e-01  5.59657106e+09\n",
      "   6.95218177e-01  6.86630261e-01  6.38386151e-01  4.15249483e-01]]\n",
      "Policy matrix after 378001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 381001 iterations:\n",
      "[[ 8.04826807e-01  8.25562303e-01  9.13815369e-01  6.94396100e+09\n",
      "   7.96624222e-01  8.55541615e+09  6.97489771e-01  2.26406827e+09\n",
      "   7.38877763e-01  5.92421415e-01  6.24718889e-01 -7.15908060e-01]\n",
      " [ 8.45802987e-01  9.04947084e-01  9.56397778e-01  5.51195807e+09\n",
      "   7.45762419e-01  2.12100012e+09 -6.46599315e-01  2.14481050e+09\n",
      "   5.99159881e-01  5.31755873e-01  3.81077457e-01  2.38584707e-01]\n",
      " [ 7.61164449e-01  8.56483814e-01  7.03805612e-01  1.16510824e+09\n",
      "   7.03021416e-01  5.30423046e+09  4.40851252e-01  5.01049261e+09\n",
      "   6.80341525e-01  6.30221319e-01  5.34470172e-01  3.67498803e-01]\n",
      " [ 7.96400647e-01  8.06495259e-01  8.39552178e-01  1.14452396e+09\n",
      "   7.24391447e-01  3.45244244e+09  6.58597838e-01  5.59657106e+09\n",
      "   6.95084241e-01  6.86718890e-01  6.38872889e-01  4.15397022e-01]]\n",
      "Policy matrix after 381001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 384001 iterations:\n",
      "[[ 8.04732159e-01  8.25879867e-01  9.13902689e-01  6.94396100e+09\n",
      "   7.96634242e-01  8.55541615e+09  6.97297658e-01  2.26406827e+09\n",
      "   7.38876391e-01  5.92963055e-01  6.24604831e-01 -7.15771908e-01]\n",
      " [ 8.45817172e-01  9.04970535e-01  9.56389131e-01  5.51195807e+09\n",
      "   7.46041377e-01  2.12100012e+09 -6.46691134e-01  2.14481050e+09\n",
      "   5.99930847e-01  5.32285589e-01  3.81916681e-01  2.38280371e-01]\n",
      " [ 7.61450728e-01  8.56497625e-01  7.02593995e-01  1.16510824e+09\n",
      "   7.02983033e-01  5.30423046e+09  4.40516760e-01  5.01049261e+09\n",
      "   6.80648971e-01  6.30220689e-01  5.35083369e-01  3.67534685e-01]\n",
      " [ 7.96355921e-01  8.06696236e-01  8.39661672e-01  1.14452396e+09\n",
      "   7.24392494e-01  3.45244244e+09  6.59433631e-01  5.59657106e+09\n",
      "   6.95152186e-01  6.86745944e-01  6.38698473e-01  4.15286423e-01]]\n",
      "Policy matrix after 384001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 387001 iterations:\n",
      "[[ 8.04852370e-01  8.26355761e-01  9.14228432e-01  6.94396100e+09\n",
      "   7.96707399e-01  8.55541615e+09  6.97239107e-01  2.26406827e+09\n",
      "   7.38944870e-01  5.93195074e-01  6.24545753e-01 -7.16264210e-01]\n",
      " [ 8.45906372e-01  9.05038595e-01  9.56429225e-01  5.51195807e+09\n",
      "   7.46165523e-01  2.12100012e+09 -6.46720916e-01  2.14481050e+09\n",
      "   6.00659096e-01  5.32522369e-01  3.82898689e-01  2.38797656e-01]\n",
      " [ 7.61604765e-01  8.56767701e-01  7.02401266e-01  1.16510824e+09\n",
      "   7.03291120e-01  5.30423046e+09  4.40368107e-01  5.01049261e+09\n",
      "   6.80867473e-01  6.30460250e-01  5.35639418e-01  3.68010859e-01]\n",
      " [ 7.96237115e-01  8.06623057e-01  8.39999455e-01  1.14452396e+09\n",
      "   7.24911147e-01  3.45244244e+09  6.58666825e-01  5.59657106e+09\n",
      "   6.95086446e-01  6.86885928e-01  6.38894718e-01  4.15482414e-01]]\n",
      "Policy matrix after 387001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 390001 iterations:\n",
      "[[ 8.04923636e-01  8.26683242e-01  9.14181264e-01  6.94396100e+09\n",
      "   7.96740477e-01  8.55541615e+09  6.97286198e-01  2.26406827e+09\n",
      "   7.38999814e-01  5.93185797e-01  6.24570826e-01 -7.16199662e-01]\n",
      " [ 8.45955288e-01  9.05087626e-01  9.56463516e-01  5.51195807e+09\n",
      "   7.46038819e-01  2.12100012e+09 -6.45857998e-01  2.14481050e+09\n",
      "   6.01249041e-01  5.33140277e-01  3.82589983e-01  2.38976842e-01]\n",
      " [ 7.61689446e-01  8.57054499e-01  7.02491569e-01  1.16510824e+09\n",
      "   7.03547783e-01  5.30423046e+09  4.40821292e-01  5.01049261e+09\n",
      "   6.80692789e-01  6.30866677e-01  5.35947346e-01  3.68295224e-01]\n",
      " [ 7.96285320e-01  8.06639388e-01  8.39755017e-01  1.14452396e+09\n",
      "   7.25238310e-01  3.45244244e+09  6.58987665e-01  5.59657106e+09\n",
      "   6.95341673e-01  6.86990963e-01  6.39205691e-01  4.15457238e-01]]\n",
      "Policy matrix after 390001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 393001 iterations:\n",
      "[[ 8.04998826e-01  8.27047642e-01  9.14235884e-01  6.94396100e+09\n",
      "   7.96826454e-01  8.55541615e+09  6.97245098e-01  2.26406827e+09\n",
      "   7.39170804e-01  5.93304950e-01  6.24563278e-01 -7.15777564e-01]\n",
      " [ 8.46060062e-01  9.05174676e-01  9.56525601e-01  5.51195807e+09\n",
      "   7.45742880e-01  2.12100012e+09 -6.45901925e-01  2.14481050e+09\n",
      "   6.01769709e-01  5.33724107e-01  3.83952377e-01  2.39894277e-01]\n",
      " [ 7.62046699e-01  8.56959776e-01  7.02893783e-01  1.16510824e+09\n",
      "   7.03719951e-01  5.30423046e+09  4.41310851e-01  5.01049261e+09\n",
      "   6.80806461e-01  6.30921954e-01  5.36140491e-01  3.68780318e-01]\n",
      " [ 7.96467636e-01  8.06972553e-01  8.39924299e-01  1.14452396e+09\n",
      "   7.25576573e-01  3.45244244e+09  6.58780000e-01  5.59657106e+09\n",
      "   6.95319065e-01  6.87154511e-01  6.39291809e-01  4.15880568e-01]]\n",
      "Policy matrix after 393001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 396001 iterations:\n",
      "[[ 8.05157023e-01  8.27280534e-01  9.13936414e-01  6.94396100e+09\n",
      "   7.96854682e-01  8.55541615e+09  6.97086041e-01  2.26406827e+09\n",
      "   7.39222004e-01  5.93742756e-01  6.24671527e-01 -7.15784667e-01]\n",
      " [ 8.46098037e-01  9.05193570e-01  9.56528019e-01  5.51195807e+09\n",
      "   7.45958094e-01  2.12100012e+09 -6.45765855e-01  2.14481050e+09\n",
      "   6.02458997e-01  5.34589855e-01  3.83367805e-01  2.40910138e-01]\n",
      " [ 7.62110716e-01  8.56759913e-01  7.02520951e-01  1.16510824e+09\n",
      "   7.03936690e-01  5.30423046e+09  4.40953860e-01  5.01049261e+09\n",
      "   6.80665892e-01  6.31079627e-01  5.36644227e-01  3.68838472e-01]\n",
      " [ 7.96553545e-01  8.07177709e-01  8.40401650e-01  1.14452396e+09\n",
      "   7.25448358e-01  3.45244244e+09  6.58656263e-01  5.59657106e+09\n",
      "   6.95345219e-01  6.87295901e-01  6.39426031e-01  4.15948022e-01]]\n",
      "Policy matrix after 396001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 399001 iterations:\n",
      "[[ 8.05461967e-01  8.27563292e-01  9.14199973e-01  6.94396100e+09\n",
      "   7.96821265e-01  8.55541615e+09  6.96792033e-01  2.26406827e+09\n",
      "   7.39135795e-01  5.94062132e-01  6.24515925e-01 -7.15194475e-01]\n",
      " [ 8.46090303e-01  9.05166638e-01  9.56502868e-01  5.51195807e+09\n",
      "   7.46272123e-01  2.12100012e+09 -6.46512894e-01  2.14481050e+09\n",
      "   6.02947339e-01  5.35126420e-01  3.83441798e-01  2.40888143e-01]\n",
      " [ 7.62110199e-01  8.56731748e-01  7.02640807e-01  1.16510824e+09\n",
      "   7.03750311e-01  5.30423046e+09  4.40450361e-01  5.01049261e+09\n",
      "   6.80672245e-01  6.30837836e-01  5.37250720e-01  3.69223784e-01]\n",
      " [ 7.96829459e-01  8.07378924e-01  8.39997857e-01  1.14452396e+09\n",
      "   7.25447791e-01  3.45244244e+09  6.58759375e-01  5.59657106e+09\n",
      "   6.95272196e-01  6.87278283e-01  6.39136724e-01  4.15893558e-01]]\n",
      "Policy matrix after 399001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 402001 iterations:\n",
      "[[ 8.05560820e-01  8.27564135e-01  9.14444370e-01  6.94396100e+09\n",
      "   7.96897175e-01  8.55541615e+09  6.96880948e-01  2.26406827e+09\n",
      "   7.39194214e-01  5.94609880e-01  6.24421140e-01 -7.15146101e-01]\n",
      " [ 8.46189827e-01  9.05234007e-01  9.56536841e-01  5.51195807e+09\n",
      "   7.46407936e-01  2.12100012e+09 -6.46363085e-01  2.14481050e+09\n",
      "   6.03363964e-01  5.35857266e-01  3.83984973e-01  2.40888489e-01]\n",
      " [ 7.62415420e-01  8.57040551e-01  7.02359456e-01  1.16510824e+09\n",
      "   7.03918403e-01  5.30423046e+09  4.40343478e-01  5.01049261e+09\n",
      "   6.81167039e-01  6.31293142e-01  5.36751047e-01  3.69721726e-01]\n",
      " [ 7.97088273e-01  8.07319640e-01  8.40211866e-01  1.14452396e+09\n",
      "   7.25762171e-01  3.45244244e+09  6.58964807e-01  5.59657106e+09\n",
      "   6.95583957e-01  6.87282537e-01  6.39049749e-01  4.15903517e-01]]\n",
      "Policy matrix after 402001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 405001 iterations:\n",
      "[[ 8.05588236e-01  8.27533106e-01  9.14507161e-01  6.94396100e+09\n",
      "   7.96931126e-01  8.55541615e+09  6.96882639e-01  2.26406827e+09\n",
      "   7.39197607e-01  5.95087072e-01  6.24579867e-01 -7.13648782e-01]\n",
      " [ 8.46219201e-01  9.05246218e-01  9.56546780e-01  5.51195807e+09\n",
      "   7.46521540e-01  2.12100012e+09 -6.46886781e-01  2.14481050e+09\n",
      "   6.03387644e-01  5.36670609e-01  3.83672651e-01  2.40629135e-01]\n",
      " [ 7.62528615e-01  8.57066642e-01  7.02435836e-01  1.16510824e+09\n",
      "   7.04136534e-01  5.30423046e+09  4.40723780e-01  5.01049261e+09\n",
      "   6.81276092e-01  6.31291184e-01  5.36975033e-01  3.69926350e-01]\n",
      " [ 7.97168052e-01  8.07403995e-01  8.40524121e-01  1.14452396e+09\n",
      "   7.25887297e-01  3.45244244e+09  6.59014202e-01  5.59657106e+09\n",
      "   6.95594578e-01  6.87321004e-01  6.39122667e-01  4.15837195e-01]]\n",
      "Policy matrix after 405001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 408001 iterations:\n",
      "[[ 8.05779310e-01  8.27816846e-01  9.14749934e-01  6.94396100e+09\n",
      "   7.96920674e-01  8.55541615e+09  6.96905675e-01  2.26406827e+09\n",
      "   7.39194870e-01  5.95520312e-01  6.24573455e-01 -7.13469673e-01]\n",
      " [ 8.46228801e-01  9.05271074e-01  9.56574765e-01  5.51195807e+09\n",
      "   7.46697487e-01  2.12100012e+09 -6.47741775e-01  2.14481050e+09\n",
      "   6.04168712e-01  5.37289680e-01  3.83721927e-01  2.41065785e-01]\n",
      " [ 7.62675103e-01  8.57098378e-01  7.03282183e-01  1.16510824e+09\n",
      "   7.04255647e-01  5.30423046e+09  4.40995040e-01  5.01049261e+09\n",
      "   6.81232494e-01  6.31594047e-01  5.37087890e-01  3.70258668e-01]\n",
      " [ 7.97236749e-01  8.07582141e-01  8.40796671e-01  1.14452396e+09\n",
      "   7.26155531e-01  3.45244244e+09  6.58722248e-01  5.59657106e+09\n",
      "   6.95673674e-01  6.87362152e-01  6.39248748e-01  4.16139428e-01]]\n",
      "Policy matrix after 408001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 411001 iterations:\n",
      "[[ 8.05925964e-01  8.28286683e-01  9.14902935e-01  6.94396100e+09\n",
      "   7.96820786e-01  8.55541615e+09  6.96939285e-01  2.26406827e+09\n",
      "   7.39066161e-01  5.95681641e-01  6.24498958e-01 -7.13512869e-01]\n",
      " [ 8.46170295e-01  9.05218902e-01  9.56566694e-01  5.51195807e+09\n",
      "   7.46922973e-01  2.12100012e+09 -6.46692218e-01  2.14481050e+09\n",
      "   6.04595982e-01  5.37569564e-01  3.83609067e-01  2.41093846e-01]\n",
      " [ 7.62549369e-01  8.57405128e-01  7.03308663e-01  1.16510824e+09\n",
      "   7.04284008e-01  5.30423046e+09  4.40810374e-01  5.01049261e+09\n",
      "   6.81489996e-01  6.31501009e-01  5.37124066e-01  3.70020984e-01]\n",
      " [ 7.97427368e-01  8.07401978e-01  8.40886350e-01  1.14452396e+09\n",
      "   7.26509443e-01  3.45244244e+09  6.59139252e-01  5.59657106e+09\n",
      "   6.95807725e-01  6.87235997e-01  6.39380380e-01  4.15973436e-01]]\n",
      "Policy matrix after 411001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 414001 iterations:\n",
      "[[ 8.05643687e-01  8.28511449e-01  9.15072891e-01  6.94396100e+09\n",
      "   7.96788014e-01  8.55541615e+09  6.96943971e-01  2.26406827e+09\n",
      "   7.39042311e-01  5.96064202e-01  6.24625168e-01 -7.13278904e-01]\n",
      " [ 8.46115042e-01  9.05154392e-01  9.56547899e-01  5.51195807e+09\n",
      "   7.47129873e-01  2.12100012e+09 -6.45968117e-01  2.14481050e+09\n",
      "   6.04640516e-01  5.38238742e-01  3.83682622e-01  2.41537144e-01]\n",
      " [ 7.62814785e-01  8.57149172e-01  7.03242897e-01  1.16510824e+09\n",
      "   7.04388104e-01  5.30423046e+09  4.40192362e-01  5.01049261e+09\n",
      "   6.81550822e-01  6.31402218e-01  5.37640708e-01  3.70462410e-01]\n",
      " [ 7.96740687e-01  8.07498802e-01  8.41023896e-01  1.14452396e+09\n",
      "   7.26464697e-01  3.45244244e+09  6.59141248e-01  5.59657106e+09\n",
      "   6.96150510e-01  6.87239480e-01  6.39567931e-01  4.16082492e-01]]\n",
      "Policy matrix after 414001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 417001 iterations:\n",
      "[[ 8.05776251e-01  8.28770414e-01  9.15134708e-01  6.94396100e+09\n",
      "   7.96816798e-01  8.55541615e+09  6.96973879e-01  2.26406827e+09\n",
      "   7.39099928e-01  5.96752107e-01  6.24615941e-01 -7.13089461e-01]\n",
      " [ 8.46182571e-01  9.05192584e-01  9.56562403e-01  5.51195807e+09\n",
      "   7.47397339e-01  2.12100012e+09 -6.45923731e-01  2.14481050e+09\n",
      "   6.05071621e-01  5.38645316e-01  3.83226376e-01  2.41434954e-01]\n",
      " [ 7.63042576e-01  8.56935252e-01  7.03269434e-01  1.16510824e+09\n",
      "   7.04276666e-01  5.30423046e+09  4.39760948e-01  5.01049261e+09\n",
      "   6.81963009e-01  6.31639907e-01  5.38032269e-01  3.70797596e-01]\n",
      " [ 7.96753142e-01  8.07545312e-01  8.40970645e-01  1.14452396e+09\n",
      "   7.26961108e-01  3.45244244e+09  6.58446153e-01  5.59657106e+09\n",
      "   6.96057759e-01  6.87299014e-01  6.39428366e-01  4.16009339e-01]]\n",
      "Policy matrix after 417001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 420001 iterations:\n",
      "[[ 8.05827092e-01  8.29229199e-01  9.14804100e-01  6.94396100e+09\n",
      "   7.96818728e-01  8.55541615e+09  6.96876398e-01  2.26406827e+09\n",
      "   7.39114397e-01  5.97048832e-01  6.24455297e-01 -7.12523350e-01]\n",
      " [ 8.46201632e-01  9.05164623e-01  9.56540484e-01  5.51195807e+09\n",
      "   7.47527949e-01  2.12100012e+09 -6.46355411e-01  2.14481050e+09\n",
      "   6.05628766e-01  5.38767425e-01  3.83789451e-01  2.41180065e-01]\n",
      " [ 7.63314216e-01  8.56131836e-01  7.03313719e-01  1.16510824e+09\n",
      "   7.04565972e-01  5.30423046e+09  4.40148914e-01  5.01049261e+09\n",
      "   6.82230708e-01  6.31735226e-01  5.38429177e-01  3.70327473e-01]\n",
      " [ 7.96944665e-01  8.07797527e-01  8.40950218e-01  1.14452396e+09\n",
      "   7.27143215e-01  3.45244244e+09  6.58668802e-01  5.59657106e+09\n",
      "   6.96155746e-01  6.87290557e-01  6.39578701e-01  4.16304900e-01]]\n",
      "Policy matrix after 420001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 423001 iterations:\n",
      "[[ 8.05926287e-01  8.29696843e-01  9.14662450e-01  6.94396100e+09\n",
      "   7.96790927e-01  8.55541615e+09  6.96855781e-01  2.26406827e+09\n",
      "   7.39091826e-01  5.97469257e-01  6.24513291e-01 -7.12708864e-01]\n",
      " [ 8.46196498e-01  9.05147144e-01  9.56534809e-01  5.51195807e+09\n",
      "   7.47295022e-01  2.12100012e+09 -6.45173481e-01  2.14481050e+09\n",
      "   6.06014870e-01  5.39218855e-01  3.84738217e-01  2.40194524e-01]\n",
      " [ 7.63434528e-01  8.55978351e-01  7.03845004e-01  1.16510824e+09\n",
      "   7.04597775e-01  5.30423046e+09  4.40765737e-01  5.01049261e+09\n",
      "   6.82432301e-01  6.32132108e-01  5.39026115e-01  3.71116599e-01]\n",
      " [ 7.96893264e-01  8.08082726e-01  8.41224608e-01  1.14452396e+09\n",
      "   7.27306873e-01  3.45244244e+09  6.58091425e-01  5.59657106e+09\n",
      "   6.96395069e-01  6.87236144e-01  6.39635692e-01  4.16219466e-01]]\n",
      "Policy matrix after 423001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 426001 iterations:\n",
      "[[ 8.05783142e-01  8.30089043e-01  9.14163862e-01  6.94396100e+09\n",
      "   7.96829978e-01  8.55541615e+09  6.96950840e-01  2.26406827e+09\n",
      "   7.39152513e-01  5.97706533e-01  6.24581383e-01 -7.12855845e-01]\n",
      " [ 8.46244544e-01  9.05173180e-01  9.56559045e-01  5.51195807e+09\n",
      "   7.47530143e-01  2.12100012e+09 -6.45030248e-01  2.14481050e+09\n",
      "   6.06412174e-01  5.40097861e-01  3.84967853e-01  2.40741579e-01]\n",
      " [ 7.63404731e-01  8.55689412e-01  7.03781879e-01  1.16510824e+09\n",
      "   7.04791485e-01  5.30423046e+09  4.41331753e-01  5.01049261e+09\n",
      "   6.82535171e-01  6.32482844e-01  5.38892480e-01  3.70960088e-01]\n",
      " [ 7.96846837e-01  8.08232929e-01  8.41274994e-01  1.14452396e+09\n",
      "   7.27465618e-01  3.45244244e+09  6.58621718e-01  5.59657106e+09\n",
      "   6.96475904e-01  6.87348946e-01  6.40032219e-01  4.16061533e-01]]\n",
      "Policy matrix after 426001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 429001 iterations:\n",
      "[[ 8.05772187e-01  8.30213793e-01  9.14168059e-01  6.94396100e+09\n",
      "   7.96855748e-01  8.55541615e+09  6.96851160e-01  2.26406827e+09\n",
      "   7.39200198e-01  5.98163087e-01  6.24604622e-01 -7.13145462e-01]\n",
      " [ 8.46270699e-01  9.05181611e-01  9.56550775e-01  5.51195807e+09\n",
      "   7.47715344e-01  2.12100012e+09 -6.44595119e-01  2.14481050e+09\n",
      "   6.06996766e-01  5.40573873e-01  3.84639844e-01  2.40750285e-01]\n",
      " [ 7.63311219e-01  8.55776408e-01  7.03862276e-01  1.16510824e+09\n",
      "   7.04525362e-01  5.30423046e+09  4.40380086e-01  5.01049261e+09\n",
      "   6.82892828e-01  6.32122439e-01  5.39151868e-01  3.71626401e-01]\n",
      " [ 7.96684793e-01  8.08453137e-01  8.41332079e-01  1.14452396e+09\n",
      "   7.27763163e-01  3.45244244e+09  6.58163778e-01  5.59657106e+09\n",
      "   6.96746347e-01  6.87372533e-01  6.39878262e-01  4.15925756e-01]]\n",
      "Policy matrix after 429001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 432001 iterations:\n",
      "[[ 8.05655604e-01  8.30491964e-01  9.14400083e-01  6.94396100e+09\n",
      "   7.96859809e-01  8.55541615e+09  6.96657080e-01  2.26406827e+09\n",
      "   7.39183700e-01  5.98616918e-01  6.24575982e-01 -7.12319667e-01]\n",
      " [ 8.46280412e-01  9.05186454e-01  9.56563739e-01  5.51195807e+09\n",
      "   7.47776191e-01  2.12100012e+09 -6.44377490e-01  2.14481050e+09\n",
      "   6.07480455e-01  5.41060444e-01  3.85132508e-01  2.41039752e-01]\n",
      " [ 7.63474236e-01  8.55867334e-01  7.03901486e-01  1.16510824e+09\n",
      "   7.04316082e-01  5.30423046e+09  4.39829277e-01  5.01049261e+09\n",
      "   6.83022515e-01  6.32255493e-01  5.39460701e-01  3.72018438e-01]\n",
      " [ 7.96752404e-01  8.08551174e-01  8.41564533e-01  1.14452396e+09\n",
      "   7.28018764e-01  3.45244244e+09  6.57990351e-01  5.59657106e+09\n",
      "   6.96812458e-01  6.87344581e-01  6.39664448e-01  4.15945499e-01]]\n",
      "Policy matrix after 432001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 435001 iterations:\n",
      "[[ 8.05786173e-01  8.30667206e-01  9.14251132e-01  6.94396100e+09\n",
      "   7.96734257e-01  8.55541615e+09  6.96517097e-01  2.26406827e+09\n",
      "   7.39049682e-01  5.98892113e-01  6.24646838e-01 -7.11217649e-01]\n",
      " [ 8.46186781e-01  9.05123281e-01  9.56521355e-01  5.51195807e+09\n",
      "   7.47495656e-01  2.12100012e+09 -6.43547376e-01  2.14481050e+09\n",
      "   6.07997543e-01  5.41471613e-01  3.85757886e-01  2.41226281e-01]\n",
      " [ 7.63747780e-01  8.55976984e-01  7.03085689e-01  1.16510824e+09\n",
      "   7.04248528e-01  5.30423046e+09  4.40365602e-01  5.01049261e+09\n",
      "   6.82999997e-01  6.32369921e-01  5.39216904e-01  3.71715933e-01]\n",
      " [ 7.96834469e-01  8.08578248e-01  8.41465396e-01  1.14452396e+09\n",
      "   7.27988343e-01  3.45244244e+09  6.58525216e-01  5.59657106e+09\n",
      "   6.96716293e-01  6.87168472e-01  6.39579551e-01  4.15931928e-01]]\n",
      "Policy matrix after 435001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 438001 iterations:\n",
      "[[ 8.05762118e-01  8.31096662e-01  9.14275735e-01  6.94396100e+09\n",
      "   7.96745497e-01  8.55541615e+09  6.96518784e-01  2.26406827e+09\n",
      "   7.39047392e-01  5.98991074e-01  6.24532885e-01 -7.11095983e-01]\n",
      " [ 8.46236358e-01  9.05177217e-01  9.56563237e-01  5.51195807e+09\n",
      "   7.47647549e-01  2.12100012e+09 -6.43305580e-01  2.14481050e+09\n",
      "   6.08465571e-01  5.42081311e-01  3.86327043e-01  2.41921398e-01]\n",
      " [ 7.63475209e-01  8.56074919e-01  7.03207243e-01  1.16510824e+09\n",
      "   7.04343888e-01  5.30423046e+09  4.40018393e-01  5.01049261e+09\n",
      "   6.82815126e-01  6.32465092e-01  5.39738206e-01  3.72263409e-01]\n",
      " [ 7.97108971e-01  8.08681694e-01  8.41776603e-01  1.14452396e+09\n",
      "   7.28512541e-01  3.45244244e+09  6.57829130e-01  5.59657106e+09\n",
      "   6.96833188e-01  6.87235620e-01  6.39729893e-01  4.16341663e-01]]\n",
      "Policy matrix after 438001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 441001 iterations:\n",
      "[[ 8.05772535e-01  8.31425466e-01  9.14013317e-01  6.94396100e+09\n",
      "   7.96674101e-01  8.55541615e+09  6.96396489e-01  2.26406827e+09\n",
      "   7.38969825e-01  5.99334491e-01  6.24491590e-01 -7.11573824e-01]\n",
      " [ 8.46212565e-01  9.05133236e-01  9.56513978e-01  5.51195807e+09\n",
      "   7.47727646e-01  2.12100012e+09 -6.43705044e-01  2.14481050e+09\n",
      "   6.08931487e-01  5.42564738e-01  3.85941970e-01  2.42377408e-01]\n",
      " [ 7.63389779e-01  8.56302622e-01  7.03647439e-01  1.16510824e+09\n",
      "   7.04617862e-01  5.30423046e+09  4.40430397e-01  5.01049261e+09\n",
      "   6.82842278e-01  6.32552861e-01  5.39622875e-01  3.72329433e-01]\n",
      " [ 7.97039651e-01  8.08955027e-01  8.41951242e-01  1.14452396e+09\n",
      "   7.28761665e-01  3.45244244e+09  6.57588109e-01  5.59657106e+09\n",
      "   6.96895695e-01  6.87160287e-01  6.39450709e-01  4.16234365e-01]]\n",
      "Policy matrix after 441001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 444001 iterations:\n",
      "[[ 8.05667664e-01  8.31623205e-01  9.14042620e-01  6.94396100e+09\n",
      "   7.96654537e-01  8.55541615e+09  6.96328265e-01  2.26406827e+09\n",
      "   7.38875661e-01  5.99597050e-01  6.24572569e-01 -7.11841302e-01]\n",
      " [ 8.46184571e-01  9.05095657e-01  9.56484897e-01  5.51195807e+09\n",
      "   7.47968629e-01  2.12100012e+09 -6.43485903e-01  2.14481050e+09\n",
      "   6.09417756e-01  5.43176091e-01  3.86772766e-01  2.42083663e-01]\n",
      " [ 7.63620258e-01  8.56372982e-01  7.03905350e-01  1.16510824e+09\n",
      "   7.04702336e-01  5.30423046e+09  4.40288454e-01  5.01049261e+09\n",
      "   6.82637426e-01  6.32508621e-01  5.39559632e-01  3.72036179e-01]\n",
      " [ 7.97303255e-01  8.08854011e-01  8.41995917e-01  1.14452396e+09\n",
      "   7.28876978e-01  3.45244244e+09  6.57277492e-01  5.59657106e+09\n",
      "   6.96875430e-01  6.87073090e-01  6.39495662e-01  4.16273458e-01]]\n",
      "Policy matrix after 444001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 447001 iterations:\n",
      "[[ 8.06015130e-01  8.31838971e-01  9.14102240e-01  6.94396100e+09\n",
      "   7.96675478e-01  8.55541615e+09  6.96227177e-01  2.26406827e+09\n",
      "   7.38858694e-01  5.99916448e-01  6.24587263e-01 -7.11686346e-01]\n",
      " [ 8.46217162e-01  9.05099762e-01  9.56459607e-01  5.51195807e+09\n",
      "   7.47942761e-01  2.12100012e+09 -6.43119247e-01  2.14481050e+09\n",
      "   6.09787726e-01  5.43461938e-01  3.87692248e-01  2.41301230e-01]\n",
      " [ 7.63467800e-01  8.56631846e-01  7.03452690e-01  1.16510824e+09\n",
      "   7.04394655e-01  5.30423046e+09  4.39740668e-01  5.01049261e+09\n",
      "   6.82854156e-01  6.32533361e-01  5.39543073e-01  3.72776151e-01]\n",
      " [ 7.97255647e-01  8.08859390e-01  8.41872950e-01  1.14452396e+09\n",
      "   7.29231985e-01  3.45244244e+09  6.57294251e-01  5.59657106e+09\n",
      "   6.96727035e-01  6.87048412e-01  6.39338694e-01  4.16425171e-01]]\n",
      "Policy matrix after 447001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 450001 iterations:\n",
      "[[ 8.05750441e-01  8.31911675e-01  9.14131627e-01  6.94396100e+09\n",
      "   7.96674422e-01  8.55541615e+09  6.96009948e-01  2.26406827e+09\n",
      "   7.38820948e-01  6.00187265e-01  6.24420896e-01 -7.11850968e-01]\n",
      " [ 8.46218085e-01  9.05081223e-01  9.56416490e-01  5.51195807e+09\n",
      "   7.47937416e-01  2.12100012e+09 -6.42099020e-01  2.14481050e+09\n",
      "   6.09934680e-01  5.43284377e-01  3.88062760e-01  2.40562173e-01]\n",
      " [ 7.63704685e-01  8.56921693e-01  7.02716551e-01  1.16510824e+09\n",
      "   7.04620124e-01  5.30423046e+09  4.39526152e-01  5.01049261e+09\n",
      "   6.83029815e-01  6.32682884e-01  5.39674564e-01  3.72887839e-01]\n",
      " [ 7.97283932e-01  8.09021181e-01  8.42310260e-01  1.14452396e+09\n",
      "   7.29717928e-01  3.45244244e+09  6.57324856e-01  5.59657106e+09\n",
      "   6.96875579e-01  6.86939865e-01  6.39194583e-01  4.16354051e-01]]\n",
      "Policy matrix after 450001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 453001 iterations:\n",
      "[[ 8.05834891e-01  8.32055056e-01  9.14199765e-01  6.94396100e+09\n",
      "   7.96725831e-01  8.55541615e+09  6.96174904e-01  2.26406827e+09\n",
      "   7.38879297e-01  6.00581483e-01  6.24497305e-01 -7.11197515e-01]\n",
      " [ 8.46247042e-01  9.05106861e-01  9.56440830e-01  5.51195807e+09\n",
      "   7.48087998e-01  2.12100012e+09 -6.41361448e-01  2.14481050e+09\n",
      "   6.10174370e-01  5.43633600e-01  3.88296211e-01  2.41552471e-01]\n",
      " [ 7.63859143e-01  8.57100981e-01  7.02873857e-01  1.16510824e+09\n",
      "   7.04761775e-01  5.30423046e+09  4.38901913e-01  5.01049261e+09\n",
      "   6.83136458e-01  6.32696269e-01  5.39576481e-01  3.73942869e-01]\n",
      " [ 7.96834121e-01  8.09191334e-01  8.41969841e-01  1.14452396e+09\n",
      "   7.29868417e-01  3.45244244e+09  6.56841926e-01  5.59657106e+09\n",
      "   6.97077591e-01  6.86979355e-01  6.39437955e-01  4.16731147e-01]]\n",
      "Policy matrix after 453001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 456001 iterations:\n",
      "[[ 8.06116030e-01  8.32191762e-01  9.13965030e-01  6.94396100e+09\n",
      "   7.96692143e-01  8.55541615e+09  6.95947093e-01  2.26406827e+09\n",
      "   7.38908735e-01  6.00841397e-01  6.24573369e-01 -7.11235042e-01]\n",
      " [ 8.46247207e-01  9.05091886e-01  9.56410861e-01  5.51195807e+09\n",
      "   7.47870856e-01  2.12100012e+09 -6.40829731e-01  2.14481050e+09\n",
      "   6.10746655e-01  5.44378951e-01  3.88801066e-01  2.41767915e-01]\n",
      " [ 7.63783768e-01  8.57182027e-01  7.02669428e-01  1.16510824e+09\n",
      "   7.04771921e-01  5.30423046e+09  4.39236699e-01  5.01049261e+09\n",
      "   6.82797440e-01  6.32814904e-01  5.39792787e-01  3.74379982e-01]\n",
      " [ 7.96957311e-01  8.09186648e-01  8.42322219e-01  1.14452396e+09\n",
      "   7.29653277e-01  3.45244244e+09  6.56618037e-01  5.59657106e+09\n",
      "   6.96950905e-01  6.87115421e-01  6.39555938e-01  4.17044111e-01]]\n",
      "Policy matrix after 456001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 459001 iterations:\n",
      "[[ 8.06067018e-01  8.32410151e-01  9.14129768e-01  6.94396100e+09\n",
      "   7.96688759e-01  8.55541615e+09  6.95942674e-01  2.26406827e+09\n",
      "   7.38902198e-01  6.00946057e-01  6.24669272e-01 -7.11706109e-01]\n",
      " [ 8.46252713e-01  9.05091005e-01  9.56415624e-01  5.51195807e+09\n",
      "   7.47800558e-01  2.12100012e+09 -6.41272020e-01  2.14481050e+09\n",
      "   6.11037283e-01  5.45176914e-01  3.88978290e-01  2.42014969e-01]\n",
      " [ 7.64058877e-01  8.57396769e-01  7.02612553e-01  1.16510824e+09\n",
      "   7.04951067e-01  5.30423046e+09  4.39119915e-01  5.01049261e+09\n",
      "   6.82589512e-01  6.32817541e-01  5.39975602e-01  3.74982474e-01]\n",
      " [ 7.96943798e-01  8.09307776e-01  8.42317569e-01  1.14452396e+09\n",
      "   7.29884790e-01  3.45244244e+09  6.56232017e-01  5.59657106e+09\n",
      "   6.97043688e-01  6.87118510e-01  6.39606845e-01  4.17092699e-01]]\n",
      "Policy matrix after 459001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 462001 iterations:\n",
      "[[ 8.06304470e-01  8.32693682e-01  9.14125274e-01  6.94396100e+09\n",
      "   7.96688172e-01  8.55541615e+09  6.95977592e-01  2.26406827e+09\n",
      "   7.38870962e-01  6.01117799e-01  6.24648874e-01 -7.12177815e-01]\n",
      " [ 8.46288554e-01  9.05114283e-01  9.56454322e-01  5.51195807e+09\n",
      "   7.48016544e-01  2.12100012e+09 -6.41920895e-01  2.14481050e+09\n",
      "   6.11363548e-01  5.45757777e-01  3.89094845e-01  2.42087406e-01]\n",
      " [ 7.64301990e-01  8.57634197e-01  7.02656406e-01  1.16510824e+09\n",
      "   7.05208686e-01  5.30423046e+09  4.39129809e-01  5.01049261e+09\n",
      "   6.82580023e-01  6.32980897e-01  5.40405107e-01  3.75480429e-01]\n",
      " [ 7.97138526e-01  8.09394682e-01  8.42261802e-01  1.14452396e+09\n",
      "   7.29825306e-01  3.45244244e+09  6.56286597e-01  5.59657106e+09\n",
      "   6.97159712e-01  6.87082252e-01  6.39733027e-01  4.17057095e-01]]\n",
      "Policy matrix after 462001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 465001 iterations:\n",
      "[[ 8.06527022e-01  8.32808779e-01  9.14183412e-01  6.94396100e+09\n",
      "   7.96691696e-01  8.55541615e+09  6.96107857e-01  2.26406827e+09\n",
      "   7.38929665e-01  6.01197571e-01  6.24611414e-01 -7.11149545e-01]\n",
      " [ 8.46321543e-01  9.05158453e-01  9.56483128e-01  5.51195807e+09\n",
      "   7.47898590e-01  2.12100012e+09 -6.42367417e-01  2.14481050e+09\n",
      "   6.11929849e-01  5.46299340e-01  3.90104604e-01  2.42202072e-01]\n",
      " [ 7.64428542e-01  8.57660372e-01  7.02411226e-01  1.16510824e+09\n",
      "   7.05340898e-01  5.30423046e+09  4.39794777e-01  5.01049261e+09\n",
      "   6.82710517e-01  6.33101952e-01  5.40987280e-01  3.76062532e-01]\n",
      " [ 7.97220611e-01  8.09629327e-01  8.42300039e-01  1.14452396e+09\n",
      "   7.29741125e-01  3.45244244e+09  6.56675583e-01  5.59657106e+09\n",
      "   6.97223604e-01  6.87188800e-01  6.40117125e-01  4.17587837e-01]]\n",
      "Policy matrix after 465001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 468001 iterations:\n",
      "[[ 8.06524254e-01  8.33086590e-01  9.14374990e-01  6.94396100e+09\n",
      "   7.96603402e-01  8.55541615e+09  6.96283371e-01  2.26406827e+09\n",
      "   7.38814505e-01  6.01561765e-01  6.24641461e-01 -7.11294132e-01]\n",
      " [ 8.46275280e-01  9.05123237e-01  9.56441640e-01  5.51195807e+09\n",
      "   7.47898380e-01  2.12100012e+09 -6.42559480e-01  2.14481050e+09\n",
      "   6.12062478e-01  5.46805138e-01  3.90270112e-01  2.42254087e-01]\n",
      " [ 7.64505859e-01  8.57924889e-01  7.03184295e-01  1.16510824e+09\n",
      "   7.05450176e-01  5.30423046e+09  4.39619725e-01  5.01049261e+09\n",
      "   6.82510296e-01  6.33047232e-01  5.41412943e-01  3.75860098e-01]\n",
      " [ 7.97282252e-01  8.09729896e-01  8.42484442e-01  1.14452396e+09\n",
      "   7.29978867e-01  3.45244244e+09  6.57817899e-01  5.59657106e+09\n",
      "   6.97127922e-01  6.87058672e-01  6.40068730e-01  4.17622398e-01]]\n",
      "Policy matrix after 468001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 471001 iterations:\n",
      "[[ 8.06495965e-01  8.33151285e-01  9.14611670e-01  6.94396100e+09\n",
      "   7.96625512e-01  8.55541615e+09  6.96334502e-01  2.26406827e+09\n",
      "   7.38852456e-01  6.01919554e-01  6.24671545e-01 -7.11266118e-01]\n",
      " [ 8.46307020e-01  9.05133018e-01  9.56475339e-01  5.51195807e+09\n",
      "   7.47838614e-01  2.12100012e+09 -6.43018709e-01  2.14481050e+09\n",
      "   6.12296445e-01  5.47276849e-01  3.90251653e-01  2.42745408e-01]\n",
      " [ 7.64610601e-01  8.57823167e-01  7.03249891e-01  1.16510824e+09\n",
      "   7.05467829e-01  5.30423046e+09  4.39848005e-01  5.01049261e+09\n",
      "   6.82178907e-01  6.33280606e-01  5.41594184e-01  3.76452666e-01]\n",
      " [ 7.97515030e-01  8.09810605e-01  8.42416233e-01  1.14452396e+09\n",
      "   7.30305754e-01  3.45244244e+09  6.58477159e-01  5.59657106e+09\n",
      "   6.96908564e-01  6.87168534e-01  6.40240852e-01  4.17634431e-01]]\n",
      "Policy matrix after 471001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 474001 iterations:\n",
      "[[ 8.06295710e-01  8.33322281e-01  9.14712089e-01  6.94396100e+09\n",
      "   7.96651316e-01  8.55541615e+09  6.96365492e-01  2.26406827e+09\n",
      "   7.38890558e-01  6.02123781e-01  6.24708919e-01 -7.11331615e-01]\n",
      " [ 8.46317232e-01  9.05116656e-01  9.56474959e-01  5.51195807e+09\n",
      "   7.47892394e-01  2.12100012e+09 -6.43037020e-01  2.14481050e+09\n",
      "   6.12921274e-01  5.47994030e-01  3.90715995e-01  2.42235310e-01]\n",
      " [ 7.64460178e-01  8.57974724e-01  7.03829711e-01  1.16510824e+09\n",
      "   7.05532938e-01  5.30423046e+09  4.39728135e-01  5.01049261e+09\n",
      "   6.82146495e-01  6.33656054e-01  5.42178290e-01  3.76615787e-01]\n",
      " [ 7.97669585e-01  8.09713256e-01  8.42091502e-01  1.14452396e+09\n",
      "   7.30497569e-01  3.45244244e+09  6.58445877e-01  5.59657106e+09\n",
      "   6.96824995e-01  6.87277325e-01  6.40483378e-01  4.17826831e-01]]\n",
      "Policy matrix after 474001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 477001 iterations:\n",
      "[[ 8.06459921e-01  8.33822480e-01  9.14561903e-01  6.94396100e+09\n",
      "   7.96692520e-01  8.55541615e+09  6.96633840e-01  2.26406827e+09\n",
      "   7.38965354e-01  6.02569928e-01  6.24685122e-01 -7.11486122e-01]\n",
      " [ 8.46379420e-01  9.05171409e-01  9.56511578e-01  5.51195807e+09\n",
      "   7.47921256e-01  2.12100012e+09 -6.43073558e-01  2.14481050e+09\n",
      "   6.13467665e-01  5.48318440e-01  3.91231673e-01  2.42677423e-01]\n",
      " [ 7.64175047e-01  8.57869457e-01  7.04918270e-01  1.16510824e+09\n",
      "   7.05660680e-01  5.30423046e+09  4.39873337e-01  5.01049261e+09\n",
      "   6.82314037e-01  6.33880632e-01  5.42568456e-01  3.77167445e-01]\n",
      " [ 7.97942703e-01  8.09770600e-01  8.42428083e-01  1.14452396e+09\n",
      "   7.30649494e-01  3.45244244e+09  6.59031269e-01  5.59657106e+09\n",
      "   6.96714983e-01  6.87358640e-01  6.40676504e-01  4.18049154e-01]]\n",
      "Policy matrix after 477001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 480001 iterations:\n",
      "[[ 8.06353146e-01  8.34138502e-01  9.14798663e-01  6.94396100e+09\n",
      "   7.96683705e-01  8.55541615e+09  6.96713038e-01  2.26406827e+09\n",
      "   7.38945251e-01  6.02845010e-01  6.24649163e-01 -7.11626172e-01]\n",
      " [ 8.46389666e-01  9.05185073e-01  9.56535742e-01  5.51195807e+09\n",
      "   7.47645847e-01  2.12100012e+09 -6.41874679e-01  2.14481050e+09\n",
      "   6.13618177e-01  5.48907253e-01  3.91756562e-01  2.43170157e-01]\n",
      " [ 7.64327496e-01  8.58130423e-01  7.05250865e-01  1.16510824e+09\n",
      "   7.05842149e-01  5.30423046e+09  4.39800452e-01  5.01049261e+09\n",
      "   6.82534165e-01  6.33976438e-01  5.42468896e-01  3.77035570e-01]\n",
      " [ 7.97847195e-01  8.09840142e-01  8.42116283e-01  1.14452396e+09\n",
      "   7.31003698e-01  3.45244244e+09  6.59346899e-01  5.59657106e+09\n",
      "   6.96412285e-01  6.87376003e-01  6.40785580e-01  4.18164044e-01]]\n",
      "Policy matrix after 480001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 483001 iterations:\n",
      "[[ 8.06510602e-01  8.34204865e-01  9.15000836e-01  6.94396100e+09\n",
      "   7.96721832e-01  8.55541615e+09  6.96720370e-01  2.26406827e+09\n",
      "   7.38970160e-01  6.02973875e-01  6.24701972e-01 -7.11419651e-01]\n",
      " [ 8.46428422e-01  9.05208624e-01  9.56520080e-01  5.51195807e+09\n",
      "   7.47603307e-01  2.12100012e+09 -6.41914619e-01  2.14481050e+09\n",
      "   6.13932701e-01  5.49046131e-01  3.91835351e-01  2.43135581e-01]\n",
      " [ 7.64254103e-01  8.58411032e-01  7.04983794e-01  1.16510824e+09\n",
      "   7.05980014e-01  5.30423046e+09  4.39549469e-01  5.01049261e+09\n",
      "   6.82806612e-01  6.34087660e-01  5.42668789e-01  3.77415564e-01]\n",
      " [ 7.97756297e-01  8.09915334e-01  8.41991252e-01  1.14452396e+09\n",
      "   7.31289729e-01  3.45244244e+09  6.59597842e-01  5.59657106e+09\n",
      "   6.96292204e-01  6.87427423e-01  6.40829285e-01  4.18163175e-01]]\n",
      "Policy matrix after 483001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 486001 iterations:\n",
      "[[ 8.06275975e-01  8.34485730e-01  9.14826114e-01  6.94396100e+09\n",
      "   7.96693685e-01  8.55541615e+09  6.96567515e-01  2.26406827e+09\n",
      "   7.38943221e-01  6.03296378e-01  6.24536477e-01 -7.11855124e-01]\n",
      " [ 8.46402781e-01  9.05190582e-01  9.56492119e-01  5.51195807e+09\n",
      "   7.47798942e-01  2.12100012e+09 -6.41806161e-01  2.14481050e+09\n",
      "   6.14114965e-01  5.49574388e-01  3.91912551e-01  2.43146894e-01]\n",
      " [ 7.63864081e-01  8.58464429e-01  7.04501051e-01  1.16510824e+09\n",
      "   7.06145788e-01  5.30423046e+09  4.39963715e-01  5.01049261e+09\n",
      "   6.83072815e-01  6.33995347e-01  5.42559730e-01  3.77194559e-01]\n",
      " [ 7.97833609e-01  8.10191067e-01  8.42277911e-01  1.14452396e+09\n",
      "   7.31575225e-01  3.45244244e+09  6.59654326e-01  5.59657106e+09\n",
      "   6.96383875e-01  6.87320840e-01  6.41039288e-01  4.18108740e-01]]\n",
      "Policy matrix after 486001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 489001 iterations:\n",
      "[[ 8.06036306e-01  8.34775461e-01  9.14979336e-01  6.94396100e+09\n",
      "   7.96783314e-01  8.55541615e+09  6.96621103e-01  2.26406827e+09\n",
      "   7.39021851e-01  6.03513075e-01  6.24462446e-01 -7.11881716e-01]\n",
      " [ 8.46462264e-01  9.05246216e-01  9.56538890e-01  5.51195807e+09\n",
      "   7.48030787e-01  2.12100012e+09 -6.41920972e-01  2.14481050e+09\n",
      "   6.14590352e-01  5.50160352e-01  3.92453149e-01  2.43007725e-01]\n",
      " [ 7.64050554e-01  8.58475559e-01  7.04256801e-01  1.16510824e+09\n",
      "   7.06217065e-01  5.30423046e+09  4.40157102e-01  5.01049261e+09\n",
      "   6.83186038e-01  6.34112239e-01  5.43014946e-01  3.77189646e-01]\n",
      " [ 7.98058453e-01  8.10120099e-01  8.42266852e-01  1.14452396e+09\n",
      "   7.31843602e-01  3.45244244e+09  6.59765159e-01  5.59657106e+09\n",
      "   6.96400267e-01  6.87402611e-01  6.41252853e-01  4.17952284e-01]]\n",
      "Policy matrix after 489001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 492001 iterations:\n",
      "[[ 8.06168445e-01  8.34956319e-01  9.15128987e-01  6.94396100e+09\n",
      "   7.96775351e-01  8.55541615e+09  6.96523684e-01  2.26406827e+09\n",
      "   7.39015044e-01  6.03893888e-01  6.24494977e-01 -7.12234962e-01]\n",
      " [ 8.46433056e-01  9.05224548e-01  9.56528803e-01  5.51195807e+09\n",
      "   7.47574643e-01  2.12100012e+09 -6.42312645e-01  2.14481050e+09\n",
      "   6.14841492e-01  5.50609246e-01  3.92124515e-01  2.43131240e-01]\n",
      " [ 7.64229774e-01  8.58209789e-01  7.04313566e-01  1.16510824e+09\n",
      "   7.06358789e-01  5.30423046e+09  4.39544994e-01  5.01049261e+09\n",
      "   6.83273454e-01  6.34388796e-01  5.43030597e-01  3.77143173e-01]\n",
      " [ 7.97929394e-01  8.10189978e-01  8.42359018e-01  1.14452396e+09\n",
      "   7.32057747e-01  3.45244244e+09  6.60254776e-01  5.59657106e+09\n",
      "   6.96600791e-01  6.87374453e-01  6.40916619e-01  4.17853558e-01]]\n",
      "Policy matrix after 492001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 495001 iterations:\n",
      "[[ 8.06210056e-01  8.35185216e-01  9.15228665e-01  6.94396100e+09\n",
      "   7.96744850e-01  8.55541615e+09  6.96332851e-01  2.26406827e+09\n",
      "   7.38988586e-01  6.04057179e-01  6.24537673e-01 -7.12648728e-01]\n",
      " [ 8.46418379e-01  9.05199274e-01  9.56496314e-01  5.51195807e+09\n",
      "   7.47724751e-01  2.12100012e+09 -6.42766067e-01  2.14481050e+09\n",
      "   6.14900044e-01  5.50907316e-01  3.92501719e-01  2.43073267e-01]\n",
      " [ 7.63834964e-01  8.58225529e-01  7.03558588e-01  1.16510824e+09\n",
      "   7.06212042e-01  5.30423046e+09  4.39803031e-01  5.01049261e+09\n",
      "   6.83489777e-01  6.34399087e-01  5.43025755e-01  3.77686771e-01]\n",
      " [ 7.97877835e-01  8.10026723e-01  8.42125445e-01  1.14452396e+09\n",
      "   7.32234501e-01  3.45244244e+09  6.60729257e-01  5.59657106e+09\n",
      "   6.96672388e-01  6.87333729e-01  6.41016001e-01  4.18004290e-01]]\n",
      "Policy matrix after 495001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "\n",
      "State-Action matrix after 498001 iterations:\n",
      "[[ 8.06293796e-01  8.35525363e-01  9.15424003e-01  6.94396100e+09\n",
      "   7.96769901e-01  8.55541615e+09  6.96383045e-01  2.26406827e+09\n",
      "   7.39003197e-01  6.04301658e-01  6.24510880e-01 -7.12445078e-01]\n",
      " [ 8.46474950e-01  9.05239912e-01  9.56536580e-01  5.51195807e+09\n",
      "   7.47869474e-01  2.12100012e+09 -6.42343798e-01  2.14481050e+09\n",
      "   6.15121963e-01  5.50905036e-01  3.93198535e-01  2.44104985e-01]\n",
      " [ 7.63887769e-01  8.58252589e-01  7.04106307e-01  1.16510824e+09\n",
      "   7.06207219e-01  5.30423046e+09  4.39710347e-01  5.01049261e+09\n",
      "   6.83509973e-01  6.34669808e-01  5.43420187e-01  3.77551601e-01]\n",
      " [ 7.98133397e-01  8.09739939e-01  8.42349186e-01  1.14452396e+09\n",
      "   7.32585191e-01  3.45244244e+09  6.60300150e-01  5.59657106e+09\n",
      "   6.96721397e-01  6.87341743e-01  6.41039437e-01  4.18265581e-01]]\n",
      "Policy matrix after 498001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "Utility matrix after 500000 iterations:\n",
      "[[ 8.06207155e-01  8.35731359e-01  9.15407006e-01  6.94396100e+09\n",
      "   7.96751479e-01  8.55541615e+09  6.96371150e-01  2.26406827e+09\n",
      "   7.38972441e-01  6.04398716e-01  6.24512904e-01 -7.12422290e-01]\n",
      " [ 8.46492210e-01  9.05249275e-01  9.56547028e-01  5.51195807e+09\n",
      "   7.47974359e-01  2.12100012e+09 -6.41817883e-01  2.14481050e+09\n",
      "   6.15234154e-01  5.51397316e-01  3.93587472e-01  2.44197060e-01]\n",
      " [ 7.63874999e-01  8.58366898e-01  7.04097659e-01  1.16510824e+09\n",
      "   7.06222456e-01  5.30423046e+09  4.39979622e-01  5.01049261e+09\n",
      "   6.83505938e-01  6.34663500e-01  5.43664379e-01  3.77796215e-01]\n",
      " [ 7.98330493e-01  8.09940957e-01  8.42498624e-01  1.14452396e+09\n",
      "   7.32607947e-01  3.45244244e+09  6.60411129e-01  5.59657106e+09\n",
      "   6.96690666e-01  6.87320570e-01  6.41125328e-01  4.18359932e-01]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    env = GridWorld(3, 4)\n",
    "\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.zeros((3,4))\n",
    "    state_matrix[0, 3] = 1\n",
    "    state_matrix[1, 3] = 1\n",
    "    state_matrix[1, 1] = -1\n",
    "    print(\"State Matrix:\")\n",
    "    print(state_matrix)\n",
    "\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.full((3,4), -0.04)\n",
    "    reward_matrix[0, 3] = 1\n",
    "    reward_matrix[1, 3] = -1\n",
    "    print(\"Reward Matrix:\")\n",
    "    print(reward_matrix)\n",
    "\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "    #Random policy\n",
    "    policy_matrix = np.random.randint(low=0, high=4, size=(3, 4)).astype(np.float32)\n",
    "    policy_matrix[1,1] = np.NaN #NaN for the obstacle at (1,1)\n",
    "    policy_matrix[0,3] = policy_matrix[1,3] = -1 #No action for the terminal states\n",
    "\n",
    "    #Set the matrices in the world\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "    state_action_matrix = np.random.random_sample((4,12)) # Q\n",
    "    #init with 1.0e-10 to avoid division by zero\n",
    "    running_mean_matrix = np.full((4,12), 1.0e-10) \n",
    "    gamma = 0.999\n",
    "    tot_epoch = 500000\n",
    "    print_epoch = 3000\n",
    "\n",
    "    for epoch in range(tot_epoch):\n",
    "        #Starting a new episode\n",
    "        episode_list = list()\n",
    "        #Reset and return the first observation and reward\n",
    "        observation = env.reset(exploring_starts=True)\n",
    "        #action = np.random.choice(4, 1)\n",
    "        #action = policy_matrix[observation[0], observation[1]]\n",
    "        #episode_list.append((observation, action, reward))\n",
    "        is_starting = True\n",
    "        for _ in range(1000):\n",
    "            #Take the action from the action matrix\n",
    "            action = policy_matrix[observation[0], observation[1]]\n",
    "            #If the episode just started then it is\n",
    "                #necessary to choose a random action (exploring starts)\n",
    "            if(is_starting): \n",
    "                action = np.random.randint(0, 4)\n",
    "                is_starting = False      \n",
    "            #Move one step in the environment and get obs and reward\n",
    "            new_observation, reward, done = env.step(action)\n",
    "            #Append the visit in the episode list\n",
    "            episode_list.append((observation, action, reward))\n",
    "            observation = new_observation\n",
    "            if done: break\n",
    "        #The episode is finished, now estimating the utilities\n",
    "        counter = 0\n",
    "        #Checkup to identify if it is the first visit to a state\n",
    "        checkup_matrix = np.zeros((4,12))\n",
    "        #This cycle is the implementation of First-Visit MC.\n",
    "        #For each state stored in the episode list check if it\n",
    "        #is the rist visit and then estimate the return.\n",
    "        for visit in episode_list:\n",
    "            observation = visit[0]\n",
    "            action = visit[1]\n",
    "            col = int(observation[1] + (observation[0]*4))\n",
    "            row = int(action)\n",
    "            if(checkup_matrix[row, col] == 0):\n",
    "                return_value = get_return(episode_list[counter:], gamma)\n",
    "                running_mean_matrix[row, col] += 1\n",
    "                state_action_matrix[row, col] += return_value\n",
    "                checkup_matrix[row, col] = 1\n",
    "            counter += 1\n",
    "        #Policy Update\n",
    "        policy_matrix = update_policy(episode_list, \n",
    "                                      policy_matrix, \n",
    "                                      state_action_matrix/running_mean_matrix)\n",
    "        #Printing\n",
    "        if(epoch % print_epoch == 0):\n",
    "            print(\"\")\n",
    "            print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "            print(state_action_matrix / running_mean_matrix)\n",
    "            print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "            print(policy_matrix)\n",
    "            print_policy(policy_matrix)\n",
    "    #Time to check the utility matrix obtained\n",
    "    print(\"Utility matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "    print(state_action_matrix / running_mean_matrix)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the code below with the one used in MC for prediction we will notice some important differences, for example the following condition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if(is_starting): \n",
    "    action = np.random.randint(0, 4)\n",
    "    is_starting = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This condition assures to satisfy the exploring starts. The MC algorithm will converge to the optimal solution only if we assure the exploring starts. In MC for control it is not sufficient to select random starting states. During the iterations the algorithm will improve the policy only if all the actions have a non-zero probability to be chosen. In this sense when the episode start we have to select a random action, this must be done only for the starting state.\n",
    "\n",
    "There is another subtle difference that we must analyse. In the code I differentiate between observation and new_observation meaning the observation at time t and observation at time t+1. What we have to store in our episode list is the observation at t, the action taken at t and the reward obtained at t+1. Remember that we are interested in the utility of taking a certain action in a certain state.\n",
    "\n",
    "It is time to run the script and see what we obtain. Before remember that for the special 4x3 world we already know the optimal policy. If you go back to the first post you will see that we found the optimal policy in case of reward equal to -0.04 (for non terminal states), and in case of transition model with 80-10-10 percent probabilities. This optimal policy is the following:\n",
    "\n",
    "<img src=\"../images/RL-lecture2-9.png\" alt=\"Drawing\" style=\"width: 150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the optimal policy the robot will move far away from the stairs at state (4, 2) and will reach the charging station through the longest path. Now I will show you the evolution of the policy once we run the script for MC control estimation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "At the beginning the MC method is initialized with a random policy, it is not a surprise that the first policy is a complete non-sense. \n",
    "After 3000 iteration the algorithm find aÂ sub-optimal policy. In this policy the robot moves close to the stairs in order to reach the charging station. As we said in the previous post this is risky because the robot can fall down. At iteration 78000 the algorithm finds another policy, which is always sub-optimal, but it is slightly better than the previous one. Finally atÂ iteration 405000 the algorithm finds the optimal policyÂ and stick to it until the end.\n",
    "\n",
    "TheÂ MC method cannot converge to any sub-optimal policy. Looking to the GPI scheme this is obvious. If the algorithm converges to a sub-optimal policy then the utility function would eventually converge to the utility function for that policy and that in turn would cause the policy to change.Â StabilityÂ is reached only when both theÂ policy and the utility function are optimal. Convergence to this optimal fixed point seems inevitable but has not yet been formally proved.\n",
    "\n",
    "I would like toÂ reflect for a moment on the beauty of the MC algorithm. In MC for control the method can estimate the best policy having nothing. The robot is moving in the environment trying different actions and following the consequences of those actions until the end. Thatâ€™s all. The robot does not know the reward function, it does not know the transition model and it does not have any policy to follow. Nevertheless the algorithm improves until reaching the optimal strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOP\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        TOP\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(0)\n",
    "Using our cleaning robot we can easily see what is the difference between TD and MC learning and what each one does at each step,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The update rule found in the previous part is the simplest form of TD learning, the TD(0) algorithm. \n",
    "+ TD(0) allows estimating the utility values following a specific policy. \n",
    "+ We are in the **passive learning** case for prediction, and we are in **model-free reinforcement learning**, meaning that we do not have the **transition model**. \n",
    "+ To estimate the state-value function, we can only move in the world. \n",
    "+ Let's see what does it mean to apply the TD algorithm to a single episode. \n",
    "+ I am going to use the same episode where the robot starts at (1,1) and reaches the terminal state at (4,3) after seven steps.\n",
    "<img src=\"../images/RL-lecture2-1.png\" alt=\"Drawing\" style=\"width: 500px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the TD algorithm means to move step by step considering only the state at t and the state at t+1. Thatâ€™s it, after each step we get the utility value and the reward at t+1 and we update the value at t. The TD(0) algorithm ignores the past states and this is shown by the shadow I added above those states. Applying the algorithm to the episode ($\\gamma=0.9, \\alpha=0.1$) leads to the following changes in the utility matrix:\n",
    "<img src=\"../images/RL-lecture2-2.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The red frame highlights the utility value that has been updated at each visit. The matrix is initialised with zeros. \n",
    "+ At k=1 the state (1,1) is updated since the robot is in the state (1,2) and the first reward (-0.04) is available. \n",
    "+ The calculation for updating the utility at (1,1) is: 0.0 + 0.1 (-0.04 + 0.9 (0.0) - 0.0) = -0.004. Similarly to (1,1) the algorithm updates the state at (1,2). \n",
    "+ At k=3 the robot goes back and the calculation take the form: 0.0 + 0.1 (-0.04 + 0.9 (-0.004) - 0.0) = -0.00436. \n",
    "+ At k=4 the robot changes again its direction. In this case the algorithm update for the second time the state (1,2) as follow: -0.004 + 0.1 (-0.04 + 0.9 (-0.00436) + 0.004) = -0.0079924. \n",
    "+ The same process is applied until the end of the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the class GridWorld contained in the module gridworld.py again. <br/>\n",
    "I will use again the 4x3 world with a charging station at (4,3) and the stairs at (4,2). <br/>\n",
    "The optimal policy and the state-values of this world are the same we obtained in the previous example:\n",
    "<img src=\"../images/RL-lecture2-3.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule of TD(0) can be implemented in a few lines:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_utility(utility_matrix, observation, new_observation, \n",
    "                   reward, alpha, gamma):\n",
    "    '''Return the updated utility matrix\n",
    "    @param utility_matrix the matrix before the update\n",
    "    @param observation the state obsrved at t\n",
    "    @param new_observation the state observed at t+1\n",
    "    @param reward the reward observed after the action\n",
    "    @param alpha the ste size (learning rate)\n",
    "    @param gamma the discount factor\n",
    "    @return the updated utility matrix\n",
    "    '''\n",
    "    u = utility_matrix[observation[0], observation[1]]\n",
    "    u_t1 = utility_matrix[new_observation[0], new_observation[1]]\n",
    "    utility_matrix[observation[0], observation[1]] += \\\n",
    "        alpha * (reward + gamma * u_t1 - u)\n",
    "    return utility_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop is much simpler than the one of MC methods. In this case we do not have any first-visit constraint and the only thing to do is to apply the update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n",
      "Policy Matrix:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      "\n",
      "Utility matrix after 1 iterations:\n",
      "[[-0.004 -0.004  0.1    0.   ]\n",
      " [-0.004  0.     0.     0.   ]\n",
      " [-0.004  0.     0.     0.   ]]\n",
      "\n",
      "Utility matrix after 50001 iterations:\n",
      "[[0.86135319 0.92089811 0.95584791 0.        ]\n",
      " [0.81809754 0.         0.58523962 0.        ]\n",
      " [0.74892387 0.70315465 0.         0.        ]]\n",
      "\n",
      "Utility matrix after 100001 iterations:\n",
      "[[0.85271854 0.91885718 0.96663391 0.        ]\n",
      " [0.80126948 0.         0.89066707 0.        ]\n",
      " [0.74468331 0.69195097 0.         0.        ]]\n",
      "\n",
      "Utility matrix after 150001 iterations:\n",
      "[[0.79229969 0.83139772 0.90190888 0.        ]\n",
      " [0.75893635 0.         0.37511175 0.        ]\n",
      " [0.71456017 0.68488362 0.         0.        ]]\n",
      "\n",
      "Utility matrix after 200001 iterations:\n",
      "[[0.83677492 0.89617591 0.93833395 0.        ]\n",
      " [0.78807944 0.         0.73801214 0.        ]\n",
      " [0.72421951 0.66136388 0.         0.        ]]\n",
      "\n",
      "Utility matrix after 250001 iterations:\n",
      "[[0.842108   0.89768059 0.96722275 0.        ]\n",
      " [0.79826527 0.         0.76655475 0.        ]\n",
      " [0.74449719 0.69283668 0.         0.        ]]\n",
      "Utility matrix after 300000 iterations:\n",
      "[[0.86051493 0.91363598 0.97268959 0.        ]\n",
      " [0.80173212 0.         0.86548473 0.        ]\n",
      " [0.76113172 0.68953337 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    env = GridWorld(3, 4)\n",
    "\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.zeros((3,4))\n",
    "    state_matrix[0, 3] = 1\n",
    "    state_matrix[1, 3] = 1\n",
    "    state_matrix[1, 1] = -1\n",
    "    print(\"State Matrix:\")\n",
    "    print(state_matrix)\n",
    "\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.full((3,4), -0.04)\n",
    "    reward_matrix[0, 3] = 1\n",
    "    reward_matrix[1, 3] = -1\n",
    "    print(\"Reward Matrix:\")\n",
    "    print(reward_matrix)\n",
    "\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "    #Define the policy matrix\n",
    "    #This is the optimal policy for world with reward=-0.04\n",
    "    policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                              [0, np.NaN,  0,  -1],\n",
    "                              [0,      3,  3,   3]])\n",
    "    print(\"Policy Matrix:\")\n",
    "    print(policy_matrix)\n",
    "\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "    utility_matrix = np.zeros((3,4))\n",
    "    gamma = 0.999\n",
    "    alpha = 0.1 #constant step size\n",
    "    tot_epoch = 300000\n",
    "    print_epoch = 50000\n",
    "\n",
    "    for epoch in range(tot_epoch):\n",
    "        #Reset and return the first observation\n",
    "        observation = env.reset(exploring_starts=False)\n",
    "        for step in range(1000):\n",
    "            #Take the action from the action matrix\n",
    "            action = policy_matrix[observation[0], observation[1]]\n",
    "            #Move one step in the environment and get obs and reward\n",
    "            new_observation, reward, done = env.step(action)\n",
    "            utility_matrix = update_utility(utility_matrix, observation, \n",
    "                                            new_observation, reward, alpha, gamma)\n",
    "            observation = new_observation\n",
    "            #print(utility_matrix)\n",
    "            if done: break\n",
    "\n",
    "        if(epoch % print_epoch == 0):\n",
    "            print(\"\")\n",
    "            print(\"Utility matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "            print(utility_matrix)\n",
    "    #Time to check the utility matrix obtained\n",
    "    print(\"Utility matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "    print(utility_matrix)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOP\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        TOP\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(lambda)\n",
    "The Python implementation of TD(Î») is straightforward. We only need to add an eligibility matrix and its update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_utility(utility_matrix, trace_matrix, alpha, delta):\n",
    "    '''Return the updated utility matrix\n",
    "    @param utility_matrix the matrix before the update\n",
    "    @param alpha the step size (learning rate)\n",
    "    @param delta the error (Taget-OldEstimte) \n",
    "    @return the updated utility matrix\n",
    "    '''\n",
    "    utility_matrix += alpha * delta * trace_matrix\n",
    "    return utility_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_eligibility(trace_matrix, gamma, lambda_):\n",
    "    '''Return the updated trace_matrix\n",
    "    @param trace_matrix the eligibility traces matrix\n",
    "    @param gamma discount factor\n",
    "    @param lambda_ the decaying value\n",
    "    @return the updated trace_matrix\n",
    "    '''\n",
    "    trace_matrix = trace_matrix * gamma * lambda_\n",
    "    return trace_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The main loop introduces some new components compared to the TD(0) case. \n",
    "+ We have the estimation of delta in a separate line and the management of the trace_matrix in two lines. \n",
    "+ First of all the states are increased (+1) and then they are decayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n",
      "Policy Matrix:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      "Trace Matrix:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Utility matrix after 1 iterations:\n",
      "[[0.01895203 0.04595    0.1        0.        ]\n",
      " [0.00479687 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "[[0.12462537 0.24950025 0.4995     0.        ]\n",
      " [0.09334444 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Utility matrix after 50001 iterations:\n",
      "[[0.8834698  0.94154579 1.00014216 0.        ]\n",
      " [0.83124815 0.         0.75262683 0.        ]\n",
      " [0.77955405 0.73278148 0.69709184 0.59609685]]\n",
      "[[1.24626311e-01 2.49505888e-01 4.99809008e-01 0.00000000e+00]\n",
      " [6.22508424e-02 0.00000000e+00 1.65572368e-04 0.00000000e+00]\n",
      " [3.88522718e-02 2.13423268e-02 9.66840731e-04 4.82936931e-04]]\n",
      "\n",
      "Utility matrix after 100001 iterations:\n",
      "[[0.8088043  0.83881903 0.81287252 0.        ]\n",
      " [0.75996436 0.         0.26094672 0.        ]\n",
      " [0.6970898  0.65683999 0.65424465 0.40596205]]\n",
      "[[6.27634047e-02 1.25652462e-01 2.55434629e-01 0.00000000e+00]\n",
      " [3.13578286e-02 0.00000000e+00 4.66933596e-07 0.00000000e+00]\n",
      " [1.55352307e-02 7.75797629e-03 5.92649254e-27 4.99500000e-01]]\n",
      "\n",
      "Utility matrix after 150001 iterations:\n",
      "[[0.82383712 0.87385149 0.8735216  0.        ]\n",
      " [0.80476861 0.         0.62872915 0.        ]\n",
      " [0.76910532 0.71530171 0.6706363  0.44241926]]\n",
      "[[1.58198237e-01 2.53495973e-01 5.07504126e-01 0.00000000e+00]\n",
      " [6.32473076e-02 0.00000000e+00 1.55338224e-02 0.00000000e+00]\n",
      " [1.50310475e-05 7.50072813e-06 4.36470801e-13 2.18017165e-13]]\n",
      "\n",
      "Utility matrix after 200001 iterations:\n",
      "[[0.87657585 0.9259513  0.98284345 0.        ]\n",
      " [0.83295853 0.         0.94512919 0.        ]\n",
      " [0.78414802 0.72847695 0.68702032 0.20409164]]\n",
      "[[1.24625871e-01 2.49621737e-01 5.04671616e-01 0.00000000e+00]\n",
      " [6.22506225e-02 0.00000000e+00 2.44861696e-03 0.00000000e+00]\n",
      " [3.10940694e-02 1.55314877e-02 7.75797810e-03 1.05288541e-16]]\n",
      "\n",
      "Utility matrix after 250001 iterations:\n",
      "[[0.79580886 0.79984202 0.86494014 0.        ]\n",
      " [0.75531532 0.         0.26814538 0.        ]\n",
      " [0.72561576 0.69318518 0.6612604  0.52399382]]\n",
      "[[4.11657482e-03 2.26211199e-01 7.65544604e-01 0.00000000e+00]\n",
      " [2.05628595e-03 0.00000000e+00 1.31187738e-05 0.00000000e+00]\n",
      " [6.02154888e-05 1.13179850e-10 5.63424751e-11 1.35716502e-14]]\n",
      "Utility matrix after 300000 iterations:\n",
      "[[0.88875599 0.934602   1.00936962 0.        ]\n",
      " [0.83659343 0.         0.89039963 0.        ]\n",
      " [0.77849216 0.7252412  0.64365343 0.36919417]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    env = GridWorld(3, 4)\n",
    "\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.zeros((3,4))\n",
    "    state_matrix[0, 3] = 1\n",
    "    state_matrix[1, 3] = 1\n",
    "    state_matrix[1, 1] = -1\n",
    "    print(\"State Matrix:\")\n",
    "    print(state_matrix)\n",
    "\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.full((3,4), -0.04)\n",
    "    reward_matrix[0, 3] = 1\n",
    "    reward_matrix[1, 3] = -1\n",
    "    print(\"Reward Matrix:\")\n",
    "    print(reward_matrix)\n",
    "\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "    #Define the policy matrix\n",
    "    #This is the optimal policy for world with reward=-0.04\n",
    "    policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                              [0, np.NaN,  0,  -1],\n",
    "                              [0,      3,  3,   3]])\n",
    "    print(\"Policy Matrix:\")\n",
    "    print(policy_matrix)\n",
    "\n",
    "    #Define and print the eligibility trace matrix\n",
    "    trace_matrix = np.zeros((3,4))\n",
    "    print(\"Trace Matrix:\")\n",
    "    print(trace_matrix)\n",
    "\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "    utility_matrix = np.zeros((3,4))\n",
    "    gamma = 0.999 #discount rate\n",
    "    alpha = 0.1 #constant step size\n",
    "    lambda_ = 0.5 #decaying factor\n",
    "    tot_epoch = 300000\n",
    "    print_epoch = 50000\n",
    "\n",
    "    for epoch in range(tot_epoch):\n",
    "        #Reset and return the first observation\n",
    "        observation = env.reset(exploring_starts=True)        \n",
    "        for step in range(1000):\n",
    "            #Take the action from the action matrix\n",
    "            action = policy_matrix[observation[0], observation[1]]\n",
    "            #Move one step in the environment and get obs and reward\n",
    "            new_observation, reward, done = env.step(action)\n",
    "            #Estimate the error delta (Target - OldEstimate)\n",
    "            delta = reward + gamma * utility_matrix[new_observation[0], new_observation[1]] - \\\n",
    "                                     utility_matrix[observation[0], observation[1]]\n",
    "            #Adding +1 in the trace matrix for the state visited\n",
    "            trace_matrix[observation[0], observation[1]] += 1\n",
    "            #Update the utility matrix\n",
    "            utility_matrix = update_utility(utility_matrix, trace_matrix, alpha, delta)\n",
    "            #Update the trace matrix (decaying)\n",
    "            trace_matrix = update_eligibility(trace_matrix, gamma, lambda_)\n",
    "            observation = new_observation\n",
    "            if done: break #return\n",
    "\n",
    "        if(epoch % print_epoch == 0):\n",
    "            print(\"\")\n",
    "            print(\"Utility matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "            print(utility_matrix)\n",
    "            print(trace_matrix)\n",
    "    #Time to check the utility matrix obtained\n",
    "    print(\"Utility matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "    print(utility_matrix)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Comparing the final utility matrix with the one obtained without the use of eligibility traces in TD(0) you will notice similar values. \n",
    "+ Whatâ€™s the advantage eligibility traces? \n",
    "    + The eligibility traces version converges faster. \n",
    "\n",
    "This advantage become clear when dealing with sparse reward in a large state space. In this case the eligibility trace mechanism can considerably speeds up the convergence propagating what learnt at t+1 back to the last states visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOP\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        TOP\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
