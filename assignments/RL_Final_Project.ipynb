{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL1 Final Project\n",
    "## Introduction\n",
    "Welcome to the RL final project! We'll explore two different RL methods worked during our course:\n",
    "+ Dynamic Programming (DP)\n",
    "+ Monte Carlo (MC)\n",
    "A simulated real problem related to the placement of Ads on a website will be used to implement these solutions. \n",
    "\n",
    "## The Context\n",
    "### Predicting Ads Positioning \n",
    "Placement of ads on website is the primary problem for companies that operate on ad revenue. The position where the ad is placed plays pivotal role on whether or not the ad will be clicked. Here we have the following choices:\n",
    "+ Place them randomly, or Place the ad on the same position\n",
    "\n",
    "The problem with placing the ad on the same position is the user, after a certain time, will start ignoring the space since he's used to seeing ad at the place, he will end up ignoring that particular position hereafter. Hence, this will reduce the number of clicks on ads. <br/>\n",
    "\n",
    "The problem with placing them randomly is that it wouldn't take optimal positions into consideration. <br/>\n",
    "For instance, text beside images are viewed higher number of times than those text which are placed at a distance. It is infeasible to go through every website and repeat the procedure.\n",
    "\n",
    "## Approach\n",
    "### Reinforcement Learning\n",
    "\n",
    "Using Reinforcement Learning we can approximate the human behavior.\n",
    "\n",
    "## Data\n",
    "We will use a dataset from Kaggle: ads-ctr-optimisation. <br/>\n",
    "It is already in our S3 datalake and the code to access we give you below.\n",
    "## The Project\n",
    "Your team will solve this problem using two algorithms you have learned during this course:\n",
    "+ __Dynamic Programming (DP)__\n",
    "+ __Monte Carlo with $\\epsilon-greedy$(MC)__\n",
    "\n",
    "At the end you can compare the results.\n",
    "\n",
    "## The Business Problem\n",
    "> We can present a particular Ads in different positions related to 10 different advertisiments on our webpage.\n",
    "\n",
    "> People visit the webpage and can select on one or more advertisiments, or not select anything at all.\n",
    "\n",
    "We need to find the position where the Ads is more likely to be selected, so we can present to our customers that advertise in our webpages different proces. \n",
    "\n",
    "__NOTE:__ although more than one position can be selected in one visit, we are considering them unrelated events for this problem. In other words, each selection is independent from other possible selections on the webpage in this visit.\n",
    "\n",
    "We don't have any information about the particular visitors and we are assuming that the visitors are representative of the regular community that visits regularly our website. <br/>\n",
    "\n",
    "Our reinforced learning agent will learn, from the selection behaviour of people visiting our website, the best positions (more selected ones) in our Ads.\n",
    "\n",
    "## Resources\n",
    "I've always found experience the best teacher, so I advocate just diving in and trying to implement things.  However, it is always good to have other sources to reference, so every week I'll place links to things that I think are helpful to learn the material.\n",
    "* [this book](http://incompleteideas.net/book/bookdraft2018jan1.pdf) about the concepts worked here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import routines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import boto3\n",
    "from os import path\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Environment\n",
    "\n",
    "> __Environment:__ a matrix (or dataframe) containing 10 Ads position per row having values either 1, when the ad is clicked, or 0 when it is not, taken from 10,000 visits to our website. \n",
    "\n",
    "> __State:__ each row showing the positions for Ads that have been selected in a particular visit to our webpage.\n",
    "\n",
    "> __Rewards:__ 1, when Ads is clicked, 0 when it is not. Each cell of our environment is a reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our dataset (environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "bucketname = 'mlu-courses-datalake' # replace with your bucket name\n",
    "filename2 = 'RL1/Ads_CTR_Optimisation.csv' # replace with your object key\n",
    "s3 = boto3.resource('s3')\n",
    "if not path.exists(\"Ads_CTR_Optimisation.csv\"):\n",
    "    s3.Bucket(bucketname).download_file(filename2, 'Ads_CTR_Optimisation.csv')\n",
    "    \n",
    "env = pd.read_csv('Ads_CTR_Optimisation.csv')\n",
    "env.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positions with the Best Returns\n",
    "Another question we might ask, is to display the ad where it is clicked the most number of times. <br/>\n",
    "\n",
    "For instance, there might be a certain position where the ad is selected with a higher probability. <br/>\n",
    "Since the values of the rows is either 1 or 0, we can sum across the columns and count the number of times ad in the position was clicked.\n",
    "\n",
    "**This will be useful to compare the two approaches: MC and DP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_counts = env.values.sum(axis=0)\n",
    "best_counts = pd.DataFrame({\"ad\": np.arange(1, 11), \"counts\": clicked_counts})\n",
    "best_counts.set_index(\"ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates ad 5 was seelcted 2695 times. So if we were to always place an ad on position 5, it would be click around 2695 times. \n",
    "\n",
    "### But you can we do better then that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I\n",
    "## Dynamic Programming (Policy Iteration) \n",
    "Implement here a Dynamic Programming solution to learn from our environment.\n",
    "\n",
    "For your algorithm, consider:\n",
    "+ First, create a random policy reflecting a random choice between one, and only one, Ad for each state, for all 10,000 states.\n",
    "\n",
    "+ run a maximum of 1000 episodes, using this stop criterion of convergence: 1e-20 \n",
    "+ consider a $\\gamma=0.9$\n",
    "+ initialize the state-value function for all states with random values close to zero\n",
    "+ When running your episodes:\n",
    "    + For policy evaluation part:\n",
    "        + run until convergence or max episodes defined\n",
    "        + get the actions from your policy  \n",
    "    + For policy improvement part:\n",
    "        + Update your policy based on the state-value function\n",
    "        + Save the state-value changes, delta = $||V_{k-1} - V_k||$ for each step in a list\n",
    "\n",
    "> __1. Plot the delta against the episodes to show the convergence__\n",
    "\n",
    "> __2. Create a bar plot comparing the most chosen positions between the original Ads environment and the optimal policy learned.__\n",
    "\n",
    "> __3. Can you see if the learned policy follows the same pattern of selections for each Ads?__\n",
    "\n",
    "> __4. Compare the five top Ads selected from the original environment with the ones from the learned policy to see how many matches do you have.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II\n",
    "## Monte Carlo using  $\\epsilon-greedy$\n",
    "+ Use $\\epsilon-greedy$ policy for this case with $\\epsilon=0.1$\n",
    "\n",
    "+ Save the state-value changes, delta = $||Q_{k-1} - Q_k||$ for each step in a list\n",
    "\n",
    "> __1. Create a bar plot comparing the most chosen positions between the original Ads environment and the optimal policy learned.__\n",
    "\n",
    "> __2. Can you see if the learned policy follows the same pattern of selections for each Ads?__\n",
    "\n",
    "> __3. Compare the five top Ads selected from the original environment with the ones from the learned policy to see how many matches do you have.__\n",
    "#### Optional\n",
    "> __4. Apply a decay factor to the $\\epsilon-greedy$ and compare the convergence plots.__<br/>\n",
    ">    Start with $\\epsilon=1.0$ and let it decay exponentially to $\\epsilon=0.01$ through the episodes, with a decay rate of 0.01.\n",
    "\n",
    "> Has the decaying factor a faster convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
