{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 Support Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "<p>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Short-Corredor-Problem\">\n",
    "        <span class=\"toc-item-num\">1&nbsp;&nbsp;</span>\n",
    "        Short Corredor Problem\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Clifwalk-REINFORCE-with-Baseline\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>\n",
    "        Clifwalk REINFORCE with Baseline\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Clifwalk-Actor-Critic-for-Discrete-Action-Spaces\"><span class=\"toc-item-num\">3.1.&nbsp;&nbsp;</span>\n",
    "        Clifwalk Actor Critic for Discrete Action Spaces\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Mountain-Car-Actor-Critic-for-Continuous-Action-Spaces\"><span class=\"toc-item-num\">3.2.&nbsp;&nbsp;</span>\n",
    "        Mountain Car Actor Critic for Continuous Action Spaces\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Corredor Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref. https://github.com/jcassiojr/Reinforcement-Learning-In-Motion/tree/master/Unit-8-The-Mountaincar\n",
    "others: reinforce, discrete and continuous mountain from sutton exercices (Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "matplotlib.style.use('seaborn')\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 5)\n",
    "\n",
    "from IPython.display import display, Image\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib_rl.envs.cliff_walking import CliffWalkingEnv\n",
    "from lib_rl import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Continuous case libraries\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE BELOW!!!\n",
    "https://github.com/zyxue/sutton-barto-rl-exercises/blob/master/reinforcement/policy-gradient-methods/Example-13.1-short-corridor-with-switched-actions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the small corridor gridworld shown inset in the graph below. The reward\n",
    "is 1 per step, as usual. In each of the three nonterminal states there are only\n",
    "two actions, right and left. These actions have their usual consequences in the first\n",
    "and third states (left causes no movement in the first state), but in the second\n",
    "state they are reversed, so that right moves to the left and left moves to the right.\n",
    "The problem is dicult because all the states appear identical under the function\n",
    "approximation. In particular, we define x(s,right) = [1, 0]> and x(s, left) = [0, 1]>,\n",
    "for all s. An action-value method with \"-greedy action selection is forced to choose\n",
    "between just two policies: choosing right with high probability 1  \"/2 on all steps\n",
    "or choosing left with the same high probability on all time steps. If \" = 0.1, then\n",
    "these two policies achieve a value (at the start state) of less than 44 and 82,\n",
    "respectively, as shown in the graph. A method can do significantly better if it can\n",
    "learn a specific probability with which to select right. The best probability is about\n",
    "0.59, which achieves a value of about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/short-corridor-with-switched-actions.png\" alt=\"Drawing\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_STATE = 0\n",
    "TERMINAL_STATE = 3\n",
    "# state that reverse the result of the action\n",
    "REVERSE_STATE = 1\n",
    "LEFT = -1\n",
    "RIGHT = 1\n",
    "ACTIONS = [LEFT, RIGHT]\n",
    "\n",
    "def generate_an_episode(starting_state, greedy_action, prob_right):\n",
    "    state = starting_state\n",
    "    reward = 0\n",
    "    while state != TERMINAL_STATE:\n",
    "        if np.random.random() <= prob_right:\n",
    "            act = RIGHT\n",
    "        else:\n",
    "            act = LEFT\n",
    "\n",
    "        if state == REVERSE_STATE:\n",
    "            state -= act\n",
    "        else:\n",
    "            state += act\n",
    "        reward -= 1\n",
    "        state = max(state, 0)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "# epsilon-greedy left\n",
    "prob_right = epsilon / 2\n",
    "\n",
    "rewards = []\n",
    "for i in range(2000):\n",
    "    rewards.append(generate_an_episode(INIT_STATE, RIGHT, prob_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-83.8885"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "# epsilon-greedy right\n",
    "prob_right = (1 - epsilon) + epsilon / 2\n",
    "\n",
    "rewards = []\n",
    "for i in range(2000):\n",
    "    rewards.append(generate_an_episode(INIT_STATE, RIGHT, prob_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-45.8545"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximize\n",
    "$$f(p) = p(1-p)p = p^2 - p^3$$\n",
    "to obtain the best policy. Then $p = \\frac{2}{3}$ (differentiate $f$ and set derivative to zero), to calculate the corresponding $\\epsilon$, set\n",
    "$$\\frac{\\epsilon}{2} + (1 - \\epsilon) = \\frac{2}{3}$$\n",
    "It turns out $\\epsilon = \\frac{2}{3}$, too.\n",
    "Calculate the opitmal value of the initial state may be more involved, so just simulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,"
     ]
    }
   ],
   "source": [
    "# probabilities of going right\n",
    "probs = np.sort(np.arange(0.02, 1, 0.02).tolist() + [2/3])\n",
    "\n",
    "avg_rewards = []\n",
    "std_rewards = []\n",
    "for k, p in enumerate(probs):\n",
    "    print(k, end=',')\n",
    "    rewards = []\n",
    "    for i in range(2000):\n",
    "        rewards.append(generate_an_episode(INIT_STATE, RIGHT, p))\n",
    "    avg_rewards.append(np.mean(rewards))\n",
    "    std_rewards.append(np.std(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Value of state S')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(probs, avg_rewards, yerr=std_rewards)\n",
    "idx = np.where(probs == 2/3)[0][0]\n",
    "plt.scatter([probs[idx]], avg_rewards[idx], label='optimal probability')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Probability of moving right')\n",
    "plt.ylabel('Value of state S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best reward is around 2/3 because the region around it is very flat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"lev1\">\n",
    "    <a href=\"#Lecture-4-Support-Notebook\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        Go to TOP\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clifwalk REINFORCE with Baseline\n",
    "## <TODO explanation of clifwalk environment (from assignment N)\n",
    "### The Clifwalk Problem\n",
    "<img src=\"../images/RL-lecture3-1.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "The Mountain Car is a classic reinforcement learning problem where the objective is to create an algorithm which learns to climb a steep hill to reach the goal marked by a flag. The car’s engine is not powerful enough to drive up the hill without a head start so the car must drive up the left hill to obtain enough momentum to scale the steeper hill to the right and reach the goal.<br/>\n",
    "\n",
    "+ When the problem begins the car is dropped into the valley and given an initial position and velocity as a vector. **This is the car’s state**.\n",
    "<img src=\"../images/RL-lecture3-2.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "+ The agent must tell the car to take one of three actions: \n",
    "<img src=\"../images/RL-lecture3-3.png\" alt=\"Drawing\" style=\"width: 200px;\">\n",
    "+ This action is sent to the Mountain Car environment algorithm which returns a new state (position and velocity) as well as a reward. \n",
    "+ For each step that the car does not reach the goal, located at position 0.5, the environment returns a reward of -1. \n",
    "+ We will use these rewards in our Q-Learning algorithm to solve the Mountain Car problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Discrete(48)\n",
      "Action space:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the state space represents a 2-dimensional box, so each state observation is a vector of 2 (float) values, and that the action space comprises three discrete actions.\n",
    "\n",
    "By default, the three actions are represented by the integers 0, 1 and 2. However, we don’t know what values the elements of the state vector can take. This can be found using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=env.action_space.n,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.action_probs = tf.squeeze(tf.nn.softmax(self.output_layer))\n",
    "            self.picked_action_prob = tf.gather(self.action_probs, self.action)\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -tf.log(self.picked_action_prob) * self.target\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.action_probs, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, action, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target, self.action: action  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient) Algorithm. Optimizes the policy\n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized \n",
    "        estimator_value: Value function approximator, used as a baseline\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = estimator_policy.predict(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "            # sys.stdout.flush()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "        # Go through the episode and make policy updates\n",
    "        for t, transition in enumerate(episode):\n",
    "            # The return after this timestep\n",
    "            total_return = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n",
    "            # Calculate baseline/advantage\n",
    "            baseline_value = estimator_value.predict(transition.state)            \n",
    "            advantage = total_return - baseline_value\n",
    "            # Update our value estimator\n",
    "            estimator_value.update(transition.state, total_return)\n",
    "            # Update our policy estimator\n",
    "            estimator_policy.update(transition.state, advantage, transition.action)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "policy_estimator = PolicyEstimator()\n",
    "value_estimator = ValueEstimator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Note, due to randomness in the policy the number of episodes you need to learn a good\n",
    "    # policy may vary. ~2000-5000 seemed to work well for me.\n",
    "    stats = reinforce(env, policy_estimator, value_estimator, 2000, discount_factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"lev1\">\n",
    "    <a href=\"#Lecture-4-Support-Notebook\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        Go to TOP\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clifwalk Actor Critic for Discrete Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=env.action_space.n,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.action_probs = tf.squeeze(tf.nn.softmax(self.output_layer))\n",
    "            self.picked_action_prob = tf.gather(self.action_probs, self.action)\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -tf.log(self.picked_action_prob) * self.target\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.action_probs, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, action, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target, self.action: action  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy \n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized \n",
    "        estimator_value: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = estimator_policy.predict(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = estimator_value.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - estimator_value.predict(state)\n",
    "            \n",
    "            # Update the value estimator\n",
    "            estimator_value.update(state, td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            estimator_policy.update(state, td_error, action)\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa21a349c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa21a349c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa21a349c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa21a349c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-14-3accf08a4b7a>:28: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa21a349198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa21a349198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa21a349198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa21a349198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Step 15 @ Episode 300/300 (-114.0))"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "policy_estimator = PolicyEstimator()\n",
    "value_estimator = ValueEstimator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Note, due to randomness in the policy the number of episodes you need to learn a good\n",
    "    # policy may vary. ~300 seemed to work well for me.\n",
    "    stats = actor_critic(env, policy_estimator, value_estimator, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"lev1\">\n",
    "    <a href=\"#Lecture-4-Support-Notebook\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        Go to TOP\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Actor Critic for Continuous Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.36159036], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.54248476,  0.00816753], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box(2,)\n",
      "Action space:  Box(1,)\n"
     ]
    }
   ],
   "source": [
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to converte a state to a featurizes represenation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_state(state):\n",
    "    \"\"\"\n",
    "    Returns the featurized representation for a state.\n",
    "    \"\"\"\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just linear classifier\n",
    "            self.mu = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            self.mu = tf.squeeze(self.mu)\n",
    "            \n",
    "            self.sigma = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            \n",
    "            self.sigma = tf.squeeze(self.sigma)\n",
    "            self.sigma = tf.nn.softplus(self.sigma) + 1e-5\n",
    "            self.normal_dist = tf.contrib.distributions.Normal(self.mu, self.sigma)\n",
    "            self.action = self.normal_dist._sample_n(1)\n",
    "            self.action = tf.clip_by_value(self.action, env.action_space.low[0], env.action_space.high[0])\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -self.normal_dist.log_prob(self.action) * self.target\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.loss -= 1e-1 * self.normal_dist.entropy()\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.action, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, action, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state, self.target: target, self.action: action  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just linear classifier\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy \n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized \n",
    "        estimator_value: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # env.render()\n",
    "            \n",
    "            # Take a step\n",
    "            action = estimator_policy.predict(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = estimator_value.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - estimator_value.predict(state)\n",
    "            \n",
    "            # Update the value estimator\n",
    "            estimator_value.update(state, td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            estimator_policy.update(state, td_error, action)\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.72 µs\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-26-57dd81837574>:27: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From <ipython-input-26-57dd81837574>:38: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Step 410 @ Episode 50/50 (-46.384251971235585)"
     ]
    }
   ],
   "source": [
    "%time\n",
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "policy_estimator = PolicyEstimator(learning_rate=0.001)\n",
    "value_estimator = ValueEstimator(learning_rate=0.1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Note, due to randomness in the policy the number of episodes you need varies\n",
    "    # TODO: Sometimes the algorithm gets stuck, I'm not sure what exactly is happening there.\n",
    "    stats = actor_critic(env, policy_estimator, value_estimator, 50, discount_factor=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
